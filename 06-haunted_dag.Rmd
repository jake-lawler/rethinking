# The Haunted DAG & The Causal Terror {#haunted_dag}

```{r echo = FALSE, warning = FALSE, message = FALSE}

library(tidyverse)
library(rethinking)
library(vroom)
library(GGally)
library(dagitty)
library(tidybayes)
library(tidybayes.rethinking)
library(modelr)
library(ggdist)

```

## Chapter Notes

The chapter opens with an example of a selection effect. Assuming that journal editors and peer reviewers care about both novelty and rigour, we can expect to see a negative correlation between the two in published papers.

Here's a simulation and a chart:

```{r}

set.seed(1914) 
N <- 200 # we simulate 200 papers
p <- 0.1 

nw <- rnorm(N) # we create a newsworthiness figure for each paper
tw <- rnorm(N) # we create a trustworthiness figure for each paper

# note that there is no correlation between the two in the simualation

s <- nw + tw 
q <- quantile( s , 1-p ) # the top 10% of papers are accepted
selected <- ifelse( s >= q , "Yes" , "No" ) # are the papers selected? i.e. published


data_pub <- tibble(Trustworthiness = tw, Newsworthiness= nw,Selected =selected)

ggplot(mapping=aes(Newsworthiness,Trustworthiness,colour=Selected))+
  geom_point(data=data_pub)+
    geom_smooth(data=filter(data_pub,Selected=="Yes"),method=lm,formula ='y ~ x',se=FALSE)
  
```
The blue line is the negative correlation between newsworthiness and trustworthiness induced by the selection process. This effect can occur inside of a multiple regression, where it is called collider bias.


### Multicollinearity {-}

How to multiple regression models behave when the predictor variables are highly correlated?

The chapter contains a simulation for the effect of leg length on height.

```{r}

set.seed(909) 
N <- 100 # we have 100 people
height <- rnorm(N,10,2) # we simulate their heights
leg_prop <- runif(N,0.4,0.5) #we simulate their leg length as a proportion of their height
leg_left <- leg_prop*height + rnorm( N , 0 , 0.02 ) # we calculate the length of each leg using the above 
leg_right <- leg_prop*height + rnorm( N , 0 , 0.02 ) # making sure to include a little error


data_leg <- tibble(height,leg_left,leg_right)

# we fit a model and plot the parameters

m6_1 <- quap( 
  alist(
    height ~ dnorm( mu , sigma ) , 
    mu <- a + bl*leg_left + br*leg_right , 
    a ~ dnorm( 10 , 100 ) , 
    bl ~ dnorm( 2 , 10 ) , 
    br ~ dnorm( 2 , 10 ) , 
    sigma ~ dexp( 1 )
) , data=data_leg )

gather_draws(m6_1,a, bl, br, sigma) %>%
  median_qi(.width=0.89)%>%
ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper), )+
  geom_pointinterval(colour = "dark grey")+
  ylab("parameters")
  
```

Our simulation specified that leg length would be highly correlated with height. However, the model is very uncertain about the influence of either leg on height. Why is this? The chapter contains a nice explanation. Our predictor variables are so highly correlated that it is almost as if we have only one predictor, used twice:

$$
\begin{aligned}
y_i &∼ \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 x_i + \beta_2 x_i
\end{aligned}
$$
or 

$$
\mu_i = \alpha + (\beta_1  + \beta_2 )x_i
$$
There are a huge number of combinations of $\beta_1$ and $\beta_2$ that could produce the data.

Here's the chapter:

> Recall that a multiple linear regression answers the question: *What is the value of knowing each predictor, after already knowing all of the other predictors?*

Here we sample the model parameters $b_l$ and $b_r$ and plot them and their sum. There are highly correlated, and their sum is centered on the true parameter value for regressing leg length on height.  

```{r}

data_leg_post <- spread_draws(m6_1,br,bl) %>%
  mutate(`sum of bl and br`=br+bl)

ggplot(data_leg_post)+
  geom_point(aes(x=br,y=bl),col='#77AADD',alpha=0.2)

ggplot(data_leg_post)+
  geom_density(aes(x=`sum of bl and br`),col='#77AADD',size=1)


```  

The chapter reintroduces the primate milk example from chapter 5.

```{r}

# load data
data_milk <- read_delim("data/milk.csv", delim = ";",col_names = TRUE)%>%
  mutate(K=standardize(kcal.per.g),
         F=standardize(perc.fat),
         L=standardize(perc.lactose))

# kcal.per.g regressed on perc.fat 

m6_3 <- quap(
  alist(
    K ~ dnorm( mu , sigma ), 
    mu <- a + bF*F, 
    a ~ dnorm( 0 , 0.2 ), 
    bF ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
) , data=data_milk )

# kcal.per.g regressed on perc.lactose 


m6_4 <- quap( 
  alist(
    K ~ dnorm( mu , sigma )
    , mu <- a + bL*L, 
    a ~ dnorm( 0 , 0.2 ), 
    bL ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
) , data=data_milk )


# kcal.per.g regressed on both 

m6_5 <- quap( 
  alist(
    K ~ dnorm( mu , sigma )
    , mu <- a + bF*F + bL*L, 
    a ~ dnorm( 0 , 0.2 ),
    bF ~ dnorm( 0 , 0.5 ),
    bL ~ dnorm( 0 , 0.5 ), 
    sigma ~ dexp( 1 )
) , data=data_milk )

``` 

Here is the parameter summary for regression of kcal per gram of milk on the fat percentage of the milk:

```{r}
summary_6_3 <- m6_3%>%
  gather_draws(a,bF,sigma)%>%
  mean_qi(.width = 0.89)%>%
  mutate(model="Fat")

summary_6_3

```

And here's the summary for the regression of kcals on lactose content:

```{r}
summary_6_4 <- m6_4%>%
  gather_draws(a,bL,sigma)%>%
  mean_qi(.width = 0.89)%>%
  mutate(model="Lactose")

summary_6_4

```
The parameters are almost mirror images of each other: the more fat content of the milk the higher the calories, the higher lactose content the lower the calories. This makes sense since fat is more energy dense than carbohydrate and fat and lactose contents are negatively correlated. 

Here's what happens when both are included in the same model:

```{r}
summary_6_5 <- m6_5%>%
  gather_draws(a,bF,bL,sigma)%>%
  mean_qi(.width = 0.89)%>%
  mutate(model="Both")

summary_6_5
```

Here's a plot:

```{r}
bind_rows(summary_6_3, summary_6_4,summary_6_5) %>%
  ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) +
  geom_pointinterval(position = position_dodge(width = -0.3))+
  geom_vline(xintercept=0, linetype = "dashed")+
  ylab("parameters")
```

Including both fat and lactose contents in the model pulls the parameter estimates $b_F$ and $b_L$ towards zero, and about doubles the uncertainty around both.

Here's a pairs plot of the two variables:

```{r}
ggpairs(data=select(data_milk,kcal.per.g,perc.fat,perc.lactose))
```

The chapter summarises:

> These two variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps as much once you already know the other.... The problem of multicollinearity is amember of a family of problems with fitting models, a family sometimes known as non-identifiability. When a parameter is non-identifiable, it means that the structure of the data and model do not make it possible to estimate the parameter’s value. 


And here's a DAG the chapter suggests for this case. D here is how dense the milk needs to be, and is related to how frequently the species nurses. Species that nurse more often tend to have relatively more lactose and less fat in their milk.

```{r echo=FALSE,message=FALSE}

dag_milk <- dagitty( "dag{ D [latent]; D -> L; D -> F; F -> K; L-> K }" ) 
coordinates(dag_milk) <- list( x=c(L=0,D=1,K=1,F=2) , y=c(L=0,D=0,K=1,F=0) )
drawdag( dag_milk )


```

Simulating Colliinearity

```{r echo=FALSE,message=FALSE}

data_milk <- vroom("data/milk.csv")%>%
  mutate(K=standardize(kcal.per.g),
         F=standardize(perc.fat),
         L=standardize(perc.lactose))

sim.coll <- function( r=0.9 ) { 
  data_milk$x <- rnorm( nrow(data_milk) , mean=r*data_milk$perc.fat , sd=sqrt( (1-r^2)*var(data_milk$perc.fat) ) )
  m <- lm( kcal.per.g ~ perc.fat + x , data=data_milk ) 
  sqrt( diag( vcov(m) ) )[2] # stddev of parameter
}

rep.sim.coll <- function( r=0.9 , n=100 ) { 
  stddev <- replicate( n , sim.coll(r) ) 
  mean(stddev)
}

r.seq <- seq(from=0,to=0.99,by=0.01) 
stddev <- sapply( r.seq , rep.sim.coll, n=100 )


ggplot(mapping=aes(x=r.seq,y=stddev))+
  geom_line(col="blue")+
  xlab("Correlation")


```

```{r echo=FALSE,message=FALSE, cache = TRUE}

dag_fungus <- dagitty("dag{M [unobserved]; H_0 -> H_1; M -> H_1; M -> F; T -> F; }")
coordinates(dag_fungus) <- list( x=c(H_0=0,H_1=1,M=1.5, F=2, T=3) , y=c(H_0=0,H_1=0,M=1.1, F=0, T=0) )
drawdag( dag_fungus)

set.seed(71) 
N <- 1000 
h0 <- rnorm(N,10,2) 
treatment <- rep( 0:1 , each=N/2 ) 
M <- rbern(N) 
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 + 0.4*M ) 
h1 <- h0 + rnorm( N , 5 + 3*M )
d2 <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )

set.seed(100)
m6_7 <- quap( alist( 
  h1 ~ dnorm( mu , sigma ), 
  mu <- h0 * p, 
  p <- a + bt*treatment + bf*fungus, 
  a ~ dlnorm( 0 , 0.2 ) , 
  bt ~ dnorm( 0 , 0.5 ), 
  bf ~ dnorm( 0 , 0.5 ), 
  sigma ~ dexp( 1 )
), data=d2 )

set.seed(100)
m6_8 <- quap( alist( 
  h1 ~ dnorm( mu , sigma ), 
  mu <- h0 * p, 
  p <- a + bt*treatment, 
  a ~ dlnorm( 0 , 0.2 ), 
  bt ~ dnorm( 0 , 0.5 ), 
  sigma ~ dexp( 1 )
), data=d2 )

ggplot(data=precis( m6_7 , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6_7 , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()+
  ggtitle("Including Fungus as Variable")

ggplot(data=precis( m6_8 , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6_8 , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()+
  ggtitle("Excluding Fungus as Variable")  

```

Here's my very rough attempt at explaining to myself why conditioning on fungus makes treatment appear to have a small positive effect.

If fungus is present we may have:

* treatment present and moisture present
* treatment absent and moisture either way (leaning towards present)

If fungus is absent we may have:

* treatment absent and moisture absent
* treatment present and moisture either way (leaning towards absent)

So once we know fungus there is a weak positive correlation between treatment and moisture. Since moisture is good for plant growth, this means a positive relationship between treatment and growth.




## Questions

### 6E1 {-}

#### Question {-}

>List three mechanisms by which multiple regression can produce false inferences about causal effects.

#### Answer {-}

1. Multicollinearity - regression on highly correlated predictors can produce misleading parameters, as in the leg, or primate milk examples.

2. Post-treatment bias - regression on post-treatment effects can make it appear that the treatment is not effective, as in the fungus example.

3. Collider bias - regression on a collider can create the appearance of an association between two variables that does not exist.

### 6E3 {-}

#### Question {-}

>List the four elemental confounds. Can you explain the conditional dependencies of each?

#### Answer {-}

1. Fork - X and Y are independent, once we condition on Z.

```{r echo=FALSE,message=FALSE}

dag_fork <- dagitty( "dag{ Z -> X; Z -> Y }" ) 
coordinates(dag_fork) <- list( x=c(X=0,Z=1,Y=2) , y=c(X=0,Y=0,Z=1) )
drawdag( dag_fork)


```

2. Pipe - X and Y are independent, once we condition on Z.

```{r echo=FALSE,message=FALSE}

dag_pipe <- dagitty( "dag{ X -> Z; Z -> Y }" ) 
coordinates(dag_pipe) <- list( x=c(X=0,Y=2,Z=1) , y=c(X=0,Y=2,Z=1) )
drawdag( dag_pipe)


```

3. Collider - Conditioning on Z creates an association between X and Y.

```{r echo=FALSE,message=FALSE}

dag_collider <- dagitty( "dag{ X -> Z; Y -> Z }" ) 
coordinates(dag_collider) <- list( x=c(X=0,Y=2,Z=1) , y=c(X=1,Y=1,Z=0) )
drawdag( dag_collider)


```

4. Descendant - Conditioning on D partly conditions on Z.

```{r echo=FALSE,message=FALSE}

dag_descendant <- dagitty( "dag{ X -> Z; Y -> Z; Z -> D}" ) 
coordinates(dag_descendant) <- list( x=c(X=0,Y=2,Z=1,D=1) , y=c(X=1,Y=1,Z=0,D=0.8) )
drawdag( dag_descendant)


```

### 6E4 {-}

#### Question {-}

>How is a biased sample like conditioning on a collider? Think of the example at the open of the chapter.

#### Answer {-}

Using the publishing example at the beginning of the chapter, we know that there is no causal relationship between trustworthiness and newsworthiness (because that's how the simulation is constructed). However, both cause selection for publication, creating a collider. We saw that sampling only the published papers created a negative correlation between the two predictors. 

Conditioning on publication would have the same effect - once we know newsworthiness and publication status, we can deduce some information about trustworthiness, or vice versa. For example, a study that was published but had low newsworthiness must be quite trustworthy. A study with high trustworthiness that wasn't published is probably not very newsworthy. 

Conditioning on the collider has the same result as only sampling from papers that were published: the creation of a spurious association.

### 6M1 {-}

#### Question {-}

>Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y: C ← V→ Y. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which variables should you condition on now?

#### Answer {-}

Here's the original DAG:

```{r echo=FALSE,message=FALSE}

dag_6M1A <- dagitty( "dag { U [unobserved] X -> Y; U-> X; A->U; A -> C ; C -> Y; U -> B; C-> B}")
coordinates(dag_6M1A) <- list( x=c(X=0,U=0,A=1,B=1,C=2,Y=2) , y=c(X=1,U=0.3,A=0,B=0.6,C=0.3,Y=1) )

drawdag(dag_6M1A)


```

Now we add the unobserved V:

```{r echo=FALSE,message=FALSE}

dag_6M1B <- dagitty( "dag { U [unobserved] V [unobserved] X -> Y X <- U <- A -> C -> Y U -> B <- C; V -> C; V-> Y}")
coordinates(dag_6M1B) <- list( x=c(X=0,U=0,A=1,B=1,C=2,Y=2,V=2.5) , y=c(X=1,U=0.3,A=0,B=0.6,C=0.3,Y=1,V=0.6) )

drawdag(dag_6M1B)

```

Previously, we could condition on either A or C to find the direct casual effect of X on Y. Now C is a collider, so we should condition on A.

The dagitty package corrobates this:

```{r echo=TRUE,message=FALSE}

adjustmentSets( dag_6M1B , exposure="X" , outcome="Y" )

```


### 6M2 {-}

#### Question {-}

>Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG X → Z → Y. Simulate data from this DAG so that the correlation between X and Z is very large. Then include both in a model predicting Y. 
>
Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter?

#### Answer {-}

Modifying the leg example:

```{r message=FALSE, cache = TRUE}

N <- 1000 
set.seed(909) 

Y <- rnorm(N,10,2)
Y_prop <- runif(N,0.4,0.5) 

Z <- Y_prop*Y + rnorm( N , 0 , 0.02 )
Z_prop <- runif(N,0.8,0.9) 

X <- -Z_prop*Z + rnorm( N , 0 , 0.02 )

data_6M2 <- bind_cols(X=X,Z=Z,Y=Y)

ggpairs(data_6M2)

set.seed(100)
m6M2 <- quap( 
  alist(
    Y ~ dnorm( mu , sigma ) , 
    mu <- a + bX*X + bZ*Z , 
    a ~ dnorm( 10 , 100 ) , 
    bX ~ dnorm( -3 , 10 ) , 
    bZ ~ dnorm( 2 , 10 ) , 
    sigma ~ dexp( 1 )
) , data=data_6M2 )


ggplot(data=precis(m6M2))+
  geom_pointrange(aes(x=rownames(precis(m6M2)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()
  
```

In the legs example, the model was very uncertain about the parameter values for both legs. This is not the case here, the parameter estimate for the influence of Z is about as expected, 2-2.5. The model has correctly identified that the only influence of X on Y is through Z, and so produces parameter estimates for X much smaller than expected. We have a pipe, and including Z blocks the path from X to Y, but we don't have the problems with identifiability that we had in the legs example. 

There is no way to tell this scenario apart from the legs example simply from looking at the pairwise correlations.


### 6M3 {-}

#### Question {-}

>Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (condition on) to estimate the total causal influence of X on Y.

#### Answer {-}


```{r echo=FALSE,message=FALSE}

dag_6M3.1 <- dagitty( "dag {A -> Y; A -> Z; Z->X; Z -> Y ; X -> Y}")
coordinates(dag_6M3.1) <- list( x=c(X=0,Z=0.5,A=1,Y=1) , y=c(Z=0,A=0,X=1,Y=1) )

dag_6M3.2 <- dagitty( "dag {A -> Y; A -> Z; X->Z; Z -> Y ; X -> Y}")
coordinates(dag_6M3.2) <- list( x=c(X=0,Z=0.5,A=1,Y=1) , y=c(Z=0,A=0,X=1,Y=1) )

dag_6M3.3 <- dagitty( "dag {A -> X; A -> Z; X->Z; Y -> Z ; X -> Y}")
coordinates(dag_6M3.3) <- list( x=c(X=0,Z=0.5,A=0,Y=1) , y=c(Z=0,A=0,X=1,Y=1) )

dag_6M3.4 <- dagitty( "dag {A -> X; A -> Z; X->Z; Z -> Y ; X -> Y}")
coordinates(dag_6M3.4) <- list( x=c(X=0,Z=0.5,A=0,Y=1) , y=c(Z=0,A=0,X=1,Y=1) )

```

1. We should condition on Z to block the backdoor path.


```{r message=FALSE}

drawdag(dag_6M3.1)

adjustmentSets(dag_6M3.1,exposure = "X",outcome = "Y",effect = "total")

```

2. We no longer want to condition on Z - it is a collider.


```{r message=FALSE}

drawdag(dag_6M3.2)

adjustmentSets(dag_6M3.2,exposure = "X",outcome = "Y",effect = "total")

```

3. We no longer want to condition on Z - it is a collider. 


```{r message=FALSE}

drawdag(dag_6M3.3)

adjustmentSets(dag_6M3.3,exposure = "X",outcome = "Y",effect = "total")

```

4. We don't want to condition on Z here because we are looking for the *total* casual influence of X on Y - one route of this influence goes through Z. We should condition on A however, to block the backdoor path.


```{r message=FALSE}

drawdag(dag_6M3.4)

adjustmentSets(dag_6M3.4,exposure = "X",outcome = "Y",effect = "total")

```

### 6H1 {-}

#### Question {-}

>Use the Waffle House data to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph.

#### Answer {-}

Looking through the available variables, I think I want to consider the following:
* Number of Waffle Houses (W)
* Divorce rate (D)
* Whether we're in the South (S)
* Marriage rate (M)
* Median age of marriage (A)

Here's my proposed DAG:

```{r message=FALSE}

dag_waf <- dagitty( "dag {S -> W; S -> M; S->A; W -> D ; M -> D; A -> D;}")
coordinates(dag_waf) <- list( x=c(W=0,S=1,M=1,D=1,A=2) , y=c(S=0,W=1,M=1,A=1,D=2) )

drawdag(dag_waf )

adjustmentSets(dag_waf,exposure = "W",outcome = "D",effect = "total")

```

So we need to include S in the model, to block the backdoor path through A and M.

```{r echo=FALSE ,message=FALSE}

data_waf <- read_csv2("data/WaffleDivorce.csv")%>%
  select(WaffleHouses,Divorce,South,Marriage,MedianAgeMarriage)%>%
  mutate(South=if_else(South==1,2,1),
         WaffleHouses=standardize(WaffleHouses),
         Divorce=standardize(Divorce),
         Marriage=standardize(Marriage),
         MedianAgeMarriage=standardize(MedianAgeMarriage))

# Prior Simulation

waf_prior_sim <- tibble(a=rnorm(100,0,0.6),bW=rnorm(100,0,0.2),mu=a+bW)

waf_prior_plot <- ggplot()+
  geom_abline(data=waf_prior_sim, mapping=aes(slope=bW,intercept=a))+
  geom_hline(yintercept = 2 ,colour="red")+
  geom_hline(yintercept = -2,colour="red")+
  xlim(min(data_waf$WaffleHouses),max(data_waf$WaffleHouses))

```

After loading the data, standardising, and doing some prior simulation, I have the model below:

```{r message=FALSE, cache=TRUE}

set.seed(100)
m6H1 <- quap( 
  alist(
    Divorce ~ dnorm( mu , sigma ) , 
    mu <- a[South] + bW*WaffleHouses, 
    a[South] ~ dnorm( 0 , 0.6 ) , 
    bW ~ dnorm( 0 , 0.2 ) , 
    sigma ~ dexp( 1 )
) , data=data_waf )


ggplot(data=precis(m6H1,depth = 2))+
  geom_pointrange(aes(x=rownames(precis(m6H1,depth = 2)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()
```

This is consistent with the number of Waffle Houses having no causal effect on the divorce rate.

### 6H2 {-}

#### Question {-}

>Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? 
>
Does the graph need more or fewer arrows? Feel free to nominate variables that aren’t in the data.

#### Answer {-}

```{r message=FALSE}
impliedConditionalIndependencies(dag_waf)
```

We'll first test that the divorce rate is independent of being in the South, conditional on number of waffle houses, median age of marriage, and marriage rate.

```{r message=FALSE, cache = TRUE}

set.seed(100)
m6H2.1 <- quap( 
  alist(
    Divorce ~ dnorm( mu , sigma ) , 
    mu <- a[South] + bW*WaffleHouses + bM*Marriage + bA*MedianAgeMarriage, 
    a[South] ~ dnorm( 0 , 0.6 ) , 
    bW ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 1 ) ,
    bA ~ dnorm( 0 , 1 ) ,
    sigma ~ dexp( 1 )
) , data=data_waf )

m6H2.1_post <- post <- extract.samples(m6H2.1)
m6H2.1_post$diff_south <- m6H2.1_post$a[,1] - m6H2.1_post$a[,2]

ggplot(data=precis( m6H2.1_post , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6H2.1_post , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()

```

The values of the diff_south parameter are consistent with the conditional independence, but I'm not completely happy that I've caught all the ways that being in the South can influence the divorce rate. Perhaps I should add an arrow directly from the South to the divorce rate, or add in an unobserved variable to stand in for cultural/ religious attitudes towards divorce.


We'll test one more conditional independence, one that I think is more likely to be true. Let's see if the median age of marriage is independent of the number of waffle houses, once we condition on being in the south.

```{r message=FALSE, cache = TRUE}

set.seed(100)
m6H2.2 <- quap( 
  alist(
    MedianAgeMarriage ~ dnorm( mu , sigma ) , 
    mu <- a[South] + bW*WaffleHouses, 
    a[South] ~ dnorm( 0 , 0.6 ) , 
    bW ~ dnorm( 0 , 1 ) , 
    sigma ~ dexp( 1 )
) , data=data_waf )

m6H2.2_post <- extract.samples(m6H2.2)
m6H2.2_post$diff_south <- m6H2.2_post$a[,1] - m6H2.2_post$a[,2]

ggplot(data=precis( m6H2.2_post , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6H2.2_post , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()

```

The parameter bW is estimated to be quite close to zero.


### 6H3 {-}

#### Question {-}

>Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? 
>
You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range.

#### Answer {-}

We don't need to condition on any other parameters since we're looking for the total causal effect.

```{r echo=FALSE ,message=FALSE, cache = TRUE}

data_foxes <- read_csv2("data/foxes.csv",col_types = "fnnnn")%>%
  mutate(avgfood=standardize(avgfood),
         area=standardize(area),
         groupsize=standardize(groupsize),
         weight=standardize(weight))

# Draw DAG

dag_foxes <- dagitty( "dag {A -> F; F -> G; F->W; G -> W}")
coordinates(dag_foxes) <- list( x=c(F=0,A=1,W=1,G=2) , y=c(A=0,F=1,G=1,W=2) )

drawdag(dag_foxes )

adjustmentSets(dag_foxes,exposure = "A",outcome = "W",effect = "total")

# Prior Simulation

foxes_prior_sim <- tibble(a=rnorm(100,0,0.7),bA=rnorm(100,0,0.3),mu=a+bA)

foxes_prior_plot <- ggplot()+
  geom_abline(data=foxes_prior_sim, mapping=aes(slope=bA,intercept=a))+
  geom_hline(yintercept = 2 ,colour="red")+
  geom_hline(yintercept = -2,colour="red")+
  xlim(min(data_foxes$area),max(data_foxes$area))

# Model

set.seed(100)
m6H3.1 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bA*area, 
    a ~ dnorm( 0 , 0.7 ) , 
    bA ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

# Posterior

ggplot(data=precis( m6H3.1 , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6H3.1 , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()


```


Unexpectedly, the total causal impact of area on weight appears to be zero, or slightly negative. Increasing area would not make foxes heavier.


### 6H4 {-}

#### Question {-}

>Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food?

#### Answer {-}

We don't need to condition on any other parameters assuming the DAG we're given is correct.

```{r echo=FALSE ,message=FALSE, cache = TRUE}

adjustmentSets(dag_foxes,exposure = "F",outcome = "W",effect = "total")

# Prior Simulation

foxes_prior_sim <- tibble(a=rnorm(100,0,0.7),bF=rnorm(100,0,0.3),mu=a+bF)

foxes_prior_plot <- ggplot()+
  geom_abline(data=foxes_prior_sim, mapping=aes(slope=bF,intercept=a))+
  geom_hline(yintercept = 2 ,colour="red")+
  geom_hline(yintercept = -2,colour="red")+
  xlim(min(data_foxes$avgfood),max(data_foxes$avgfood))

# Model

set.seed(100)
m6H4.1 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bF*avgfood, 
    a ~ dnorm( 0 , 0.7 ) , 
    bF ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

# Posterior

ggplot(data=precis( m6H4.1 , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6H4.1 , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()

```


The total causal impact of food on weight again appears to be negative. Increasing food would not make foxes heavier.


### 6H5 {-}

#### Question {-}

>Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking at the posterior distribution of the resulting model, what do you think explains these data? That is, can you explain the estimates for all three problems? How do they go together?

#### Answer {-}

Now we have to condition on food to block the backdoor path.

```{r echo=FALSE ,message=FALSE, cache = TRUE}

adjustmentSets(dag_foxes,exposure = "G",outcome = "W",effect = "total")

# Prior Simulation

foxes_prior_sim <- tibble(a=rnorm(100,0,0.7),bG=rnorm(100,0,0.3),mu=a+bG)

foxes_prior_plot <- ggplot()+
  geom_abline(data=foxes_prior_sim, mapping=aes(slope=bG,intercept=a))+
  geom_hline(yintercept = 2 ,colour="red")+
  geom_hline(yintercept = -2,colour="red")+
  xlim(min(data_foxes$groupsize),max(data_foxes$groupsize))

# Model

set.seed(100)
m6H5.1 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bF*avgfood + bG*groupsize, 
    a ~ dnorm( 0 , 0.7 ) , 
    bF ~ dnorm( 0 , 0.3 ) , 
    bG ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

# Posterior

ggplot(data=precis( m6H5.1 , depth=2 ))+
  geom_pointrange(aes(x=rownames(precis( m6H5.1 , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  geom_hline(yintercept = 0,col="red")+
  xlab("parameter")+
  coord_flip()

```

We see that the causal impact of group size is negative, and that the direct effect of food is zero or slightly positive. It is at least not so negative as the total causal effect.

I'd suggest that what's happening here is that the main effect of an increase in food (either directly or by an increase in area) would be to increase group size, which has a detrimental effect on weight. This effect seems to overwhelm any direct effect of increasing food on weight.


## Further Reading {-}


