# Monsters and Mixtures {#monsters_mixtures}


```{r echo=FALSE, warning=FALSE,message=FALSE}

library(MASS)
library(tidyverse)
library(rethinking)
library(GGally)
library(dagitty)
library(knitr)
library(RColorBrewer)
library(cowplot)

inverse_logit <- function(x){
  exp(x) / (1 + exp(x))
}

```

## Chapter Notes


### Over-Dispersed Counts {-}

The chapter opens with a discussion of over-dispersion in count data - when the data exhibits more variation than can be explained by a binomial or Poisson distribution. We'll try to address this using two types of continuous mixture models - beta-binomial and negative-binomial models. 

The beta-binomial distribution is the binomial distribution, except that instead of the probability of success p being fixed, it is drawn from some beta distribution.

The chapter example returns to the UCB admissions data from the previous chapter, except this time we allow each row of the data (i.e. each department / gender combination) is allowed to have a different probability of admission - drawn from a beta distribution. I'd previously seen beta distributions with $\alpha$ and $\beta$ parametrisation, but the chapter uses $\bar{p}$ and $\theta$, with $\alpha = \bar{p}\theta$ and $\beta = (1-\bar{p})\theta$. Here $\bar{p}$ is the average probability and $\theta$ is a shape parameter.

Here is the model used for the UCB data:

$$
\begin{aligned}
A_i &\sim \text{BetaBinomial}(N_i,\bar{p_i},\theta) \\
\text{logit}(\bar{p_i}) &= \alpha_{\text{gen}[i]} \\
\alpha_j &\sim \text{Normal}(0,1.5) \\
\theta & \sim \phi + 2 \\
\phi & \sim \text{Exponential}(1)
\end{aligned}
$$
The higher the value of $\theta$, the more concentrated the probability. When $\bar{p}_i$ is 0.5, a $\theta$ of 2 gives a completely flat distribution. This is why $\theta$ is assigned a minimum of two in the model above.

We fit the model, and examine the posterior:

```{r include = FALSE, cache = TRUE}
data("UCBadmit")

data_UCB <- as_tibble(UCBadmit) %>%
  mutate(gen = if_else(applicant.gender == "male", 1, 2),
         gen = as_factor(gen))

list_UCB <- with(data_UCB, list( A=admit , N=applications , gen=gen ) )

m12.1 <- ulam( alist( 
  A ~ dbetabinom( N , pbar , theta ), 
  logit(pbar) <- a[gen], 
  a[gen] ~ dnorm( 0 , 1.5 ), 
  transpars> theta <<- phi + 2.0, 
  phi ~ dexp(1)
), data=list_UCB , chains=4, cmdstan = TRUE)

post_UCB <- with(extract.samples( m12.1 ),tibble(a1=a[,1],a2=a[,2],phi = phi, theta = theta)) %>%
  mutate(diff_a = a1 - a2,
         pbar1 = inverse_logit(a1),
         pbar2 = inverse_logit(a2))
                      


```

```{r echo = FALSE}

precis(m12.1, depth = 2)

```

The probability of admission increases with the value of $\alpha$. The difference between $\alpha$ for men and women is 

```{r}
mean(post_UCB$diff_a)
```

suggesting the model believes women are more likely to be admitted. However the standard deviation of this value is `r sd(post_UCB$diff_a)`;the model is very uncertain. We contrast this with model m11.7 in the last chapter, which predicted that men were more likely to be admitted, and was quite a bit more confident about this. Even though we haven't included department in the model, allowing $p$ to vary by department / gender combination has captured some of the variation between departments.

Here's a plot: 

```{r echo = FALSE, cache = TRUE}

plot_12.1 <- ggplot()+xlim(0,1)+ylim(0,3)+xlab("Probability of Admission")+ylab("Density")

  for(i in 1:100){
  plot_12.1 <- plot_12.1+
            geom_function(fun = dbeta2, args = list(prob = post_UCB$pbar2[[i]], theta = post_UCB$theta[[i]]), alpha=0.1)
  }

plot_12.1

```


The chapter then moves on to the use of negative binomial (or gamma-Poisson) continuous mixture models to address over-dispersion. These are Poisson models, where the rate is allowed to vary across observations by drawing it from a gamma distribution. The gamma-Poisson distribution has two parameters, one is a rate parameter $\lambda$ and one ($\phi$) controls the variance. The distribution has $\text{var} = \lambda + \frac{\lambda^2}{\phi}$ so smaller $\phi$ implies larger variance.

The chapter refits the tool data from chapter 11 with a gamma-Poisson distribution, the idea is that we expect an outlier point like Hawaii to become less influential, because the model can accommodate more variation (in a Poisson distribution the variance necessarily equals the mean).

```{r include = FALSE, cache = TRUE}

data(Kline)

data_tool <- as_tibble(Kline)%>%
  mutate(P = standardize(log(population)),          
         cid = if_else(contact=="high",2,1),
         cid = as_factor(cid))

list_tool <- with(data_tool, list(T=total_tools, P=population, cid=cid))

m12.2 <- ulam( alist( 
  T ~ dgampois( lambda , phi ), 
  lambda <- exp(a[cid])*P^b[cid] / g, 
  a[cid] ~ dnorm(1,1), 
  b[cid] ~ dexp(1), 
  g ~ dexp(1), 
  phi ~ dexp(1)
), data=list_tool , chains=4 , log_lik=TRUE, cmdstan = TRUE )

```

Here are the posterior plots of the tools model using a Poisson distribution, and using the gamma-Poisson:

```{r include = FALSE, cache = TRUE}

################### Creating the Poisson graph ###################

list_tool_std <- with(data_tool, list(T=total_tools, P=P, cid=cid))

m11.10 <- ulam( alist( 
  T ~ dpois( lambda ), 
  log(lambda) <- a[cid] + b[cid]*P, 
  a[cid] ~ dnorm( 3 , 0.5 ), 
  b[cid] ~ dnorm( 0 , 0.2 )
), data=list_tool_std , chains=4 , log_lik=TRUE, cmdstan = TRUE )

seq_P <- seq( from=-1.4 , to=3 , length.out=100 )
seq_pop <- exp( seq_P*1.53 + 9 )    # reversing the effect of the scale function (by multiplying by the sd and adding the mean) then exponentiating
psis_tool <- PSIS( m11.10 , pointwise=TRUE )$k

# mean and compatibility interval for low contact
post_tool_lct <- as_tibble(link(m11.10, data= tibble(P = seq_P, cid = 1)))

tool_lct_lower <- purrr::map_dbl(post_tool_lct ,quantile,probs=0.025,names=FALSE)

tool_lct_mean <- purrr::map_dbl(post_tool_lct ,mean)

tool_lct_upper <- purrr::map_dbl(post_tool_lct ,quantile,probs=0.975,names=FALSE)

# mean and compatibility interval for high contact
post_tool_hct <- as_tibble(link(m11.10, data= tibble(P = seq_P, cid = 2)))

tool_hct_lower <- purrr::map_dbl(post_tool_hct ,quantile,probs=0.025,names=FALSE)

tool_hct_mean <- purrr::map_dbl(post_tool_hct ,mean)

tool_hct_upper <- purrr::map_dbl(post_tool_hct ,quantile,probs=0.975,names=FALSE)


post_plot_tool_pop <- tibble(pop = seq_pop, lct_lower = tool_lct_lower, lct_mean = tool_lct_mean, lct_upper = tool_lct_upper,
                                               hct_lower = tool_hct_lower, hct_mean = tool_hct_mean, hct_upper = tool_hct_upper)

# plot population scale
plot_tool_pop <- ggplot(data = post_plot_tool_pop)+
  geom_point(data = cbind(Kline,psis_tool), mapping = aes(x = population, y = total_tools, size = psis_tool, colour = contact))+
  geom_line(aes(x = pop, y = lct_mean), colour= "#F8766D")+ 
  geom_line(aes(x = pop, y = hct_mean), colour= "#00BFC4")+ 
  geom_ribbon(aes(x=pop,ymin=lct_lower,ymax=lct_upper),alpha=0.1,fill="#F8766D")+
  geom_ribbon(aes(x=pop,ymin=hct_lower,ymax=hct_upper),alpha=0.1,fill="#00BFC4")+
  xlab("population")+
  ylab("total tools")+
  theme(legend.position =  "none")+
  coord_cartesian( xlim = c(0, 300000), ylim = c(0, 80))+
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))+
  scale_colour_manual(values = c("#00BFC4","#F8766D"))

################### Creating the gamma-Poisson graph ###################

psis_tool_gp <- PSIS( m12.2 , pointwise=TRUE )$k

# mean and compatibility interval for low contact
post_tool_lct_gp <- as_tibble(link(m12.2, data= tibble(P = seq_pop, cid = 1)))

tool_lct_lower_gp <- purrr::map_dbl(post_tool_lct_gp ,quantile,probs=0.025,names=FALSE)

tool_lct_mean_gp <- purrr::map_dbl(post_tool_lct_gp ,mean)

tool_lct_upper_gp <- purrr::map_dbl(post_tool_lct_gp ,quantile,probs=0.975,names=FALSE)

# mean and compatibility interval for high contact
post_tool_hct_gp <- as_tibble(link(m12.2, data= tibble(P = seq_pop, cid = 2)))

tool_hct_lower_gp <- purrr::map_dbl(post_tool_hct_gp ,quantile,probs=0.025,names=FALSE)

tool_hct_mean_gp <- purrr::map_dbl(post_tool_hct_gp ,mean)

tool_hct_upper_gp <- purrr::map_dbl(post_tool_hct_gp ,quantile,probs=0.975,names=FALSE)


post_plot_tool_gp <- tibble(pop = seq_pop, lct_lower = tool_lct_lower_gp, lct_mean = tool_lct_mean_gp, lct_upper = tool_lct_upper_gp,
                                               hct_lower = tool_hct_lower_gp, hct_mean = tool_hct_mean_gp, hct_upper = tool_hct_upper_gp)

# plot population scale
plot_tool_gp <- ggplot(data = post_plot_tool_gp)+
  geom_point(data = cbind(Kline,psis_tool_gp), mapping = aes(x = population, y = total_tools, size = psis_tool, colour = contact))+
  geom_line(aes(x = pop, y = lct_mean), colour= "#F8766D")+ 
  geom_line(aes(x = pop, y = hct_mean), colour= "#00BFC4")+ 
  geom_ribbon(aes(x=pop,ymin=lct_lower,ymax=lct_upper),alpha=0.1,fill="#F8766D")+
  geom_ribbon(aes(x=pop,ymin=hct_lower,ymax=hct_upper),alpha=0.1,fill="#00BFC4")+
  xlab("population")+
  ylab("total tools")+
  theme(legend.position =  "none")+
  coord_cartesian( xlim = c(0, 300000), ylim = c(0, 80))+
  scale_x_continuous(labels = function(x) format(x, scientific = FALSE))+
  scale_colour_manual(values = c("#00BFC4","#F8766D"))
```

```{r echo = FALSE}

plot_grid(plot_tool_pop+ggtitle("Poisson"),plot_tool_gp+ggtitle("Gamma-Poisson"))

```

Here blue dots are high contact, and red low contact societies. The size of the points is scaled by Pareto k-value. The gamma-Poisson is less influenced by Hawaii, and consequently much more uncertain in large populations.


### Zero-Inflated Outcomes {-}

The zero-inflated Poisson model is introduced as an example of a mixture model: models that use multiple probability distribution to measure the influence of more than one cause. With zero-inflation, we aim to model a count variable where zeros can be produced in more than one way. In the monastery example in the chapter, each day monks have a fixed probability of taking the day off (maybe they spend the day drinking wine). On these days they will produce zero manuscripts. If they do work, they will produce some (low) number of manuscripts over the course of the day, and this might also be zero (maybe they just finished a bunch). So a zero can be produced two ways (broadly, as the outcome of a binomial process, or a Poisson process).

The chapter introduces the zero-inflated Poisson distribution: a binomial / Poisson mixture. The probability of a zero is:

$$
\begin{aligned}
\text{Pr}(0|p,\lambda) & = \text{Pr}(\text{drink}|p) + \text{Pr}(\text{work}|p) \times \text{Pr}(0|\lambda) \\
&= p + (1-p)\exp(-\lambda)
\end{aligned}
$$
and the probability of some non-zero figure is:

$$
\begin{aligned}
\text{Pr}(y > 0|p,\lambda) & = \text{Pr}(\text{work}|p) \times \text{Pr}(y|\lambda) \\
&= (1-p)\frac{\lambda^y \exp(-\lambda)}{y!}
\end{aligned}
$$
The formulas here come the Poisson likelihood (rate $\lambda$) and the binomial (probability $p$ of taking the day off).

A zero-inflated Poisson model will look something like this, for some predictor x:

$$
\begin{aligned}
y_i & \sim \text{ZIPoisson}(p_i,\lambda_i) \\
\text{logit}(p_i)&= \alpha_p + \beta_p x_i\\
\log(\lambda_i)&= \alpha_\lambda + \beta_\lambda x_i\\
\end{aligned}
$$

The chapter expands on zero-inflation Poisson models by simulating some data from our fictional monastery, fitting a model, and attempting to recover the data-generating process.


### Ordered Categorical Outcomes {-}

Here the outcome we want to predict is made up of some number of categories, like a multinomial. Except that the categories are ordered, e.g. an approval rating from 1 (strongly disapprove) to 5 (strongly approve). The ordering is important, but the scale is not necessarily linear, and so shouldn't be modelled as a continuous outcome.

The way of dealing with this described in the chapter is to use a log cumulative odds function, as we have used the log odds link in previous chapters. The chapter introduces a trolley problem example where respondents grade the moral permissability of action in a scenario on a scale of 1 to 7.

Here I've reproduced some charts in the chapter that show the counts of each response, the cumulative proportion, and then the log cumulative odds.

```{r eval = FALSE}
data(Trolley)

data_trol <- as_tibble(Trolley)%>%
  mutate(response = as_factor(response),
         prop = )

plot_trol_hist <- ggplot(data_trol)+
                    geom_histogram(aes(x = response), stat="count")


hist_trol <- data_trol%>%
  group_by(response)%>%
  summarise(n = n())%>%
  mutate(prop = n / sum(n),
         prop_cum = cumsum(prop),
         log_cum_odds = log(prop_cum / (1- prop_cum)))

plot_trol_prop <- ggplot(hist_trol, mapping = aes(x = response, y = prop_cum,group = 1))+
            geom_line()+
            geom_point()+
            ylab("cumulative proportion")

plot_trol_odd <-  ggplot(hist_trol%>%filter(log_cum_odds!= Inf), mapping = aes(x = response, y = log_cum_odds,group = 1))+
                    geom_line()+
                    geom_point()+
                    ylab("log cumulative odds")

plot_grid(plot_trol_hist,plot_trol_prop,plot_trol_odd,nrow =1)

```

A model with no predictors is introduced, to check that we can recover the cumulative proportions in the data in the posterior distribution: 

```{r eval = FALSE}

m12.4 <- ulam( alist( 
  R ~ dordlogit( 0 , cutpoints ), 
  cutpoints ~ dnorm( 0 , 1.5 )
) , data=list( R=data_trol$response ), chains=4 , cores=4,cmdstan = TRUE )


# cumulative proportions in the data
round(hist_trol$prop_cum, 3)

# model expectations for cumulative proportions
round( inverse_logit(coef(m12.4)) , 3 )


```

Then the chapter explains how to include predictors in this kind of model. The log cumulative odds for each response k is modelled as a linear combination of it's intercept $\alpha_k$ and a standard linear model:

$$
\begin{aligned}
\log\frac{\text{Pr}(y_i \leq k)}{1 - \text{Pr}(y_i \leq k)} &= \alpha_k - \phi_i \\
\phi_i & = \beta x_i
\end{aligned}
$$

The subtraction is conventional, to ensure that positive $\beta$ means the predictor $x$ is positively associated with the outcome $y$.


The model actually used for the trolley data looks like this:

$$
\begin{aligned}
\log\frac{\text{Pr}(y_i \leq k)}{1 - \text{Pr}(y_i \leq k)} &= \alpha_k - \phi_i \\
\phi_i & = \beta_A A_i + \beta_C C_i + \beta_{I,i} I_i \\
\beta_{I,i} &= \beta_I + \beta_{IA} A_i + \beta_{IC} C_i 
\end{aligned}
$$
Here:
* $A_i$ is the value of action on row $i$, 0 or 1.
* $I_i$ is the value of intention on row $i$, 0 or 1.
* $C_i$ is the value of contact on row $i$, 0 or 1.
* $B_I,i$ introduces an interaction effect between intention and action, and intention and contact.

```{r eval = FALSE}

list_trol <- with(data_trol, list(R = response, A = action, C = contact, I = intention))

m12.5 <- ulam( alist( 
  R ~ dordlogit( phi , cutpoints ), 
  phi <- bA*A + bC*C + BI*I , 
  BI <- bI + bIA*A + bIC*C , 
  c(bA,bI,bC,bIA,bIC) ~ dnorm( 0 , 0.5 ), 
  cutpoints ~ dnorm( 0 , 1.5 )
) , data=list_trol , chains=4 , cores=4, cmdstan = TRUE )

```

```{r eval = FALSE}
ggplot(data=precis(m12.5))+
  geom_pointrange(aes(x=rownames(precis(m12.5)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+
  xlab("parameter")+
  coord_flip()

```

We can see that all of the predictors (action - bA, contact - bC, intention - bI) are all negatively associated with permissability. 

Need to revisit this for posterior plots. And also the section on ordered categorical predictors.


## Questions

### 12E1 {-}

#### Question {-}


#### Answer {-}



## Further Resources {-}


