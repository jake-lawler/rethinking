# Ulysses' Compass {#ulysses}

```{r echo = FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(rethinking)
library(GGally)
library(dagitty)
library(tidybayes)
library(tidybayes.rethinking)
library(modelr)
library(ggdist)
library(knitr)
```

```{r}
# Colours by Paul Tol.
# Defined in R by Joachim Goedhart 
# Source: doi 10.5281/zenodo.3381072

tol_light <- c('#BBCC33', '#AAAA00', '#77AADD', '#EE8866', '#EEDD88', '#FFAABB', '#99DDFF', '#44BB99', '#DDDDDD')
```

## Chapter Notes

This chapter is all about model selection. The title refers to our need to navigate between the Scylla of overfitting and the Charybdis of underfitting. Respectively learning too much and too little from the data.

In the previous chapter about confounding, we learned that in some cases adding predictor variables to your model can makes inferences worse. However if we only care about prediction, and not inference, these variables can still help. So should we just add all plausible predictors to our models?

This chapter answers no. Even if all we care about is prediction, we are at risk of overfitting to the training data set, which will make our model worse when predicting on new data.

Here's an example from the chapter, on hominin brain volumes, and body mass:

### Brain Volume model {-}

```{r}

data_hominin <- tibble(
  species = c( "afarensis","africanus","habilis","boisei", "rudolfensis","ergaster","sapiens"),
  brain_vol = c( 438 , 452 , 612, 521, 752, 871, 1350 ), # brain volume
  body_size = c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 ), # body mass
  vol_std = brain_vol/max(brain_vol), # keeping zero brain volume as a reference point
  mass_std = (body_size - mean(body_size))/sd(body_size)
)

```

The structure of this section is that we're going to regress brain volume on body mass using increasingly higher-order polynomials and see what happens to $R^2$. Here's a simple linear regression:

$$
\begin{aligned}
b_i &\sim \text{Normal}(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta m_i \\
\alpha &\sim \text{Normal}(0.5,1)&& (\text{Centred on approx. mean brain volume.})\\
\beta &\sim \text{Normal}(0,10)&& (\text{Very flat.})\\
\sigma &\sim \text{Log-Normal}(0,1)\\
\end{aligned}
$$



```{r }

set.seed(71)

m7_1 <- quap( alist(
  vol_std ~ dnorm( mu , exp(log_sigma) ), 
  mu <- a + b*mass_std, 
  a ~ dnorm( 0.5 , 1 ), 
  b ~ dnorm( 0 , 10 ), 
  log_sigma ~ dnorm( 0 , 1 )
), data=data_hominin )


```

We calculate the $R^2$ of this model:

```{r}

# we define a function for the empirical variace
# since var() in R uses the denominator n-1
var_n <- function(vector){
  mean((mean(vector)-vector)^2)
}

# we define a function to produce the R^2 of each model we'll be fitting
hominin_r_squared <- function(model){
  
  pred_hominin <- add_predicted_draws(data_hominin,model)%>% # predict brain volume using observed body weights
  group_by(species,vol_std)%>%
  summarise(pred_vol=mean(.prediction), .groups = "keep")%>% # we'll use the mean of our predictions
  mutate(residuals = pred_vol-vol_std)
  
  1 - var_n(pred_hominin$residuals)/var_n(pred_hominin$vol_std) # our R^2 calc
}

hominin_r_squared(m7_1)

```

We then do the same for polynomials of increasing order, starting with a model like this one:

$$
\begin{aligned}
b_i &\sim \text{Normal}(\mu_i,\sigma)\\
\mu_i &= \alpha + \beta_1 m_i + \beta_2 m_i^2 && (\text{A second degree polynomial})\\
\alpha &\sim \text{Normal}(0.5,1)\\
\beta &\sim \text{Normal}(0,10)\\
\sigma &\sim \text{Log-Normal}(0,1)\\
\end{aligned}
$$

We fit all these models, up to a polynomial of degree six and calculate the $R^2$. I'll hide this code because it's pretty repetitive.

```{r echo = FALSE}

m7_2 <- quap(
    alist(
        vol_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=data_hominin , start=list(b=rep(0,2)) ) # start tells quap how long our b vector is


m7_3 <- quap(
    alist(
        vol_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=data_hominin , start=list(b=rep(0,3)) )

m7_4 <- quap(
    alist(
        vol_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=data_hominin , start=list(b=rep(0,4)) )

m7_5 <- quap(
    alist(
        vol_std ~ dnorm( mu , exp(log_sigma) ),
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 ),
        log_sigma ~ dnorm( 0 , 1 )
    ), data=data_hominin , start=list(b=rep(0,5)) )

m7_6 <- quap(
    alist(
        vol_std ~ dnorm( mu , 0.001 ), # must set sd to a fixed number in this one
        mu <- a + b[1]*mass_std + b[2]*mass_std^2 +
                  b[3]*mass_std^3 + b[4]*mass_std^4 +
                  b[5]*mass_std^5 + b[6]*mass_std^6,
        a ~ dnorm( 0.5 , 1 ),
        b ~ dnorm( 0 , 10 )
    ), data=data_hominin , start=list(b=rep(0,6)) )

```

Now we plot the models:

```{r}

# Calculating R^2

values_r_square <- tibble(model = paste0("m7_",1:6))%>%
  rowwise()%>%
  mutate(r_squared = hominin_r_squared(get(model)), # get() allows us to refer to our models dynamically
         labels = paste("R^2 =",round(r_squared,2))) # labels for our plot later

# Create our data frame of draws from the posterior

grid_hominin <- data_grid(data_hominin,mass_std=seq_range(mass_std, n = 51))

draws_hominin <- add_predicted_draws(grid_hominin,m7_1)%>%
  mutate(model = "m7_1") # this loop be made more efficient I'm sure
  
for (i in 2:6){
    # I've also used get() here 
  draws_int <- add_predicted_draws(grid_hominin,get(paste0("m7_",i)))%>% 
    mutate(model = paste0("m7_",i))
  
  draws_hominin <- bind_rows(draws_hominin,draws_int)
}
  

draws_hominin%>%
  mutate(brain_vol = .prediction * max(data_hominin$brain_vol), # undoing the previous transformations
         body_size = mass_std*sd(data_hominin$body_size)+mean(data_hominin$body_size))%>%
  ggplot(aes(x = body_size, y=brain_vol)) +
  stat_lineribbon(aes(y = brain_vol)) +
  geom_point(data = data_hominin, colour = tol_light[[4]],size=2) +
  geom_text(data=values_r_square, # adding our R^2 lables from earlier
            aes(x=40,y=1900,label=labels),size=3)+
  facet_wrap(~model)+
  scale_fill_brewer(palette = "Greys")+
  xlab("body mass (kg)")+
  ylab("brain volumne (cc)")
  

```

With higher order polynomials we get an increasingly good fit to the data, eventually reaching $R^2 =1$. But some of these models make no sense. Model m7_6 predicts negative brain volume at body mass 58kg. If a new hominin was discovered at around this weight, this model would make horrific predictions of brain volume, even though the model perfectly retrodicts all of the training data.

Having set up the concern, the chapter goes on to outline some model selection tools: regularisation and information criteria. The first step is to define information entropy as a measure of uncertainty:

$$
H(p) = -E\log(p_i) = -\sum_{i=1}^n p_i \log(p_i)
$$
 The chapter summarises:
 
 > The uncertainty contained in a probability distribution is the average log-probability of an event.

And then Kullback-Leibler divergence as a measure of the additional uncertainty induced by using probabilities from one distribution to describe another distribution:

$$
D_{KL}(p,q) = \sum_{i} p_i ( \log(p_i) - \log(q_i))= \sum_{i} p_i  \log(\frac{p_i}{q_i}) 
$$

This is the average difference in log-probability between our target $p$ and model $q$, and we can use it to compare the accuracy of different models.

### Log Pointwise Predictive Density {-}

Revisit this section.

Computing the log pointwise predictive density. With data $y = \{y_i\}$ and posterior distribution $\Theta$:

$$
\text{lppd}(y,\Theta) = \sum_i \log \left( \frac{1}{S}\sum_s p(y_i | \theta_s) \right)
$$
Where $S$ is the number of samples and each $\theta_s$ is a sample from $\Theta$.

For an observation say $y_1$, you calculate the average probability of seeing this observation given the posterior (i.e. you sum the conditional probabilities of the observation over each posterior sample, and divide by the number of samples). You repeat this process for each observation $y_i$, take the log of each and  sum them all together. 

Here is the calculation done in R, from the "Overthinking: Computing the lppd" box:

```{r message=FALSE, warning= FALSE}

set.seed(1) 

lppd1 <- lppd( m7_1 , n=1e4 )
# Extracting posterior samples and calculating manually

logprob <- sim( m7_1 , ll=TRUE , n=1e4 )   # logprob contains 10,000 samples from the posterior distribution, each estimating the (log) probability of seeing each
                                           # of the seven observations. 

n <- ncol(logprob) 
ns <- nrow(logprob) 

average_post_prob <- function( i ) {
    log_sum_exp( logprob[,i] ) - log(ns)}     # this functions sums the log probabilities of the 10,000 samples and divides by the number of rows
                                              # except it is working on a log probability scale.

lppd2 <- sapply( 1:n , average_post_prob )    # the function is applied to each of the seven observations

lppd1
lppd2

```

Summing over these seven observations will give the total lppd.

### Simulating in and out of sample deviance for varying parameter numbers {-}

The idea here is to simulate from the following model with two parameters:

$$
\begin{aligned}
y_i & \sim \text{Normal}(\mu_i,1) \\
\mu_i &= (0.15)x_{1,i} - (0.4)x_{2,i}
\end{aligned}
$$

We fit linear regressions with between 1 and 5 parameters to the simulated data, and compare the in and out of sample deviance for these five models.

```{r eval = FALSE}

N <- 20 
kseq <- 1:5 

# sim_train_test is a function from the rethinking package.
# it simulates Gaussian data with N cases and fits a model with k parameters, 
# returning the in sample and out of sample lppd (can also do WAIC criteria)

# data_dev contains the mean in sample deviance, mean out of sample deviance, 
# sd of in sample deviance, and sd of out of sample deviance for each model with 1-5 parameters.  

data_dev <- map_dfr( kseq , function(k) {
  r <- replicate( 100 , sim_train_test( N=N, k=k , cv.cores = 8)); 
  tibble(mean_in = mean(r[1,]) ,mean_out= mean(r[2,]) ,sd_in= sd(r[1,]) ,sd_out= sd(r[2,]) )
} )

# Revisit this mess of a function, set eval to TRUE once it works again
# Add commentary below the graph

ggplot(data=data_dev)+
  geom_pointrange(aes(x=1:5,y=mean_in,ymin=mean_in-sd_in,ymax=mean_in+sd_in), colour = "blue")+
  geom_pointrange(aes(x=1.1:5.1,y=mean_out,ymin=mean_out-sd_out,ymax=mean_out+sd_out))+
  geom_text(aes(x=2.9, y = mean_in[3], label="in"),colour="blue")+
  geom_text(aes(x=3.2, y = mean_out[3], label="out"))+
  xlab("number of parameters")+
  ylab("deviance")+
  ggtitle("N = 20")


```

There is a section on the role that regularising priors can play in preventing over-fitting.

### Predicting Predicitve Accuracy {-}

This section introduces cross-validation and information criteria as a way to estimating model predictive performance on out-of-sample data. We'll end up at PSIS and WAIC respectively.

Leave-one-out cross-validation is a way to get an estimate of out-of-sample accuracy. However it is computationally expensive. A key insight when trying to approximate LOOCV is that unlikely observations have a larger effect on the posterior distribution. This insight leads to the importance sampling approach, the approach highlighted by the chapter is Pareto-smoothed importance sampling or PSIS. It has a couple of nice features, including the ability to provide diagnostic information and to estimate the standard error of out-of-sample deviance.

Revisit: Overthinking PSIS box 

Aside from cross-validation appraoches, we can also use information criteria to predict predictive accuracy. The Akaike information criterion estimates out-of-sample deviance:

$$
\text{AIC}=D_\text{train} + 2p = -2\text{lppd}+2p
$$
where $p$ is the number of free parameters in the posterior distribution. From the chapter:

> [W]hat AIC tells us is that the dimensionality of the posterior distribution is a natural measure of the model’s overfitting tendency. More complex models tend to overfit more, directly in proportion to the number of parameters... Mechanically, deriving AIC means writing down the goal, which is the expected KL divergence, and then making approximations. The expected bias turns out to be proportional to the number of parameters, provided a number of assumptions are approximately correct.

AIC requires quite strong assumptions - flat priors, approximately Gaussian posterior, large sample size. The chapter introduces Watanabe's widely applicable information criterion (WAIC), which does not make assumptions about the shape of the posterior and in a large sample converges to the cross-validation approximation.

The formula for WAIC is:

$$
\text{WAIC}(y,\Theta) = -2 \left( \text{lppd} - \sum_i \text{var}_\theta ( \log p(y_i | \theta)) \right)
$$

where the $y_i$'s are the observations and $\Theta$ is the posterior distribution. 

> The penalty term means, “compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty.”

 $\text{var}_\theta$ means taking the variance over the set of posterior samples. Like PSIS, WAIC is also calculated pointwise - they both split up the data into independent observations. This has benefits - but it's unclear how to interpret this is the case where some observations depend on previous observations (like in a time series).
 
 
The book calculates WAIC for a simple model. This code is straight out of the book:

```{r}

# fit a linear model of stopping distance & speed

data(cars) 

set.seed(100)
m <- quap( alist(
dist ~ dnorm(mu,sigma), 
mu <- a + b*speed, 
a ~ dnorm(0,100), 
b ~ dnorm(0,10), 
sigma ~ dexp(1)
) , data=cars )

set.seed(94)

cars_post <- extract.samples(m,n=1000)

# calculate (log) probability of seeing data in cars, assuming it comes from a normal distribution with mean and sd taken from the posterior samples. There are 1000 samples so logprob contains 1000 columns. There are 50 observations in cars so logprob has 50 rows.

n_samples <- 1000 

cars_logprob <- sapply( 1:n_samples , function(s) { 
  mu <- cars_post$a[s] + cars_post$b[s]*cars$speed 
  dnorm( cars$dist , mu , cars_post$sigma[s] , log=TRUE )
} )

# we take the log of the average probabilities (with a little extra code since we're working with log probabilities)

n_cases <- nrow(cars) 

cars_lppd <- sapply( 1:n_cases , function(i) log_sum_exp(cars_logprob[i,]) - log(n_samples) )

# we calculate the penalty term - taking the variance for each observation across all 1000 rows

cars_pWAIC <- sapply( 1:n_cases , function(i) var(cars_logprob[i,]) )

# we compute WAIC

-2*( sum(cars_lppd) - sum(cars_pWAIC) )

```


Model Comparison {-}

The chapter revisits the plan growth/ fungus example from chapter 6. In that chapter, including fungus in the model bias inferences against the treatment - the fungus was a post-treatment variable. The models fit were;

* m6_6 - model that didn't include either fungus or treatment as predictors
* m6_7 - model including both fungus and treatment as predictors
* m6_8 - model omitting fungus as a predictor

```{r echo = FALSE}

# I#m just copying this code from chapter 6. 
# Except model m6_6 which I didn't fit there.

set.seed(71) 
N <- 100 #100 plants
# inital heights
h0 <- rnorm(N,10,2) 
treatment <- rep( 0:1 , each=N/2 ) # 0 is no treatment, 1 is the anti-fungal treatment
#anti-fungal treatment reduces probability of fungus
fungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4) 
# final height depends on initial height and fungus
h1 <- h0 + rnorm( N , 5 -3*fungus) 

data_fungus <- tibble( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )

m6_6 <- quap( alist( 
  h1 ~ dnorm( mu , sigma ), 
  mu <- h0 * p, 
  p ~ dlnorm( 0 , 0.25 ), 
  a ~ dlnorm( 0 , 0.2 ) , 
  bt ~ dnorm( 0 , 0.5 ), 
  bf ~ dnorm( 0 , 0.5 ), 
  sigma ~ dexp( 1 )
), data=data_fungus )

m6_7 <- quap( alist( 
  h1 ~ dnorm( mu , sigma ), 
  mu <- h0 * p, 
  p <- a + bt*treatment + bf*fungus, 
  a ~ dlnorm( 0 , 0.2 ) , 
  bt ~ dnorm( 0 , 0.5 ), 
  bf ~ dnorm( 0 , 0.5 ), 
  sigma ~ dexp( 1 )
), data=data_fungus )

m6_8 <- quap( alist( 
  h1 ~ dnorm( mu , sigma ), 
  mu <- h0 * p, 
  p <- a + bt*treatment, 
  a ~ dlnorm( 0 , 0.2 ), 
  bt ~ dnorm( 0 , 0.5 ), 
  sigma ~ dexp( 1 )
), data=data_fungus )

```

We compare the models using WAIC:

```{r}
set.seed(77)

compare( m6_6 , m6_7 , m6_8 , func=WAIC )

```
What are these figures? The chapter:

> Columns from left to right are: WAIC, standard error (SE) of WAIC, difference of each WAIC from the best model, standard error (dSE) of this difference, prediction penalty (pWAIC), and finally the Akaike weight.

Smaller values of WAIC are better, and the compare function orders them from lowest to highest. The penalty figure in WAIC is sometimes called "effective number of parameters". To tell if the models can be distinguished from each other in their expected prediction performance, we need to compare the WAIC differences in the dWAIC column to the standard error of this difference in the dSE column. Model 6_7 (the confounded model) is a lot better.

As an aside, I originally used 1,000 plants in my simulation instead of the 100 the chapter uses, and I was very surprised to see my WAIC figures on the order of 10 x higher than those printed in the book. A nice, accidental lesson on when WAIC figures are (not) comparable. 

## Questions

### 7E1 {-}

#### Question {-}

>State the three motivating criteria that define information entropy. Try to express each in your own words.

#### Answer {-}

We want our measure of uncertainty to be:

1. Continuous - a small change in the probabilities should lead to a small change in uncertainty.
2. Increasing - uncertainty should increase as the number of events increases
3. Additive - the uncertainty of two successive events should be the weighted sum of the uncertainties of each event.



### 7E2 {-}

#### Question {-}

>Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

#### Answer {-}

Since the textbook uses the natural log, I'll use it too. 

$$\begin{aligned}
H(p) &= -\sum^n_{i=1}p_i \log(p_i) \\
&= -(0.7 \log(0.7) + 0.3 \log(0.3))\\
&= 0.6109
\end{aligned}$$

These are the same figures as the 'rain or shine' example in the chapter.


### 7E3 {-}

#### Question {-}

>Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, “3” 25%, and “4” 30% of the time. What is the entropy of this die?

#### Answer {-}

$$\begin{aligned}
H(p) &= -\sum^n_{i=1}p_i \log(p_i) \\
&= -(0.2 \log(0.2) + 0.25 \log(0.25) + 0.25 \log(0.25) + 0.3 \log(0.3))\\
&= 1.3762
\end{aligned}$$


### 7E4 {-}

#### Question {-}

>Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?

#### Answer {-}

$$\begin{aligned}
H(p) &= -\sum^n_{i=1}p_i \ \log(p_i) \\
&= -(\frac{1}{3}  \log(\frac{1}{3}) + \frac{1}{3}  \log(\frac{1}{3}) + \frac{1}{3}  \log(\frac{1}{3}) + 0  \log(0))\\
&= -\log(\frac{1}{3}) &&\text{using the convention that } 0  \log(0) = 0 \\
&= 1.0986
\end{aligned}$$


### 7M1 {-}

#### Question {-}

>Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?

#### Answer {-}

_AIC_

$$
\text{AIC}(y,\Theta) = -2 \text{lppd} + 2p
$$

_WAIC_

$$
\text{WAIC}(y,\Theta) = -2 \left( \text{lppd} - \sum_i \text{var}_\theta ( \log p(y_i | \theta)) \right)
$$

WAIC is more general, since the adjustment term $\sum_i \text{var}_\theta ( \log p(y_i | \theta))$ is approximately equal to the number of parameters when the following constraints are in place:

* the posterior is Gaussian
* there is a large sample size
* the prior is uninformative (or overwhelmed by the data)


### 7M2 {-}

#### Question {-}

>Explain the difference between model selection and model comparison. What information is lost under model selection?

#### Answer {-}

Model selection here involves comparing candidate models using a some criterion (say WAIC), choosing the model with the lowest WAIC, and discarding the rest.

This approach involves throwing away information about the relative differences between models, which can give hints about how confident we should be about our models.


### 7M3 {-}

#### Question {-}

>When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.


#### Answer {-}

We should be able to answer this question by inspecting the formula for lppd:

$$
\text{lppd}(y,\Theta) = \sum_i \log \left( \frac{1}{S}\sum_s p(y_i | \theta_s) \right)
$$

Changing the observations means changing the $y_i$'s and therefore the calculation of the average probability of the observations given the data $\frac{1}{S}\sum_s p(y_i | \theta_s)$. Using a different set of observations for different models will make the resulting information criterion values uninterpretable. 

An easy way to see this is to consider that changing observations will lead to different information criterion values even with the *same* model.


### 7M4 {-}

#### Question {-}

>What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

#### Answer {-}

Before performing any experiments, I'll take a guess by looking at the definition of effective number of parameters for WAIC:

$$ \sum_i \text{var}_\theta ( \log p(y_i | \theta))$$

I'd expect the variance of the log probabilities here to decrease, since informative priors give a model more concentrated expectations for the data, i.e. they lead to unusual values of $y$  being assigned less probability.

So I expect that making priors more narrowly peaked decreases the effective number of parameters.

I'll pick up the cars data set from earlier and fit a couple of models with increasingly concentrated priors.

```{r warning=FALSE, message=FALSE}

# previous model

# m <- quap( alist(
# dist ~ dnorm(mu,sigma), 
# mu <- a + b*speed, 
# a ~ dnorm(0,100), 
# b ~ dnorm(0,10), 
# sigma ~ dexp(1)
# ) , data=cars )

cars_prior_sim_1 <- tibble(a=rnorm(100,0,100),b=rnorm(100,0,10),mu=a+b)

ggplot()+
  geom_abline(data=cars_prior_sim_1, mapping=aes(slope=b,intercept=a))+
  geom_hline(yintercept = 1.2*max(cars$dist) ,colour="red")+
  geom_hline(yintercept = 0.8*min(cars$dist),colour="red")+
  xlim(min(cars$speed),max(cars$speed))


```
These priors are very flat. Let's make them more concentrated.

Logically, the intercept a should be near zero since if you're going zero mph it should take you zero feet to stop.

Also we'd expect b to be positive - the faster you go the further it takes you to stop.

```{r warning=FALSE, message=FALSE}

cars_prior_sim_2 <- tibble(a=rnorm(100,0,5),b=rnorm(100,5,2.5),mu=a+b)

ggplot()+
  geom_abline(data=cars_prior_sim_2, mapping=aes(slope=b,intercept=a))+
  geom_hline(yintercept = 1.2*max(cars$dist) ,colour="red")+
  geom_hline(yintercept = 0.8*min(cars$dist),colour="red")+
  xlim(min(cars$speed),max(cars$speed))

```

I think this is reasonable for our purposes.

Now let's make the prior very concentrated.


```{r warning=FALSE, message=FALSE}

cars_prior_sim_3 <- tibble(a=rnorm(100,0,0.5),b=rnorm(100,5,0.5),mu=a+b)

ggplot()+
  geom_abline(data=cars_prior_sim_3, mapping=aes(slope=b,intercept=a))+
  geom_hline(yintercept = 1.2*max(cars$dist) ,colour="red")+
  geom_hline(yintercept = 0.8*min(cars$dist),colour="red")+
  xlim(min(cars$speed),max(cars$speed))

```

Now we fit two new models with our new priors and calculate the effective number of parameters.

```{r warning=FALSE, message=FALSE, cache = TRUE}

set.seed(100)
 m_moderate <- quap( alist(
 dist ~ dnorm(mu,sigma), 
 mu <- a + b*speed, 
 a ~ dnorm(0,5), 
 b ~ dnorm(5,2.5), 
 sigma ~ dexp(1)
 ) , data=cars )

 set.seed(100)
 m_concentrated <- quap( alist(
 dist ~ dnorm(mu,sigma), 
 mu <- a + b*speed, 
 a ~ dnorm(0,0.5), 
 b ~ dnorm(5,0.5), 
 sigma ~ dexp(1)
 ) , data=cars )
 

moderate_post <- extract.samples(m_moderate,n=1000)
concentrated_post <- extract.samples(m_concentrated,n=1000)

n_samples <- 1000 

moderate_logprob <- sapply( 1:n_samples , function(s) { 
  mu <- moderate_post$a[s] + moderate_post$b[s]*cars$speed 
  dnorm( cars$dist , mu , moderate_post$sigma[s] , log=TRUE )
} )

concentrated_logprob <- sapply( 1:n_samples , function(s) { 
  mu <- concentrated_post$a[s] + concentrated_post$b[s]*cars$speed 
  dnorm( cars$dist , mu , concentrated_post$sigma[s] , log=TRUE )
} )

n_cases <- nrow(cars) 

moderate_pWAIC <- sapply( 1:n_cases , function(i) var(moderate_logprob[i,]) )

concentrated_pWAIC <- sapply( 1:n_cases , function(i) var(concentrated_logprob[i,]) )

sum(cars_pWAIC)
sum(moderate_pWAIC)
sum(concentrated_pWAIC)

#compare(m,m_moderate,m_concentrated, func = PSIS)

```

As expected, the effective number of parameters (in the WAIC calculation) decreases as the prior becomes more concentrated.

Running the commented-out compare function shows the same for PSIS.

### 7M5 {-}

#### Question {-}

>Provide an informal explanation of why informative priors reduce overfitting.

#### Answer {-}

Informative priors make a model more sceptical of the data, since the model has narrower expectations of plausible parameter values before it even sees the data.

Overfitting occurs when a model is too wedded to the particular data set it is trained on. It encodes features of this data set that are unlikely to be present in future data,

Because of their scepticism, informative priors reduce the risk of overfitting. The aim is that only regular features - those that you might expect to occur in a future data set - will be encoded into the model. 


### 7M6 {-}

#### Question {-}

>Provide an informal explanation of why overly informative priors result in underfitting.

#### Answer {-}

If the priors make the model too sceptical of the data, it will fail to capture some of the regular features of the data. The model would do a better job of predicting future data if it were allowed to learn more from the training data.


### 7H1 {-}

#### Question {-}

>In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in, seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. 
>
I want you to actually fit a curve to these data, found in data(Laffer). Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue?

#### Answer {-}


```{r warning=FALSE, message=FALSE, cache = TRUE}

data(Laffer)

#laffer_prior1 <- tibble(a=rnorm(100,1,0.5),b=rnorm(100,0.3,0.2),mu=a+b)

#ggplot()+
#  geom_abline(data=laffer_prior1, mapping=aes(slope=b,intercept=a))+
#  geom_hline(yintercept = 1.2*max(Laffer$tax_revenue) ,colour="red")+
#  geom_hline(yintercept = 0.8*min(Laffer$tax_revenue),colour="red")+
#  xlim(min(Laffer$tax_rate),max(Laffer$tax_rate))

set.seed(100)
 m_laffer1 <- quap( alist(
 tax_revenue ~ dnorm(mu,sigma), 
 mu <- a + b*tax_rate, 
 a ~ dnorm(1,0.5), 
 b ~ dnorm(0.3,0.2), 
 sigma ~ dexp(1)
 ) , data=Laffer )
 
 laffer1_post <- extract.samples(m_laffer1)

#laffer_prior2 <- tibble(a=rnorm(100,1,0.5),b=rnorm(100,0.01,1),c=rnorm(100,0.01,1))

#ggplot()+
#  purrr::map(1:nrow(laffer_prior2), ~geom_function(fun = function(x) laffer_prior2$a[.x] + laffer_prior2$b[.x] * x + laffer_prior2$c[.x] * x^2 ))+
#  geom_hline(yintercept = 1.2*max(Laffer$tax_revenue) ,colour="red")+
#  geom_hline(yintercept = 0.8*min(Laffer$tax_revenue),colour="red")+
#  xlim(min(Laffer$tax_rate),max(Laffer$tax_rate))
 
 set.seed(100)
 m_laffer2 <- quap( alist(
 tax_revenue ~ dnorm(mu,sigma), 
 mu <- a + b*tax_rate +c*tax_rate^2, 
 a ~ dnorm(1,0.5), 
 b ~ dnorm(0.1,1), 
 c ~ dnorm(0.1,1),
 sigma ~ dexp(1)
 ) , data=Laffer )
 
 laffer2_post <- extract.samples(m_laffer2)
 
 laffer2_mean <- tibble(a=mean(laffer2_post$a),b=mean(laffer2_post$b),c=mean(laffer2_post$c),d=mean(laffer2_post$d))
 
set.seed(101)
 m_laffer3 <- quap( alist(
 tax_revenue ~ dnorm(mu,sigma), 
 mu <- a + b*tax_rate +c*tax_rate^2 +d*tax_rate^3, 
 a ~ dnorm(1,0.5), 
 b ~ dnorm(0.1,1), 
 c ~ dnorm(0.1,1),
 d ~ dnorm(0.1,1),
 sigma ~ dexp(1)
 ) , data=Laffer )
 
 laffer3_post <- extract.samples(m_laffer3)
 
 laffer3_mean <- tibble(a=mean(laffer3_post$a),b=mean(laffer3_post$b),c=mean(laffer3_post$c),d=mean(laffer3_post$d))
 
ggplot()+
  geom_point(data=Laffer, mapping=aes(tax_rate,tax_revenue))+
  geom_abline(data=laffer1_post, mapping=aes(slope=mean(b),intercept=mean(a)),colour="red")+
  geom_function(fun = function(x) laffer2_mean$a + laffer2_mean$b*x + laffer2_mean$c*x^2, colour = "blue")+
  geom_function(fun = function(x) laffer3_mean$a + laffer3_mean$b*x + laffer3_mean$c*x^2 + laffer3_mean$d*x^3, colour = "green")+
  xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) 

compare(m_laffer1,m_laffer2,m_laffer3)

```

The linear model predicts an increasing relationship between tax rate and tax revenue.

The quadratic model predicts a decreasing relationship between tax rate and tax revenue, after reaching a maximum.

They are with one standard error of each other by WAIC so I wouldn't want to draw strong conclusions about the relationship between tax rate and revenue from this data. Tax revenue appears to increase with tax rate, with a possible leveling out or decline after a certain rate.


### 7H2 {-}

#### Question {-}

>In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student’s t distribution to revisit the curve fitting problem. How much does a curved relationship depend upon the outlier point?

#### Answer {-}

We get warnings about high Pareto k values when using PSIS to compare the models:

```{r}

compare(m_laffer1,m_laffer2,m_laffer3, func=PSIS)

```


```{r message=FALSE}

set.seed(24071847) 

PSIS_m_laffer1 <- PSIS(m_laffer1,pointwise=TRUE) 

WAIC_m_laffer1 <- WAIC(m_laffer1,pointwise=TRUE) 

ggplot()+ 
  geom_point(aes(x=PSIS_m_laffer1$k , y=WAIC_m_laffer1$penalty), colour=if_else(PSIS_m_laffer1$k>0.5,"red","dark blue"))+
  geom_vline(aes(xintercept=0.5),linetype=2)+
  xlab("PSIS Pareto k")+
  ylab("WAIC penalty")

```


We alter the models fit above to use a Student's t distribution with shape parameter 2.

```{r warning=FALSE, cache = TRUE}

data(Laffer)

set.seed(100)
 m_laffer1_t <- quap( alist(
 tax_revenue ~ dstudent(2,mu,sigma), 
 mu <- a + b*tax_rate, 
 a ~ dnorm(1,0.5), 
 b ~ dnorm(0.3,0.2), 
 sigma ~ dexp(1)
 ) , data=Laffer )
 
 laffer1_post_t <- extract.samples(m_laffer1_t)

 set.seed(100)
 m_laffer2_t <- quap( alist(
 tax_revenue ~ dstudent(2,mu,sigma), 
 mu <- a + b*tax_rate +c*tax_rate^2, 
 a ~ dnorm(1,0.5), 
 b ~ dnorm(0.1,1), 
 c ~ dnorm(0.1,1),
 sigma ~ dexp(1)
 ) , data=Laffer )
 
 laffer2_post_t <- extract.samples(m_laffer2_t)
 
 laffer2_mean_t <- tibble(a=mean(laffer2_post_t$a),b=mean(laffer2_post_t$b),c=mean(laffer2_post_t$c),d=mean(laffer2_post_t$d))
 
ggplot()+
  geom_point(data=Laffer, mapping=aes(tax_rate,tax_revenue))+
  geom_abline(data=laffer1_post_t, mapping=aes(slope=mean(b),intercept=mean(a)),colour="red")+
  geom_function(fun = function(x) laffer2_mean_t$a + laffer2_mean_t$b*x + laffer2_mean_t$c*x^2, colour = "blue")+
  xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) 
 

#compare(m_laffer1_t,m_laffer2_t)

compare(m_laffer1_t,m_laffer2_t, func = PSIS)

```

We no longer receive the message about high Pareto k values

Now the predictions of the linear and quadratic models are very close over the whole range. The quadratic model appears to show some leveling off but there is no evidence of a decreasing relationship over the range where we have data.


### 7H3 {-}

#### Question {-}

>Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:

```{r echo=FALSE}
island_birds <- tibble(` `=c("Island 1", "Island 2", "Island 3"),
       `Species A` = c(0.2, 0.8, 0.05),
       `Species B` = c(0.2, 0.1, 0.15),
       `Species C` = c(0.2, 0.05, 0.7),
       `Species D` = c(0.2, 0.025, 0.05),
       `Species E` = c(0.2, 0.025, 0.05))

kable(island_birds, digits = 3)


```
>
Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island’s bird distribution. Interpret these entropy values. Second, use each island’s bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values.
>
Which island predicts the others best? Why?

#### Answer {-}

First we calculate the entropy of the bird distributions:

```{r}

island_birds_en <- island_birds%>%rowwise()%>%
  mutate(Entropy= -sum( c_across(contains("Species")) * log(c_across(contains("Species"))) ))
  
kable(island_birds_en,digits=3)
```

Entropy here can be interpreted as a measure of biodiversity. Island 1 has the highest entropy, with no one species dominating any other. Island 2, where species A accounts for 80% of all birds, has the lowest entropy.

```{r}
island_KL <- function(a,b){ sum( island_birds[a,-1] * log( island_birds[a,-1] / island_birds[b,-1]) )}

# island_KL(a,b) estimates the divergence of distribution in row a using the distribution in row b  

# Divergence from Island 1
island_KL(2,1)
island_KL(3,1)

# Divergence from Island 2

island_KL(1,2)
island_KL(3,2)

# Divergence from Island 3

island_KL(1,3)
island_KL(2,3)

island_birds_KL <- island_birds_en%>% bind_cols( `Sum of Divergences` = c(sum(island_KL(2,1),island_KL(3,1)),
                                                    sum(island_KL(1,2),island_KL(3,2)),
                                                    sum(island_KL(1,3),island_KL(2,3))))

kable(island_birds_KL,digits=3)

```

Island 1 is the best at predicting the other islands, because it has the highest entropy. It is the least "surprised" by the other islands.


### 7H4 {-}

#### Question {-}

>Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or PSIS, they will produce identical results). 
>
Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?

#### Answer {-}

```{r echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE}

d <- sim_happiness( seed=1977 , N_years=1000 ) 

d2 <- d[ d$age>17 , ]

d2$A <- ( d2$age - 18 ) / ( 65 - 18 )

d2$mid <- d2$married + 1 

set.seed(100)
m6.9 <- quap( alist(
happiness ~ dnorm( mu , sigma ),
mu <- a[mid] + bA*A, 
a[mid] ~ dnorm( 0 , 1 ), 
bA ~ dnorm( 0 , 2 ), 
sigma ~ dexp(1)
) , data=d2 )

set.seed(100)
m6.10 <- quap( alist(
happiness ~ dnorm( mu , sigma ), 
mu <- a + bA*A, 
a ~ dnorm( 0 , 1 ), 
bA ~ dnorm( 0 , 2 ), 
sigma ~ dexp(1)
) , data=d2 )


```


```{r}
compare(m6.9, m6.10)
```

Model 6.9 is expected to make better predictions, but model 6.10 provides the correct causal inference.

Model 6.9 includes marriage status in the model, which is a collider of age and happiness. Including it misleads about their relationship. However, both age and marriage status are associated with happiness, and so including them both allows the model to make better predictions, hence the lower WAIC.


### 7H5 {-}

#### Question {-}

>Revisit the urban fox data, data(foxes), from the previous chapter’s practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables:
>
1. avgfood + groupsize + area
2. avgfood + groupsize
3. groupsize + area
4. avgfood
5. area
>
Can you explain the relative differences in WAIC scores, using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences (dSE).

#### Answer {-}

```{r echo=FALSE, message=FALSE, warning=FALSE, cache = TRUE}

data_foxes <- read_csv2("data/foxes.csv",col_types = "fnnnn")%>%
  mutate(avgfood=standardize(avgfood),
         area=standardize(area),
         groupsize=standardize(groupsize),
         weight=standardize(weight))

dag_foxes <- dagitty( "dag {A -> F; F -> G; F->W; G -> W}")
coordinates(dag_foxes) <- list( x=c(F=0,A=1,W=1,G=2) , y=c(A=0,F=1,G=1,W=2) )

set.seed(100)
m7H5.1 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bF*avgfood + bG*groupsize + bA*area, 
    a ~ dnorm( 0 , 0.7 ) , 
    bF ~ dnorm( 0 , 0.3 ) , 
    bG ~ dnorm( 0 , 0.3 ) ,
    bA ~ dnorm( 0 , 0.3 ) ,
    sigma ~ dexp( 1 )
) , data=data_foxes)

set.seed(100)
m7H5.2 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bF*avgfood + bG*groupsize, 
    a ~ dnorm( 0 , 0.7 ) , 
    bF ~ dnorm( 0 , 0.3 ) , 
    bG ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

set.seed(100)
m7H5.3 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bG*groupsize + bA*area , 
    a ~ dnorm( 0 , 0.7 ) , 
    bG ~ dnorm( 0 , 0.3 ) ,
    bA ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

set.seed(100)
m7H5.4 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bF*avgfood, 
    a ~ dnorm( 0 , 0.7 ) , 
    bF ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

set.seed(100)
m7H5.5 <- quap( 
  alist(
    weight ~ dnorm( mu , sigma ) , 
    mu <- a + bA*area, 
    a ~ dnorm( 0 , 0.7 ) , 
    bA ~ dnorm( 0 , 0.3 ) , 
    sigma ~ dexp( 1 )
) , data=data_foxes)

```


```{r}

compare(m7H5.1,m7H5.2,m7H5.3,m7H5.4,m7H5.5)

drawdag( dag_foxes )



```


Surprising! Comparing the WAIC difference and the standard errors of the difference, it looks like 3 is expected to be a more accurate model than 1. Both contain group size and area, but 1 also contains food. Adding a predictor can lower the expected out-of-sample performance if the predictor has little association with the outcome, but this is unusual in light of the DAG in chapter 6, which suggests that the only influence of area is via food. It's not clear why adding area would be useful if adding food is not useful. Perhaps the DAG is incorrect, and there is also a direct line from area to group size.

I'll revisit this later and see if I can make sense of it.


## Further Reading {-}
