[["many_variables.html", "Chapter 5 The Many Variables &amp; The Spurious Waffles 5.1 Chapter Notes 5.2 Questions Further Reading", " Chapter 5 The Many Variables &amp; The Spurious Waffles 5.1 Chapter Notes This chapter introduces multiple regression and causal models. Will try to race through it, just getting some practice visualising priors, using tidybayes to display model uncertainty etc. Model 5.1: Modelling divorce rate on age at marriage. \\[ \\begin{aligned} D_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_A A_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 0.2) \\\\ \\beta_A &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\\\ \\end{aligned} \\] Here are the variables well use in this model and later on: \\(D_i\\) - the 2009 divorce rate per 1,000 adults for each U.S. state. \\(A_i\\) - The 2005-2010 median age at marriage for each state Plot of these priors: # we generate 100 samples from our prior distributions prior_sim_5_1 &lt;- tibble(a=rnorm(100, mean = 0, sd=0.2), bA = rnorm(100, mean = 0, sd = 0.5)) # and plot the lines they imply ggplot(prior_sim_5_1)+ geom_abline(aes(slope = bA, intercept = a))+ xlim(-3,3)+ ylim(-3,3)+ xlab(&quot;Median Age of Marriage (std)&quot;)+ ylab(&quot;Divorce Rate (std)&quot;) Heres the model and the posterior mean/expected divorce rate: data(WaffleDivorce) data_divorce &lt;- WaffleDivorce%&gt;%as_tibble() data_divorce %&lt;&gt;% mutate( divorce_s = (Divorce - mean(Divorce))/sd(Divorce), marriage_s = (Marriage - mean(Marriage))/sd(Marriage), age_s = (MedianAgeMarriage - mean(MedianAgeMarriage))/sd(MedianAgeMarriage), ) m5_1 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bA * age_s , a ~ dnorm( 0 , 0.2 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_1 &lt;- m5_1 %&gt;% gather_draws(a, bA, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_1&quot;) data_grid(data_divorce,age_s=seq_range(age_s, n = 51)) %&gt;% add_linpred_draws(m5_1) %&gt;% # add draws for mean divorce rate mutate(MedianAgeMarriage= age_s * sd(data_divorce$MedianAgeMarriage) + mean(data_divorce$MedianAgeMarriage), Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%&gt;% ggplot(aes(x = MedianAgeMarriage, y=Divorce)) + stat_lineribbon() + geom_point(data = data_divorce, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) Heres the same plot for model 5.2 - which uses marriage rate as a predictor instead of age at marriage: m5_2 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bM * marriage_s , a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_2 &lt;- m5_2 %&gt;% gather_draws(a, bM, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_2&quot;) data_grid(data_divorce,marriage_s=seq_range(marriage_s, n = 51)) %&gt;% add_linpred_draws(m5_2) %&gt;% # add draws for mean divorce rate mutate(Marriage= marriage_s * sd(data_divorce$Marriage) + mean(data_divorce$Marriage), Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%&gt;% ggplot(aes(x = Marriage, y=Divorce)) + stat_lineribbon() + geom_point(data = data_divorce, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) The chapter goes on to introduce DAGs and causal reasoning, before coming back to define a regression model that uses both marriage rate and median age at marriage as predictors. Here are a couple of possible DAGs for how marriage rate and median age of marriage influence the divorce rate: par(mar = c(4, 4, .1, .1)) dag_div1 &lt;- dagitty(&quot;dag{A -&gt; M; M -&gt; D; A -&gt; D }&quot;) coordinates(dag_div1) &lt;- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1)) drawdag(dag_div1) dag_div2 &lt;- dagitty(&quot;dag{A -&gt; M; A -&gt; D }&quot;) coordinates(dag_div2) &lt;- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1)) drawdag(dag_div2) We can use our data to test these proposed causal models, because they make different predictions. The first has no conditional independencies: impliedConditionalIndependencies(dag_div1) The second predicts that D is independent from M, conditional on A: impliedConditionalIndependencies(dag_div2) ## D _||_ M | A Heres our statistical model: \\[ \\begin{aligned} D_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_M M_i + \\beta_A A_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 0.2) \\\\ \\beta_M &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\beta_A &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\\\ \\end{aligned} \\] m5_3 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bM * marriage_s + bA * age_s , a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_3 &lt;- m5_3 %&gt;% gather_draws(a, bM, bA, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_3&quot;) summary_5_3 ## # A tibble: 4 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 0.00132 -0.150 0.154 0.89 median qi m5_3 ## 2 bA -0.609 -0.852 -0.372 0.89 median qi m5_3 ## 3 bM -0.0611 -0.305 0.175 0.89 median qi m5_3 ## 4 sigma 0.787 0.659 0.912 0.89 median qi m5_3 Heres a plot for the three models: bind_rows(summary_5_1, summary_5_2,summary_5_3) %&gt;% ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) + geom_pointinterval(position = position_dodge(width = -0.3))+ ylab(&quot;parameters&quot;) As predicted by the second DAG, marriage rate is not associated with divorce rate once you condition on age of marriage. 5.2 Questions 5E1 Question Which of the linear models below are multiple linear regressions? µi =  + xi µi = x * xi + z * zi µi =  + (xi  zi) µi =  + x * xi + z * zi Answer Number 4 looks the most like the multiple regressions in the chapter: µ is regressed on both xi and zi with the intercept . (2) is just (4) with the  set to zero, so that counts too. Number 1 is just a bivariate regression. Number 3 is interesting. I think this is not really a multiple regression, even though there are two variables. Rather than attempting to determine separately the influence of x and z on µ, you are asserting in the model that they have and equal and opposite impact. I think this is not really what you want a multiple regression to do, but dont feel confident about my answer. 5E2 Question Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition. Answer Oh boy. This question immediately feels like a trap with animal diversity is linearly related to latitude. Surely if I choose to use a multiple linear regression with two variables, and control for one, the only relationships Ill observe will be linear. Im going to ignore the linearly part of the question from this point. It seems like the claim is that if I naively regress animal diversity on to latitude without accounting for plant diversity, I would find no relationship. I.e. that the relationship between latitude and animal diversity is masked. If that interpretation is correct, I would start with a bivariate model. $$ A_i Normal(, ) \\ _i = + _L*L $$ Where if the claim is true I would expect to see little relationship. I would then move on to a multiple regression including plant diversity: $$ A_i Normal(, ) \\ _i = + _L*L +_P * P $$ and examine whether it appears as if a relationship has now emerged. 5E3 Question Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on. Answer $$ T_i Normal(, ) \\ _i = + _F*F +_S * S $$ T - time to PhD degree F - amount of funding S - size of laboratory 5E4 Question Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let \\(A_i\\) be an indicator variable that is 1 where case i is in category A. Also suppose \\(B_i\\), \\(C_i\\), and \\(D_i\\) for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when its possible to compute one posterior distribution from the posterior distribution of another model. \\[ \\begin{aligned} (1) \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i \\\\ (2) \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\ (3) \\mu_i &amp;= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\ (4) \\mu_i &amp;= \\alpha_A A_i + _B B_i + _C C_i + _D D_i \\\\ (5) \\mu_i &amp;= \\alpha_A(1  B_i  C_i D_i) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i \\end{aligned} \\] Answer 1 is the standard indicator variable approach. Where A, B and D are equal to 0, \\(\\alpha\\) is the mean \\(\\mu\\) where the predictor is at level C. 2 There is redundancy in two, surely it wouldnt be possible to estimate \\(\\alpha\\) - it can take any value and produce the same \\(\\mu\\) so long as the appropriate \\(\\beta_x\\) adjusts to compensate. I dont know if that means it is not inferentially equivalent though. 3 is clearly equivalent to (1), it doesnt make a difference (except for interpretation) which of the levels you label null. 4 Is equivalent also, just set \\(\\alpha_c\\) equal to \\(\\alpha\\) from (1). 5 Is an incredibly annoying way to set up your model, but can be pretty easily transformed into (3) with some algebra and relabelling: \\[ \\begin{aligned} \\mu_i &amp;= \\alpha_A(1  B_i  C_i D_i) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i \\\\ &amp;= \\alpha_A + (\\alpha_B-\\alpha_A)B_i + (\\alpha_C -\\alpha_A)C_i +(\\alpha_D-\\alpha_A)D_i \\\\ &amp;= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\end{aligned} \\] Further Reading "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
