[["index.html", "Statistical Rethinking Notes Preface", " Statistical Rethinking Notes Jake Lawler 2022-02-19 Preface These are my notes on Richard McElreaths Statistical Rethinking. They are currently incomplete. In particular: I havent done the question sets of the final few chapters yet. If you are a student working through Rethinking and happen to stumble across these notes, please be careful! I am not a statistician, and this book was my first exposure to a lot of this material - my summaries and answers to end-of-chapter questions are probably wrong in many places (!) This is also my first use of the bookdown package. Ive really enjoyed using it as a way of structuring textbook notes, and plan to keep using it in the future. Thank you to Yihui Xie for his work on the package and for the very helpful guide bookdown: Authoring Books and Technical Documents with R Markdown. Ive hidden a lot of the code used to train the models, create the graphs etc. Its all available at https://github.com/jake-lawler. The code is a little muddled because Im trying to use tidyverse-oriented R and the rethinking package is not built with that in mind. I think once I move on to e.g. Bayesian Data Analysis Ill try to use brms, or just raw Stan. Also, thanks to Richard McElreath. I had a lot of fun working my way through the book, and have collected a long list of interesting further reading that Im only just beginning to get started on. Items to revisit: Chapter Four - Revisit the spline section once Im better at using tidybayes to extract draws from more complicated models. "],["prague.html", "Chapter 1 The Golem of Prague 1.1 Chapter Notes 1.2 Questions Further Reading", " Chapter 1 The Golem of Prague 1.1 Chapter Notes The book opens with an analogy between statistical models and the Golem of Prague. Both are powerful, but lack wisdowm of their own. They thoughtlessly follow instructions and are therefore dangerous unless great care is taken. The chapter stresses the distinction between scientific hypotheses, process models, and statistical models. A hypothesis is described as an informal, natural language statement about the world - evolution is neutral is one example given. Hypotheses are vague, and can have many different interpretations. A process model is one instantiation of a hypothesis - it is more precise about the kinds of entities that are being posited, and the logical (including causal) relations between them. For example, in the case of the evolution is neutral hypothesis, whether you assume that population size has reached an equilibrium, or whether you assume that it fluctuates over time, will lead to two different process models for the same hypothesis. Finally, in order for a process model to interact with evidence, it must be formed into a statistical model. For example one process model for the hypothesis that evolution is neutral might imply a particular frequency distribution of alleles. A statistical model can be constructed to test whether, for some data collected, this frequency is observed. The chapter describes some concerns with a purely falsificationist approach to inference, and outlines the Bayesian modelling approach the book will pursue. However it emphasises that almost all of the book could be rewritten to remove the Bayesian aspects. Finally the chapter ends with a brief summary of a topic that will be very important throughout the book - graphical causal models (e.g. DAGs) for causal inference. 1.2 Questions There are no questions at the end of this chapter. Further Reading A lot of the endnotes in this chapter concern philosophy of science. Theres a fun paper on scientists conception of Popper: Mulkay, M. J. and Gilbert, N. (1981) Putting Philosophy to Work: Karl Poppers Influence on Scientific Practice A lot of Popper is recommended. Im current about a third of the way through The Logic of Scientific Discovery. The chapter emphasises that although Popper deployed falsification as a demarcation criterion, he did not believe that falsification should or could be the only method used in scientific inference. However I find it difficult to think of what other methods Popper could allow, since he does not believe in induction. Will need to read on to find out. I want to read the Daston and Galison book on objectivity because I enjoyed Dastons book on classical probability so much. Both of the following also sound intriguing: Kitcher, P. (2011) Science in a Democratic Society Collins and Pinch (1998) The Golem: What You Should Know About Science "],["small_worlds.html", "Chapter 2 Small Worlds and Large Worlds 2.1 Chapter Notes 2.2 Questions Further Reading", " Chapter 2 Small Worlds and Large Worlds 2.1 Chapter Notes Opens by drawing a distinction between the small world of the model and the large world of natural reality we are looking to make inferences about. The model is only a representation of the large world and it may exclude or misrepresent salient features, or fall apart outside of a narrow range of contexts. With that warning, the chapter begins to introduce inverse probability. Say we have four marbles in a bag, and the marbles can be either blue or white. We have five different hypotheses about how many blue and white marbles are in the bag: All marbles are white Three marbles are white, one is blue Two marbles are white, two are blue One marble is white, three are blue All marbles are blue Which of these is most likely? We want to answer this question by making a series of draws from the bag (with replacement) and using the results of these draws to estimate the relative plausibility of these hypotheses. Lets say we make three draws with replacement. We see a blue marble, a white marble, and then a blue marble. Call this b-w-b. To illustrate whats going on when we draw from the bag, lets assume temporarily that hypothesis 2 is correct. We want to display every possible result for a series of draws with replacement. In the code below, I use tidygraph and ggraph to allow this for up to three draws, although more draws could be performed with minor adjustments. # We first create a couple of data frames. One for the nodes - it will provide them labels # and assign a colour to them. The other data frame specifies the edges - the connections # between the nodes. # Create nodes - I&#39;ve chosen to label the nodes so that the zero node is the start - # no marbles have been drawn from the bag yet. The first four possible draws are then # labelled 1-4. Then once the first draw has been made the nodes representing the second # draw have two digits - the first represents the outcome of the first draw (1-4, for each # of the four marbles that could have been drawn), and the second digit represents each # of the four possible second draws (1-4). So node 34 means that the third marble was # drawn on the first draw and the fourth marble on the second draw. The nodes for # the third draw are labelled similarly - each with three digits 1 through 4. level1 &lt;- 1:4 node &lt;- c(0,level1) # these are the nodes for one draw for( i in 1:4){ intermediate &lt;- c(i*10 + level1 ) node &lt;- c(node,intermediate) } # this loop creates the nodes after a second draw. probably there is a cleaner way to do this level2 &lt;- node[node&gt;10] for( i in 1:4){ intermediate &lt;- c(i*100 + level2 ) node &lt;- c(node,intermediate) } # this loop creates the nodes after a third draw colour &lt;- as.factor(ifelse(node %% 10 ==1,&quot;blue&quot;,&quot;white&quot;)) # the first marble of each draw is blue, the rest white node &lt;- as.character(node) # these are node names, not integers nodes &lt;- tibble(node,colour) # We now draw the edges between the nodes. Potentially confusing since the integer 1 # here refers to the first node in the list above i.e. the node labelled &quot;0&quot;, and so on. # The integer 16 refers to the 16th node in the list above, i.e. to node labelled &quot;33&quot;. from &lt;- rep(1:21,each=4) # each of the first 21 nodes will have four connections to &lt;- c(2:85) edges &lt;- tibble(from,to) # we now create a tbl_graph object for the number of draws we want to display draws_1 &lt;- tbl_graph(edges=edges[1:4,], nodes=nodes[1:5,]) draws_2 &lt;- tbl_graph(edges=edges[1:20,], nodes=nodes[1:21,]) draws_3 &lt;- tbl_graph(edges=edges, nodes=nodes) # we will use this function to plot the tbl_graph objects above plot_draws &lt;- function(data_frame, layout, circular){ ggraph(data_frame, layout, circular = circular) + geom_edge_link() + geom_node_point(shape = 21, colour = &quot;black&quot;, size = 5, stroke = 1, aes(filter = node != &quot;0&quot;, #we don&#39;t show the starting node fill=colour))+ scale_fill_manual(values = c(&quot;light blue&quot;,&quot;white&quot;))+ theme(legend.position=&quot;none&quot;) } Here are all possible outcomes of one draw from the bag: plot_draws(draws_1,&quot;tree&quot;, FALSE)+scale_y_reverse() We have once chance to get a blue marble (there is only one in the bag under our hypothesis), and we have three chances to get a white marble. We then put the marble we drew back into the bag and draw again. All the possible outcomes for our two draws are displayed below: plot_draws(draws_2,&quot;tree&quot;, FALSE)+scale_y_reverse() For example, the marble on the farthest left represents the situation where we first drew a blue marble, placed in back into the bag, and the drew it again on the second draw. All possible outcomes of three draws are shown in the below diagram. This time Ive made it circular to avoid crowding in the graph. The nodes closest to the centre of the circle represent the first draw, and the nodes on the outside represent the third draw. plot_draws(draws_3,&quot;dendrogram&quot;, TRUE)+coord_fixed() How many ways of seeing the sequence b-w-b are there under our hypothesis 2? ggraph(draws_3, &quot;dendrogram&quot;, circular = TRUE) + geom_edge_link() + geom_node_point(shape = 21, colour = &quot;black&quot;, size = 5, stroke = 1, aes(filter = str_sub(node, 1, 1) == &quot;1&quot; &amp; # criteria for displaying nodes str_sub(node, 2, 2) != &quot;1&quot; &amp; (str_sub(node, -1, -1) == &quot;1&quot; | str_length(node)==2), fill=colour))+ scale_fill_manual(values = c(&quot;light blue&quot;,&quot;white&quot;))+ theme(legend.position=&quot;none&quot;)+ coord_fixed() There are three ways of seeing this sequence, displayed above. By conducting this exercise for each of the five hypotheses above, we are part way to an answer of how plausible each hypothesis is given the sequence of draws we observed. E.g. for hypothesis 1, there are 0 ways of seeing the observed sequence, since there are no blue marbles in the hypothesis. Similarly for hypothesis 5. The table in the chapter summarises: The other information we need to assess the relative plausibility of each hypothesis given the data we need is a measure of the relative plausibility of each hypothesis before seeing the data. It might make sense in this case to say that each hypothesis is as likely as any other, but in other circumstances we might believe one of our hypotheses carries a lot more initial plausibility. Were most of the way to Bayes rule now, and the next section of the chapter introduces it - moving from the language of counting to the language of probability. Afterwards, a simple model is introduced. Heres my attempt at recreating figure 2.5: df_plot &lt;- tibble(title=character(),p_grid = numeric(),posterior = numeric(),.rows=0) grid_length &lt;- 100 num_water &lt;- c(1,1,2,3,4,4,5,6,6) num_toss &lt;- c(1,2,3,4,5,6,7,8,9) toss_results &lt;- c(&quot;W&quot;,&quot;L&quot;,&quot;W&quot;,&quot;W&quot;,&quot;W&quot;,&quot;L&quot;,&quot;W&quot;,&quot;L&quot;,&quot;W&quot;) # define grid p_grid &lt;- seq( from=0 , to=1 , length.out=grid_length ) # define prior prior &lt;- rep( 1 , grid_length ) #loop through each of the three options for (i in 1:length(num_water)){ # compute likelihood at each value in grid likelihood &lt;- dbinom( num_water[i] , size=num_toss[i] , prob=p_grid ) # compute product of likelihood and prior unstd_posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd_posterior / sum(unstd_posterior) df_plot &lt;- df_plot %&gt;% bind_rows(tibble( title = paste(toss_results[1:i], collapse =&quot; &quot;), p_grid = p_grid, posterior = posterior)) } ggplot(df_plot, aes(x=p_grid, y=posterior)) + geom_line()+ facet_wrap(~title, ncol = 3)+ xlab(&quot;proportion of water&quot;)+ ylab(&quot;posterior probability density&quot;) Whats shown here is the posterior distributions of a globe-tossing model after 9 tosses. At each step, we take a globe and throw it into the air. We catch it, and note where our index finger ends up on the globe - either on water (W) or land (L). We do this nine times and in this way build up an estimate of the proportion of the Earths surface that is covered by water. The titles of each of the plots above show the sequence that we have observed to that point. We start off by assuming that each proportion is as likely as any other - flat priors. Ive used grid approximation to plot the posteriors in the figure above. Quadratic approximation and Markov chain Monte Carlo are briefly outlined at the end of the chapter. 2.2 Questions 2E1 Question Which of the expressions below correspond to the statement: the probability of rain on Monday? Pr(rain) Pr(rain|Monday) Pr(Monday|rain) Pr(rain,Monday)/ Pr(Monday) Answer Numbers (2) and (4) are equivalent, and both have the meaning the probability of rain, given that it is Monday. 2E2 Question Which of the following statements corresponds to the expression: Pr(Monday|rain)? The probability of rain on Monday. The probability of rain, given that it is Monday. The probability that it is Monday, given that it is raining. The probability that it is Monday and that it is raining. Answer The expression should be read the probability of it being Monday, given that there is rain, which is (3). 2E3 Question Which of the expressions below correspond to the statement: the probability that it is Monday, given that it is raining? Pr(Monday|rain) Pr(rain|Monday) Pr(rain|Monday) Pr(Monday) Pr(rain|Monday) Pr(Monday)/ Pr(rain) Pr(Monday|rain) Pr(rain)/ Pr(Monday) Answer Number (1) is the probability that it is Monday, given that it is raining. Number (4) is equivalent by Bayes rule. 2E4 Question The Bayesian statistician Bruno de Finetti (19061985) began his 1973 book on probability theory with the declaration: PROBABILITY DOES NOT EXIST. The capitals appeared in the original, so I imagine de Finetti wanted us to shout this statement. What he meant is that probability is a device for describing uncertainty from the perspective of an observer with limited knowledge; it has no objective reality. Discuss the globe tossing example from the chapter, in light of this statement. What does it mean to say the probability of water is 0.7? Answer Its very fun that the question what do we mean when we talk about probability is listed as an easy question. Bruno de Finetti was a major figure in the subjective Bayesian school. For de Finetti, when someone says the probability of water is 0.7 they are expressing something about their belief. Maybe that they believe the toy globe is 70% covered in water, or that the real globe is 70% covered in water. Maybe they are expressing something about their expectation for the next toss of the globe. Sometimes subjective Bayesian statements of probability are described in terms of odds that someone would accept on a wager about the belief in question, but this isnt core to the approach (maybe this comes from an overly literal reading of the Dutch Book arguments for the laws of probability). 2M1 Question Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p.  W, W, W W, W, W, L L, W, W, L, W, W, W Answer 2M2 Question Now assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p  0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above. Answer 2M3 Question Suppose there are two globes, one for Earth and one for Mars. The Earth globe is 70% covered in water. The Mars globe is 100% land. Further suppose that one of these globes  you dont know which  was tossed in the air and produced a land observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing land, \\(\\text{Pr}(Earth | Land)\\), is 0.23. Answer Using Bayes Theorem: \\[ \\text{Posterior} = \\frac{\\text{ prior * probability of the data }}{\\text{ average probability of the data }} \\] The numerator for our calculation will be the prior probability that the globe tossed was the Earth, multiplied by the likelihood of seeing land, assuming the globe tossed was the Earth: \\(0.5 * 0.3\\). The denominator is the sum of these probabilities over each value of the prior. In this case, the prior probability of Mars * probability of seeing land given Mars + prior probability of the Earth * probability of seeing land given Earth: 0.5 * 1 + 0.5 * 0.3. So we have \\(\\frac{ 0.5 * 0.3 } { 0.5 * 1 + 0.5 * 0.3 }\\) which is ## [1] 0.2307692 as required. 2M4 Question Suppose you have a deck with only three cards. Each card has two sides, and each side is either black or white. One card has two black sides. The second card has one black and one white side. The third card has two white sides. Now suppose all three cards are placed in a bag and shuffled. Someone reaches into the bag and pulls out a card and places it flat on a table. A black side is shown facing up, but you dont know the color of the side facing down. Show that the probability that the other side is also black is 2/3. Use the counting method (Section 2 of the chapter) to approach this problem. This means counting up the ways that each card could produce the observed data (a black side facing up on the table). Answer We are looking for the posterior probability that we have chosen the black/black card (Card One), given that we see a black side facing up. We assume for now that the chance of choosing each card is the same. That is that for every 1 way of choosing Card One, there is 1 way of choosing Card Two, and 1 way of choosing Card Three. Now we need to count up the number of ways of seeing a black side for each of the cards. Card One (black/black) There are two ways of seeing a black side after choosing this card - we could lay side one face up, or side two. Card Two (black/white) There is only one way of seeing a black side after choosing this card - we would need to lay side one face up. Card Three (white/white) There are no ways of seeing a black side after choosing this card. Our calculation is \\(\\frac{1 * 2}{1 * 2 + 1 * 1 + 1 * 0} = \\frac{2}{2+1+0} = \\frac{2}{3}\\). 2M5 Question Now suppose there are four cards: B/B, B/W, W/W, and another B/B. Again suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black. Answer We are looking for the probability that we have chosen either Card One or Card Four. Following the logic above, we get \\(\\frac{2 + 2}{2 + 2 + 1 + 0} = \\frac{4}{5}\\). 2M6 Question Imagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, its less likely that a card with black sides is pulled from the bag. So again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. Again suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before. Answer We just replace the 1s from 2M4 with the new relative number of ways of pulling each card: B/B: 1 way B/W: 2 ways W/W: 3 ways Our calculation is \\(\\frac{1 * 2}{1 * 2 + 2 * 1 + 3 * 0} = \\frac{2}{2+2+0} = \\frac{1}{2}\\). 2M7 Question Assume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. Hint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card. Answer We are again looking for the probability that the first card we pulled was Card One (b/b), given the data (that we see one card with a black side and then one card with a white side). we will use a table: Cards Prior Count Black Side Count White Side Count Posterior Count Card One 1 2 3 6 Card Two 1 1 2 2 Card Three 1 0 1 0 For each row, we derive the White Side Counts by adding up the number of ways to see a white side for the two cards left in the bag. E.g. if we pulled Card One first, we have one card remaining with two white sides, and one card remaining with one white side = 3 white sides. Our probability that we first pulled Card One is then \\(\\frac{6}{6+2} = 0.75\\) as required. 2H1 Question Suppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. They differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing singleton infants. Assume these numbers are known with certainty, from many years of field research. Now suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins? Answer Species Prior Count Twins Count Posterior Count Posterior Probability Species A 1 1 1 0.33 Species B 1 2 2 0.67 Probability of having twins is \\(\\text{probability of species } A * 0.1 + \\text{probability of species } B * 0.2 = 1/3 * 0.1 + 2/3 * 0.2 = 1/6\\). 2H2 Question Recall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins. Answer This questions implies that there was an easier way of answering the first question. In any case, we have already calculated that the probability of species A is 1/3. 2H3 Question Continuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a singleton infant. Compute the posterior probability that this panda is species A. Answer Species Prior Count Twins Count Singleton Count Posterior Count Posterior Probability Species A 1 1 9 9 0.36 Species B 1 2 8 16 0.64 The probability of species A is now 36%. 2H4 Question A common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test: The probability it correctly identifies a species A panda is 0.8. The probability it correctly identifies a species B panda is 0.65. The vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well. Answer Using test results alone: Species Prior Count Test for A Posterior Count Posterior Probability Species A 1 8 8 0.8 Species B 1 2 2 0.2 Using births and test results: Species Prior Count Twins Count Singleton Count Test for A Posterior Count Posterior Probability Species A 1 1 9 8 72 0.69 Species B 1 2 8 2 32 0.31 Further Reading "],["sampling_imaginary.html", "Chapter 3 Sampling the Imaginary 3.1 Chapter Notes 3.2 Questions Further Reading", " Chapter 3 Sampling the Imaginary 3.1 Chapter Notes This chapter introduces the concept of sampling from the posterior distribution. The first example is the globe tossing model from the previous chapter - this model is straightforward enough to calculate the posterior analytically (or with simple grid approximation), and so when we take samples we can see how well they do at reproducing the expected results. The chapter walks through summary statistics of the posterior distribution, comparing the results found by grid approximation to the results we can get by sampling from the posterior. For example, we can ask how much of the posterior probability lies within some given boundary, or we can ask for confidence intervals (or compatibility intervals, which is the term McElreath prefers). Theres a nice description of the difference between percentile intervals (usually defined so that equal probability mass remains in each tail) and the highest posterior density interval (the narrowest interval that contains the specified probability mass). For highly skewed distributions these can be quite different. Theres a discussion of point estimates - should you choose to report the mean, median, or mode of your posterior distribution? Some other single value? You lose a lot of information by throwing away the posterior in favour of a single value to summarise it. If you have to report a point value, using a loss function is a principled way to determine which point value is most appropriate to your needs. In the next section a big theme of the book is introduced: simulation as a tool in model design and checking. As an example, the chapter simulates observations from the globe tossing model using rbinom(): # using the proportion of water p = 0.7, and assuming nine tosses set.seed(21) sim_single &lt;- rbinom(1e4, size = 9, prob = 0.7) ggplot()+ geom_histogram(aes(sim_single), binwidth = 0.3)+ xlab(&quot;simulated water count&quot;)+ ggtitle(&quot;simulating from a single parameter value&quot;) In the above example, we just picked a single parameter value p=0.7. In this case we know that this is close to the true value of the proportion of the Earths surface that is covered in water. However we wont know the true parameter value in most practical cases. What we should do is sample parameter values from the posterior distribution, and then simulate from the model using those sampled parameter values. Heres a nice image summarising the process of simulating predictions from the posterior: # sampling from the posterior. code from the book. p_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) prior &lt;- rep(1,1000) likelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) # assuming we&#39;ve observed 6 water in 9 tosses posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) samples &lt;- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior ) # instead of prob =0.7, we use values sampled from the posterior set.seed(21) sim_post &lt;- rbinom(1e4, size = 9, prob = samples) ggplot()+ geom_histogram(aes(sim_post), binwidth = 0.3)+ xlab(&quot;simulated water count&quot;)+ ggtitle(&quot;simulating from parameter values sampled from the posterior&quot;) Theres a lot more uncertainty in the distribution, reflecting the uncertainty in the parameter value p. Picking a single parameter value and simulating from that can lead to overconfidence. The posterior is a distribution, not a point estimate, and the uncertainty it embodies should not be thoughtlessly discarded. To close out the chapter there is a section on looking at different statistics from our 9 tosses. Weve so far looked at the number of waters observed, but we could also look at the longest run of water or land, or the number of switches between them. Doing this can help uncover some ways in which our model may be mis-specified. For example the binomial model we use assumes independence between tosses, but we could imagine that there is correlation between the tosses. We can use the same posterior predictive distribution to see how well our model retrodicts the data for these new measures, as well as the water count shown above. 3.2 Questions The Easy problems use the samples from the posterior distribution for the globe tossing example. This code will give you a specific set of samples, so that you can check your answers exactly. p_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) prior &lt;- rep( 1 , 1000 ) likelihood &lt;- dbinom( 6 , size=9 , prob=p_grid ) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) set.seed(100) samples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE ) 3E1 Question How much posterior probability lies below p = 0.2? Answer Heres the code: #Exact answer sum(posterior[p_grid&lt;0.2])*100 #Sample answer sum(samples&lt;0.2)/length(samples)*100 The exact answer is 0.09% Sampling from the posterior suggests 0.04%. 3E2 Question How much posterior probability lies above p = 0.8? Answer Similarly: #Exact answer sum(posterior[p_grid&gt;0.8])*100 #Sample answer sum(samples&gt;0.8)/length(samples)*100 The exact answer is 12.03%. Sampling from the posterior suggests 11.16%. 3E3 Question How much posterior probability lies between p = 0.2 and p = 0.8? Answer Code: #Exact answer sum(posterior[p_grid &gt; 0.2 &amp; p_grid &lt; 0.8])*100 #Sample answer sum(samples &gt; 0.2 &amp; samples &lt; 0.8)/length(samples)*100 The exact answer is 87.88%. Sampling from the posterior suggests 88.8%. 3E4 Question 20% of the posterior probability lies below which value of p? Answer Code: #Exact answer p_grid[[ which(cumsum(posterior)&gt;0.2)[[1]] ]] #Sample answer quantile(samples,0.2) The exact answer suggests that 20% of the posterior probability lies below p = 0.52. The sample answer suggests p = 0.52. 3E5 Question 20% of the posterior probability lies above which value of p? Answer Code: #Exact answer p_grid[[ which(cumsum(posterior)&gt;0.8)[[1]] ]] #Sample answer quantile(samples,0.8) The code checks the value of p such that 80% of the posterior probability lies below p. This is equivalent to 20% lying above p. The exact answer suggests p = 0.76. The sample answer suggests p = 0.76. 3E6 Question Which values of p contain the narrowest interval equal to 66% of the posterior probability? Answer #Sample answer rethinking::HPDI(samples,prob=0.66) The interval (0.51 , 0.77) for p is the narrowest interval that contains 66% of the posterior probability. 3E7 Question Which values of p contain 66% of the posterior probability, assuming equal posterior probability both below and above the interval? Answer #Sample answer quantile(samples,c(0.17,0.83)) #Figures calculated by splitting the remaining probability 0.34 in half above and below the desired interval. #quantile(samples,c( (1-0.66) / 2 ,1 - (1-0.66) / 2)) #alternatively: #rethinking::PI(samples,0.66) The interval (0.5 , 0.77) for p is the interval that contains 66% of the posterior probability, assuming equal posterior probability both below and above the interval. 3M1 Question Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before. Answer p_grid &lt;- seq(0,1,length.out = 1e4) prior &lt;- rep(1,1e4) # This next line contains the only code change likelihood &lt;- dbinom(8,size=15,prob=p_grid) posterior &lt;- prior * likelihood posterior &lt;- posterior/sum(posterior) 3M2 Question Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p. Answer set.seed(100) samples &lt;- sample(p_grid, size=1e4, prob= posterior, replace=TRUE) rethinking::HPDI(samples,0.9) ## |0.9 0.9| ## 0.3379338 0.7208721 The interval (0.34 , 0.72) for p is the narrowest interval that contains 90% of the posterior probability. 3M3 Question Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses? Answer set.seed(100) post_pred &lt;- rbinom(1e4, size=15, prob=samples) sum(post_pred==8)/length(post_pred) ## [1] 0.1473 There is a 14.7% chance of observing 8 water in 15 tosses. 3M4 Question Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses. Answer set.seed(100) post_pred &lt;- rbinom(1e4, size=9, prob=samples) sum(post_pred==6)/length(post_pred) ## [1] 0.1804 There is a 18.0% chance of observing 6 water in 9 tosses. 3M5 Question Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earths surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7. Answer #Recalculate the posterior distribution using the new prior p_grid &lt;- seq(0,1,length.out = 1e4) new_prior &lt;- c(rep(0,sum(p_grid&lt;0.5)) , rep(2,length(p_grid) - sum(p_grid&lt;0.5)) ) likelihood &lt;- dbinom(8,size=15,prob=p_grid) new_posterior &lt;- new_prior * likelihood new_posterior &lt;- new_posterior/sum(new_posterior) #Draw 10,000 samples from the grid approximation. Then use the samples to calculate the 90% HPDI for p. set.seed(100) new_samples &lt;- sample(p_grid, size=1e4, prob= new_posterior, replace=TRUE) rethinking::HPDI(new_samples,0.9) ## |0.9 0.9| ## 0.5000500 0.7152715 The 90% highest probability density interval for p was previously (0.34 , 0.72), now it is (0.5 , 0.72). The HDPI is narrower directly as a consequence of the prior eliminating estimates of p below 0.5. #Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses? set.seed(100) new_post_pred &lt;- rbinom(1e4, size=15, prob=new_samples) sum(new_post_pred==8)/length(new_post_pred) ## [1] 0.1567 #true probability of seeing water 8 times in 15 tosses dbinom(8,15,prob=0.7) ## [1] 0.08113003 There was previously a 14.7% chance of observing 8 water in 15 tosses. Now that probability is 15.7%. The model is trained on data of 8 water observations in 15 tosses, and the probability of reproducing that data from the posterior doesnt seem to change much with our new prior. The true probability is much lower, around 8% (setting p=70%). #Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses. set.seed(100) new_post_pred &lt;- rbinom(1e4, size=9, prob=new_samples) sum(new_post_pred==6)/length(new_post_pred) ## [1] 0.2292 #true probability of seeing water 6 times in 9 tosses dbinom(6,9,prob=0.7) ## [1] 0.2668279 There was previously a 18.0% chance of observing 6 water in 9 tosses. Now that probability is 22.9%. The model with the new prior does a slightly better job of approximating the true probability of 26.7% 3M6 Question Suppose you want to estimate the Earths proportion of water very precisely. Specifically, you want the 99% percentile interval of the posterior distribution of p to be only 0.05 wide. This means the distance between the upper and lower bound of the interval should be 0.05. How many times will you have to toss the globe to do this? Answer I found this one to be quite tricky. I ended up just looping through sample sizes from 1 to 10000 and measuring the width of the 99% confidence interval for each size. One thing thats unsatisfying about this approach is that Ive had to assume a number of successes for each i number of trials. Ive assumed that successes occur roughly in the 8 out of 15 proportion seen in the chapter example, but should it be the case that the number of trials required to get a percentile interval to a target width should be dependent on the proportion of successes? Also, youd think there might be an analytical approach since we know the distribution is binomial, but Ive just brute forced an answer using a loop. p_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) prior &lt;- rep( 1 , 1000 ) width &lt;- list() for (i in seq(1,10000)){ likelihood &lt;- dbinom( round(i*8/15,digits = 0) , size=i , prob=p_grid ) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) set.seed(100) samples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE ) interval &lt;- rethinking::PI(samples,0.99) width[i] &lt;- interval[2]-interval[1] } which(width&lt;0.05)[1] ## [1] 2731 This method suggests 2731 trials should be sufficient. This is one to revisit in the future. 3H1 The Hard problems here all use the data below. These data indicate the gender (male=1, female=0) of officially reported first and second born children in 100 two-child families. birth1 &lt;- c(1,0,0,0,1,1,0,1,0,1,0,0,1,1,0,1,1,0,0,0,1,0,0,0,1,0, 0,0,0,1,1,1,0,1,0,1,1,1,0,1,0,1,1,0,1,0,0,1,1,0,1,0,0,0,0,0,0,0, 1,1,0,1,0,0,1,0,0,0,1,0,0,1,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0, 1,0,1,1,1,0,1,1,1,1) birth2 &lt;- c(0,1,0,1,0,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,0,0,1,1,1,0, 1,1,1,0,1,1,1,0,1,0,0,1,1,1,1,0,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1, 1,1,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,1,1,0,0,0,1,1,1,0,0,0,0) So for example, the first family in the data reported a boy (1) and then a girl (0). The second family reported a girl (0) and then a boy (1). The third family reported two girls. Question Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability? Answer We assume for the moment that gender is independent of birth order. Then we have a binomial distribution with an unknown parameter p, which is the target of our inference. #observations in our data - this will form our likelihood births=length(birth1)+length(birth2) boys=sum(birth1)+sum(birth2) #standard grid approximation approach seen above p_grid &lt;- seq( from=0 , to=1 , length.out=1000 ) prior &lt;- rep( 1 , 1000 ) likelihood &lt;- dbinom(boys , size= births, prob=p_grid ) posterior &lt;- likelihood * prior posterior &lt;- posterior / sum(posterior) #Which parameter value maximizes the posterior probability? #We sample from the posterior as before, and find the mode. set.seed(100) samples &lt;- sample( p_grid , prob=posterior , size=1e4 , replace=TRUE ) rethinking::chainmode(samples) ## [1] 0.5552446 The parameter value that maximises the posterior probability is 55.5%. 3H2 Question Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals. Answer I could have answered the last question analytically by using which.max(posterior)/1000, but since I used samples were well set up for this question. set.seed(100) rethinking::HPDI(samples, 0.5) ## |0.5 0.5| ## 0.5265265 0.5725726 rethinking::HPDI(samples, 0.89) ## |0.89 0.89| ## 0.4994995 0.6076076 rethinking::HPDI(samples, 0.97) ## |0.97 0.97| ## 0.4824825 0.6296296 The narrowest interval for p that contains 50% of the posterior distribution is (0.527 , 0.573). The narrowest interval for p that contains 89% of the posterior distribution is (0.499 , 0.608). The narrowest interval for p that contains 97% of the posterior distribution is (0.482 , 0.630). 3H3 Question Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome? Answer set.seed(100) post_pred &lt;- rbinom(1e4,200, samples) rethinking::simplehist(post_pred) rethinking::dens(post_pred) The model appears to fit the data well, at least in the sense described: that the distribution of predictions includes the actual observation as a central, likely outcome. 3H4 Question Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light? Answer sum(birth1) ## [1] 51 set.seed(100) post_pred &lt;- rbinom(1e4,100, samples) rethinking::simplehist(post_pred) rethinking::dens(post_pred) The model does a much worse job here. The true number of boys in birth1 is 51 out of 100, while the most likely outcomes expected by the model are closer to 55, 56. 3H5 Question The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data? Answer after_girl &lt;- birth2[birth1==0] sum(after_girl) ## [1] 39 set.seed(100) post_pred &lt;- rbinom(1e4,length(after_girl), samples) rethinking::simplehist(post_pred) There were 49 girls born out of 100 births in the birth1 data. We know that these parents went on to have another child. Out of these 49 other children, our model expects the number of boys to be in the region of 27-28. In reality there were 39 boys born after the birth of a girl, well outside the central expectations of the model. No clue whats happening in the data. Theres presumably some selection effect going on, but I cant think of a plausible one. Further Reading Compatibility intervals - Greenland &amp; Gelman, quick discussion paper "],["geocentric.html", "Chapter 4 Geocentric Models 4.1 Chapter Notes 4.2 Questions Further Reading", " Chapter 4 Geocentric Models 4.1 Chapter Notes The chapter opens with an analogy between the geocentric model of the solar system and linear regression - both are useful tools to describe certain natural phenomena, but we should be wary about taking the causal structure they imply literally. The chapter is going to introduce linear regression with uncertainty described using a normal distribution. I.e. models like this: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta x_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp;\\sim \\text{Exponential}(2) \\\\ \\end{aligned} \\] There is a section justifying the use of the normal distribution here by noting how many different ways we might see this pattern in nature - through additive or multiplicative processes. There is also reference to a maximum entropy argument for the normal curve: if all we have are a mean and a variance, the normal distribution is the most appropriate fit with our assumptions. Maximum entropy arguments are described later on in Chapter 10. There is then a section on notation, before the chapter introduces the first linear regression model of the book, a model of human height based on data on the !Kung San collected by Nancy Howell in the 1960s. data(Howell1) data_howell &lt;- as_tibble(Howell1) head(data_howell) ## # A tibble: 6 x 4 ## height weight age male ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 152. 47.8 63 1 ## 2 140. 36.5 63 0 ## 3 137. 31.9 65 0 ## 4 157. 53.0 41 1 ## 5 145. 41.3 51 0 ## 6 164. 63.0 35 1 We keep only data on the adult population, and plot the heights: data_adults &lt;- data_howell %&gt;% filter(age&gt;=18) ggplot(data_adults)+ geom_histogram(aes(x=height), bins = 25) Heres a nice bit about choosing modelling assumptions: These data look rather Gaussian in shape, as is typical of height data. This may be because height is a sum of many small growth factors. As you saw at the start of the chapter, a distribution of sums tends to converge to a Gaussian distribution. Whatever the reason, adult heights from a single population are nearly always approximately normal. So its reasonable for the moment to adopt the stance that the model should use a Gaussian distribution for the probability distribution of the data. But be careful about choosing the Gaussian distribution only when the plotted outcome variable looks Gaussian to you. Gawking at the raw data, to try to decide how tomodel them, is usually not a good idea. The data could be a mixture of different Gaussian distributions, for example, and in that case you wont be able to detect the underlying normality just by eyeballing the outcome distribution. Furthermore, as mentioned earlier in this chapter, the empirical distribution neednt be actually Gaussian in order to justify using a Gaussian probability distribution. We start building up our model: \\[ h_i \\sim \\text{Normal}(\\mu, \\sigma) \\] We are assuming here that every one of the 352 adult heights in the data set \\(h_i\\) can be modelled as if it came from the same normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). A box in the chapter explains that this is a epistemological assumption, not one about the physical world. We know for example that height is correlated within family units. In making the assumption that all the heights are independent and identically distributed we are not claiming that these in-family correlations dont exist, we are only claiming that if we dont know the correlations, the i.i.d. assumption is a reasonable approximation. In our model, \\(\\mu\\) and \\(\\sigma\\) are the parameters we want to estimate, but we start off with some priors: \\[ \\begin{aligned} \\mu &amp;\\sim \\text{Normal}(178, 20) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0,50) \\end{aligned} \\] The prior for \\(\\sigma\\) here is very flat, and the prior for \\(\\mu\\) is a normal distribution centred on 178cm, with a fairly large standard deviation of 20cm. Heres a plot of our prior for the average height parameter \\(\\mu\\): ggplot()+ xlim(100,250)+ geom_function(fun = dnorm, args = list(mean = 178, sd = 20))+ ylab(&quot;Density&quot;)+ ggtitle(expression(&quot;Prior for Average Height&quot; ~ mu)) Heres a plot of the prior for the standard deviation \\(\\sigma\\): ggplot()+ xlim(-10,60)+ geom_function(fun = dunif, args = list(min = 0, max = 50))+ ylab(&quot;Density&quot;)+ ggtitle(expression(&quot;Prior for Standard Deviation&quot; ~ sigma)) We now simulate from our priors to see what they imply about height. This is called prior predictive simulation: sample_mu &lt;- rnorm(1000, mean = 178, sd = 20) sample_sigma &lt;- runif(1000, min=0, max=50) sample_height &lt;- rnorm(1000, mean=sample_mu, sd=sample_sigma) ggplot()+ geom_density(aes(sample_height))+ xlab(&quot;Height(cm)&quot;)+ ylab(&quot;Density&quot;)+ ggtitle(&quot;Model Expectations Before Seeing the Data&quot;) Most of the probability mass here is in the range of plausible human heights, but some is not. The tallest person ever measured, for example, was Robert Wadlow at 272cm. We could justify tightening up our priors, but in this case we have enough data to overwhelm the priors anyway. Here is the model, using Bayes rule: \\[ \\text{Pr}(\\mu, \\sigma | h) = \\frac{\\prod_i \\text{Normal}(h_i | \\mu, \\sigma) \\text{Normal}(\\mu | 178, 20) \\text{Uniform}(\\sigma |0 ,50) }{\\int \\int \\prod_i \\text{Normal}(h_i | \\mu, \\sigma) \\text{Normal}(\\mu | 178, 20) \\text{Uniform}(\\sigma |0 ,50) d\\mu d\\sigma} \\] To get the probability of the parameters given the total data, we compute the probability of the parameters on each \\(h_i\\) individually (using Bayes rule) and then multiply them all together. This is what the product symbol is doing in the top row. 4.1.1 Model Using Grid Approximation The chapter then uses grid approximation to get an estimate of the posterior. I think attempting to recreate this will be a useful exercise to understand conceptually the aim of later calculation methods (quadratic approximation and Markov chain Monte Carlo). # The calculation begins by defining a sequence over the parameter values # I assume 150 to 160 are chosen because McElreath knows that this range will contain almost all # of the probability mass for the average height parameter mu seq_mu &lt;- seq( from=150, to=160 , length.out=100 ) # Similarly for sigma seq_sigma &lt;- seq( from=7 , to=9 , length.out=100 ) # expand.grid gives essentially the Cartesian product of the two sequences in a data frame # they each have 100 entries, so the data frame has 100 x 100 = 10,000 rows post_grid &lt;- as_tibble(expand.grid( mu=seq_mu , sigma=seq_sigma )) # The code in the chapter then uses sapply to produce the log likelihood. # I&#39;m going to try to recreate this using purrr. # For each point in the grid we produce a sum of the log likelihoods of the heights. # Summing log likelihoods is equivalent to multiplying likelihoods. # I.e. For each mu and sigma combination, create a vector length 352 # (the number of heights in the data set) # of log likelihoods, sum that vector, and assign it to that point on the grid post_ll &lt;- post_grid %&gt;% pmap_dbl(.f = function(mu,sigma){sum(dnorm(data_adults$height,mean=mu, sd = sigma, log = TRUE))}) post_grid &lt;- post_grid %&gt;% mutate(log_likelihood = post_ll) # Then weight by the priors by adding the log probability of seeing each mu, sigma combination post_grid &lt;- post_grid %&gt;% mutate(product = log_likelihood + dnorm( post_grid$mu , 178 , 20 , log = TRUE ) + dunif( post_grid$sigma , 0 , 50 , TRUE )) # Finally, we normalise by subtracting the maximum product of likelihoods and priors. # This is equivalent to division because we are still working at the log scale. # Finally we exponentiate to get to raw probabilities again. post_grid &lt;- post_grid %&gt;% mutate(post_prob = exp( product - max(product) )) The only part I dont get yet is why simply using the maximum of the product works to get us to posterior probabilities. Heres what the chapter end note says on this: Finally, the obstacle for getting back on the probability scale is that rounding error is always a threat when moving from log-probability to probability. If you use the obvious approach, like exp( prod ), youll get a vector full of zeros, which isnt very helpful. This is a result of Rs rounding very small probabilities to zero. Remember, in large samples, all unique samples are unlikely. This is why you have to work with log-probability. The code in the box dodges this problem by scaling all of the log-products by the maximum log-product. As a result, the values in post$prob are not all zero, but they also arent exactly probabilities. Instead they are relative posterior probabilities. But thats good enough for what we wish to do with these values. Makes sense. We have a list of numbers between 0 and 1 that are correct in proportion to each other, but we dont have posterior probabilities. I wonder how we would get the actual posterior probabilities. Why wouldnt we be able to sum up all of the likelihood-prior products at each grid point and subtract that? Well we dont have the likelihood-prior products. We have the log versions. And if we try to exponentiate them before scaling they all get set to zero. head(exp(post_grid$product)) ## [1] 0 0 0 0 0 0 So we need to scale them before exponentiating but in order to get the correct scaling factor that will return posterior probabilities we need to exponentiate them. Im sure theres a solution to this but for now Ill continue on using the relative posterior probabilities weve already calculated. Heres a glimpse of our posterior probabilities for the parameters mu and sigma: head(post_grid[,-3:-4]) ## # A tibble: 6 x 3 ## mu sigma post_prob ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 150 7 1.96e-35 ## 2 150. 7 5.35e-34 ## 3 150. 7 1.36e-32 ## 4 150. 7 3.20e-31 ## 5 150. 7 7.00e-30 ## 6 151. 7 1.42e-28 We sample the mu and sigma parameters from the posterior, sampling each in proportion to its posterior probability. Since each posterior probability represents the relative plausibility of the combination of mu and sigma, we shouldnt sample them independently. We sample a row number using the probabilities in column post_prob and then sample the posterior mu and sigma from that row: sample_rows &lt;- sample(1:nrow(post_grid),1e4, replace = TRUE, prob = post_grid$post_prob) sample_mu &lt;- post_grid$mu[sample_rows] sample_sigma &lt;- post_grid$sigma[sample_rows] ggplot()+ geom_bin_2d(aes(x=sample_mu,y=sample_sigma), bins=30)+ ggtitle(&quot;Parameters Sampled from the Posterior&quot;) 4.1.2 Model Using Quadratic Approximation So that was grid approximation. It was useful to be forced to work through the calculation in detail. The chapter now reproduces this model, this time using quadratic approximation instead. The quap function in the Rethinking package takes a model definition in the form of an alist. list_height &lt;- alist( #alist(), unlike list(), does not try to evaluate its entries height ~ dnorm( mu , sigma ) , mu ~ dnorm( 178 , 20 ) , sigma ~ dunif( 0 , 50 ) ) m4_1 &lt;- quap( list_height , data=data_adults ) One of my goals in working through the book in this way is to become familiar with using Stan and tidybayes. The book wont introduce Markov chain Monte Carlo until later, but I can start working with tidybayes from this point, because the tidybayes.rethinking package works with models created with the Rethinking package, which is where the function quap comes from. Heres the rethinking vignette for tidybayes. From the vignette: The [tidybayes::]spread_draws() function yields a common format for all model types supported by tidybayes. It lets us instead extract draws into a data frame in tidy format, with a .chain and .iteration column storing the chain and iteration for each row (if available  these columns are NA for rethinking::quap() and rethinking::map() models), a .draw column that uniquely indexes each draw, and the remaining columns corresponding to model variables or variable indices. The spread_draws() function accepts any number of column specifications, which can include names for variables and names for variable indices. post_quap &lt;- m4_1 %&gt;% spread_draws(mu,sigma) head(post_quap,10) # displays the first 10 of 5,000 rows ## # A tibble: 10 x 5 ## .chain .iteration .draw mu sigma ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NA NA 1 154. 7.49 ## 2 NA NA 2 155. 7.61 ## 3 NA NA 3 155. 7.09 ## 4 NA NA 4 154. 7.53 ## 5 NA NA 5 155. 7.61 ## 6 NA NA 6 155. 7.91 ## 7 NA NA 7 155. 7.39 ## 8 NA NA 8 155. 7.75 ## 9 NA NA 9 155. 7.39 ## 10 NA NA 10 154. 7.52 Heres a summary of the posterior for our parameters using the point and interval summary functions from tidybayes: set.seed(74) m4_1 %&gt;% gather_draws(mu, sigma) %&gt;% median_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.89 median qi ## 2 sigma 7.74 7.27 8.20 0.89 median qi Here we return the parameter median in the .value column, and an 89% equi-tailed interval for our two parameters, in the .lower and .higher columns. We can instead return the mean or mode in the .value column: set.seed(74) m4_1 %&gt;% gather_draws(mu, sigma) %&gt;% mode_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.89 mode qi ## 2 sigma 7.72 7.27 8.20 0.89 mode qi The .value column has changed, but the .lower and .higher columns still specify the same 89% equi-tailed interval. If we instead want to return the highest density interval we can do that by replacing qi in the function with hdi: set.seed(74) m4_1 %&gt;% gather_draws(mu, sigma) %&gt;% median_hdi(.width=0.89) ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.89 median hdi ## 2 sigma 7.74 7.27 8.20 0.89 median hdi The summary function names are really intuitive, but if we dont want to remember them we can use the point_interval() function only with arguments .point = mean/mode/median and .interval = qi/hdi. The posterior figures above are those produced by quadratic approximation. This means that they represent Gaussian approximations for the marginal distribution of each parameter. This works fine when the posterior is broadly Gaussian. Here we compare the results from quap: m4_1 %&gt;% gather_draws(mu, sigma) %&gt;% median_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 2 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 mu 155. 154. 155. 0.89 median qi ## 2 sigma 7.73 7.26 8.18 0.89 median qi To the results from grid approximation: print(&quot;mu&quot;, quote = FALSE) ## [1] mu quantile(sample_mu, probs=c(0.055,0.945)) # the lower and upper values of an 89% interval ## 5.5% 94.5% ## 153.9394 155.2525 print(&quot;sigma&quot;, quote = FALSE) ## [1] sigma quantile(sample_sigma, probs=c(0.055,0.945)) ## 5.5% 94.5% ## 7.323232 8.252525 These are very close, especially for mu. We would expect this since mu is the outcome of a Gaussian likelihood and prior, whereas sigma was given a uniform prior with lower bound at zero. We might expect then that it would have a longer right tail than the quadratic approximation allows. 4.1.3 Adding Predictors We are going to use the weights in our data set to predict heights: ggplot(data_adults)+ geom_point(aes(x=weight, y=height))+ ggtitle(&quot;!Kung San Adults&quot;) Heres the chapter: The strategy is to make the parameter for the mean of a Gaussian distribution, \\(\\mu\\), into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the linear model. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. The golem then computes the posterior distribution of this constant relationship. Heres the previous model: \\[ \\begin{aligned} h_i &amp;\\sim \\text{Normal}(\\mu, \\sigma)\\\\ \\mu &amp;\\sim \\text{Normal}(178, 20) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0,50) \\end{aligned} \\] And heres the model including weight as a predictor: \\[ \\begin{aligned} h_i &amp;= \\text{Normal}(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta(x_i - \\bar{x}) \\\\ \\alpha &amp;\\sim \\text{Normal}(178, 20) \\\\ \\beta &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0,50) \\end{aligned} \\] Where each \\(x_i\\) is the predictor (weight) associated with each observed height \\(h_i\\). We have new parameters \\(\\alpha\\) and \\(\\beta\\). The intercept and slope of our linear model. What do our priors imply about them? We use prior predictive simulation: # we generate 100 samples from our prior distributions prior_sim &lt;- tibble(a=rnorm(100, mean = 178, sd=20), b = rnorm(100, mean = 0, sd = 10)) # we plot the lines they imply ggplot(prior_sim)+ geom_abline(aes(slope = b, intercept = a - b*mean(data_adults$weight) ))+ # explained below xlim(min(data_adults$weight),max(data_adults$weight))+ ylim(-10, 300)+ geom_hline(yintercept = 272, colour = &quot;red&quot;)+ # 272cm is Robert Wadlow&#39;s height geom_hline(yintercept = 0, colour = &quot;red&quot;)+ xlab(&quot;weight (kg)&quot;)+ ylab(&quot;average height (cm)&quot;)+ ggtitle(&quot;b ~ Normal(0,10)&quot;) For the slope of each line Ive used the slope we generated from the prior. For the intercept I used \\[ \\text{intercept = a - b*mean(weight)} \\] The reason for this is that I want the x-axis to vary over a natural range of the parameter weight to be more interpretable. And so instead of considering \\[ \\mu_i = \\alpha + \\beta(x_i - \\bar{x}) \\] over the range \\((\\min( x_i) - \\bar{x},\\max( x_i) - \\bar{x})\\), Ive looked at the equivalent line \\[ \\mu_i = (\\alpha - \\beta\\bar{x}) + \\beta(x_i ) \\] over the range \\((\\min( x_i),\\max( x_i)\\). The plot makes clear that our priors are very permissive. They allow impossible (e.g. negative) heights to be associated with observed weights. Also, just as many lines suggest that height may be negatively correlated with height as suggest a positive correlation. The chapter also trials the more informative prior \\[ \\beta \\sim \\text{Log-Normal}(0,1) \\] This forces the relationship between weight and height to be positive. It also clusters the range of expected average height more tightly - limiting them for the most part to the range of plausible values. Heres the same plot as before with the new prior: # we generate 100 samples from our prior distributions prior_sim_log &lt;- tibble(a=rnorm(100, mean = 178, sd=20), b = rlnorm(100, meanlog = 0, sdlog = 1)) # we plot the lines they imply ggplot(prior_sim_log)+ geom_abline(aes(slope = b, intercept = a - b*mean(data_adults$weight) ))+ # explained above xlim(min(data_adults$weight),max(data_adults$weight))+ ylim(-10, 300)+ geom_hline(yintercept = 272, colour = &quot;red&quot;)+ # 272cm is Robert Wadlow&#39;s height geom_hline(yintercept = 0, colour = &quot;red&quot;)+ xlab(&quot;weight (kg)&quot;)+ ylab(&quot;average height (cm)&quot;)+ ggtitle(&quot;b ~ Log-Normal(0,1)&quot;) Heres the model well use: \\[ \\begin{aligned} h_i &amp;= \\text{Normal}(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta(x_i - \\bar{x}) \\\\ \\alpha &amp;\\sim \\text{Normal}(178, 20) \\\\ \\beta &amp;\\sim \\text{Log-Normal}(0, 1) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0,50) \\end{aligned} \\] We feed it to quap and create a table of the parameters using tidybayes: xbar &lt;- mean(data_adults$weight) m4_3 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b*( weight - xbar ) , a ~ dnorm( 178 , 20 ) , b ~ dlnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) , data= data_adults) m4_3 %&gt;% gather_draws(a, b, sigma) %&gt;% median_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 3 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 155. 154. 155. 0.89 median qi ## 2 b 0.902 0.835 0.969 0.89 median qi ## 3 sigma 5.07 4.76 5.38 0.89 median qi Interpreting these: An \\(\\alpha\\) of 154.6cm means that a person of average weight is expected to be 154.6cm tall A \\(\\beta\\) of 0.9 means that an increase in weight of 1kg is associated with a increase in height of 0.9cm We plot draws from the posterior (black lines) over the data (blue dots): #extract 20 draws from the posterior post_m4_3 &lt;- m4_3%&gt;%spread_draws(a,b,ndraws = 20) ggplot()+ geom_point(data=data_adults, aes(x=weight, y=height), colour = &quot;light blue&quot;)+ geom_abline(data=post_m4_3,aes( slope = b, intercept = a - b*mean(data_adults$weight) ),alpha=0.5)+ ggtitle(&quot;Posterior Samples Drawn over the Data&quot;) How can we display the uncertainty in our model? In the plot above, Ive used 20 draws of \\(\\alpha\\) and \\(\\beta\\) to manually plot the expected height over the range of weights seen in the data. There is uncertainty about this mean height, and we can display that by plotting many lines as above, or we can use the following, more flexible approach using tidybayes. This doesnt require the manual calculation of the \\(\\mu\\)s that is implicit in the code for the graph above. This will be helpful when we move on to more complicated models. # use modelr::data_grid to create a grid of 51 weights over the range found in the data data_grid(data_adults,weight=seq_range(weight, n = 51)) %&gt;% add_linpred_draws(m4_3) %&gt;%# add draws from the posterior link-level predictor - the expected heights # 5,000 draws x 51 weights = 255,000 rows in this tibble ggplot(aes(x = weight, y=height)) + stat_lineribbon(aes(y = .linpred)) + geom_point(data = data_adults, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) The shaded regions display the uncertainty in the expected heights for each weight. However this is not the only source of uncertainty in our model. We also have uncertainty around the mean heights, represented by \\(\\sigma\\) in the model specification: \\[ h_i = \\text{Normal}(\\mu_i, \\sigma) \\] We can display this in a similar way: data_grid(data_adults,weight=seq_range(weight, n = 51)) %&gt;% add_predicted_draws(m4_3) %&gt;%# add draws from the posterior predictive - the posterior distribution of heights ggplot(aes(x = weight, y=height)) + stat_lineribbon(aes(y = .prediction)) + geom_point(data = data_adults, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) There is a lot more uncertainty here. The key difference in the above two chunks of code is which tidybayes function is used to create the draws, linpred_draws() or predicted_draws(). The function linpred_draws() draws from the posterior distribution of the linear predictor. I.e. it draws from the distribution of expected heights in the model. Whereas predicted_draws() draws from the entire posterior distribution. To do the same with Stan models later, well need to use the tidybayes function add_draws() along with rstantools functions like posterior_epred() and posterior_predict(). Documentation for tidybayes::add_draws Documentation for rstantools functions 4.1.4 Polynomial Regression Heres a model from the chapter that uses a quadratic link function. It is used to fit the height on weight model over the whole dataset, not just the adults: \\[ \\begin{aligned} h_i &amp;= \\text{Normal}(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta_1x_i + \\beta_1x_i^2\\\\ \\alpha &amp;\\sim \\text{Normal}(178, 20) \\\\ \\beta_1 &amp;\\sim \\text{Log-Normal}(0, 1) \\\\ \\beta_2 &amp;\\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp;\\sim \\text{Uniform}(0,50) \\end{aligned} \\] data_howell &lt;- data_howell%&gt;% mutate(weight_s = (weight - mean(weight))/sd(weight), weight_s2 = weight_s^2) m4_5 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s + b2*weight_s2 , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , b2 ~ dnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) , data=data_howell ) m4_5 %&gt;% gather_draws(a, b1, b2, sigma) %&gt;% median_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 4 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 146. 145. 147. 0.89 median qi ## 2 b1 21.7 21.2 22.2 0.89 median qi ## 3 b2 -7.81 -8.25 -7.36 0.89 median qi ## 4 sigma 5.77 5.48 6.06 0.89 median qi Heres a plot of draws from the posterior: data_grid(data_howell,weight_s=seq_range(weight_s, n = 51)) %&gt;% # grid of standardised weights to range over mutate(weight_s2 = weight_s^2)%&gt;% # the add_predicted_draws() function expects both weight_s and weight_s2 add_predicted_draws(m4_5) %&gt;% mutate(weight = weight_s * sd(data_howell$weight) + mean(data_howell$weight) )%&gt;% # reversing the standardisation ggplot(aes(x = weight, y=height)) + stat_lineribbon(aes(y = .prediction)) + geom_point(data = data_howell, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) 4.1.5 B-Splines The B stands for basis. Heres the data for the cherry blossom example used in this section: data(cherry_blossoms) data_blossoms &lt;- as_tibble(cherry_blossoms)%&gt;% filter(is.na(doy)==FALSE) Creating knots for the splines: num_knots &lt;- 15 # number of knots knot_list &lt;- quantile(data_blossoms$year , probs=seq(0,1,length.out=num_knots) ) Constructing the basis functions for a cubic spline using the splines package: basis &lt;- bs(data_blossoms$year, knots=knot_list[-c(1,num_knots)] , # function for generating the basis matrix degree=3 , intercept=TRUE ) # we don&#39;t want this as a tibble. We&#39;ll do matrix multiplication with it later on. Heres the model for the day in the year when the cherry blossoms appear over time: \\[ \\begin{aligned} D &amp;= \\text{Normal}(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha + \\sum^K_{k=1}w_kB_{k,i}\\\\ \\alpha &amp;\\sim \\text{Normal}(100, 10) \\\\ w_j &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] A description from the chapter: That linear model might look weird. But all it is doing is multiplying each basis value by a corresponding parameter wk and then adding up all K of those products. This is just a compact way of writing a linear model. The rest should be familiar. Using the quap engine: m4_7 &lt;- quap( alist( D ~ dnorm( mu , sigma ) , mu &lt;- a + B %*% w , a ~ dnorm(100,10), w ~ dnorm(0,10), sigma ~ dexp(1) ), data=list( D=data_blossoms$doy , B=basis ) , start=list( w=rep( 0 , ncol(basis) ) ) ) Im not familiar enough with tidybayes yet to figure out how to get draws out of this model. I could use the tools in the rethinking package, but these wouldnt transfer as well to the Stan models later in the book. Will revisit this once I know tidybayes better. 4.2 Questions 4E1 Question In the model definition below, which line is the likelihood? \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] Answer \\(y_i  \\text{Normal}(µ, )\\) 4E2 Question In the model definition just above, how many parameters are in the posterior distribution? Answer Two parameters, \\(\\mu\\) &amp; \\(\\sigma\\). \\(y_i\\) is not a parameter, its the observed data. 4E3 Question Using the model definition above, write down the appropriate form of Bayes theorem that includes the proper likelihood and priors. Answer In Bayes theorem, we want to end up with the probability of some hypothesis, given some data. In this case, our hypotheses are values for parameters \\(\\mu\\) and \\(\\sigma\\). The probability of seeing the data (\\(y_i\\)) that we do comes from our likelihood, in this case weve assumed the data is the result of a normal distribution. Lets say we want to find the probability that our parameter values are \\(\\hat\\mu\\) and \\(\\hat\\sigma\\) given some piece of data \\(y_i\\). We apply Bayes theorem like this: \\[ \\begin{aligned} P(\\hat\\mu,\\hat\\sigma | y_i) &amp;= \\frac{P(y_i | \\hat\\mu, \\hat\\sigma)P(\\hat\\mu)P(\\hat\\sigma)}{\\int\\int P(y_i | \\mu, \\sigma)P(\\mu)P(\\sigma) d\\mu d\\sigma} \\\\ \\\\ &amp;= \\frac{N(y_i | \\hat\\mu, \\hat\\sigma)N(\\hat\\mu | 0,10)\\text{Exp}(\\hat\\sigma|1)}{\\int\\int N(y_i | \\mu, \\sigma)N(\\mu | 0,10)\\text{Exp}(\\sigma|1) d\\mu d\\sigma} \\end{aligned} \\] I mean \\(N(y_i | \\mu, \\sigma)\\) to be read the probability of observing \\(y_i\\) given that it is normally distributed with parameters \\(\\mu\\) &amp; \\(\\sigma\\). That notation is copied from page 78. 4E4 Question In the model definition below, which line is the linear model? \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta x_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta &amp;\\sim \\text{Normal}(0, 1) \\\\ \\sigma &amp;\\sim \\text{Exponential}(2) \\\\ \\end{aligned} \\] Answer \\(\\mu_i = \\alpha + \\beta x_i\\) This is the assertion that \\(\\mu_i\\) is a linear function of \\(x_i\\). 4E5 Question In the model definition just above, how many parameters are in the posterior distribution? Answer Three parameters, \\(\\alpha\\), \\(\\beta\\) &amp; \\(\\sigma\\). 4M1 Question For the model definition below, simulate observed \\(y\\) values from the prior (not the posterior). \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu &amp;\\sim \\text{Normal}(0, 10) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\\\ \\end{aligned} \\] Answer num_obs &lt;- 1e4 sim_prior &lt;- rnorm(num_obs, mean=rnorm(num_obs, mean=0, sd = 10) , sd=rexp(num_obs, rate = 1)) ggplot()+ geom_density(aes(x=sim_prior)) 4M2 Question Translate the model just above into a quap formula. Answer y ~ dnorm(mu, sigma) mu ~ dnorm(0, 10) sigma ~ dexp(1) 4M3 Question Translate the quap model formula below into a mathematical model definition. y ~ dnorm( mu , sigma ), mu &lt;- a + b*x, a ~ dnorm( 0 , 10 ), . b ~ dunif( 0 , 1 ), sigma ~ dexp( 1 ) 4.2.0.1 Answer \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta x_i\\\\ \\alpha &amp;\\sim \\text{Normal}(0, 10) \\\\ \\beta &amp; \\sim \\text{Uniform}(0,1) \\\\ \\sigma &amp; \\sim \\text{Exponential}(1) \\end{aligned} \\] 4M4 Question A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors. Answer hi  Normal(µ, ) µi =  + yi  ~ Normal(178, 20)   Normal(0,10)   Exponential(0.05) 4M5 Question Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How? Answer Yes, I would revise  to something like   Exponential(0.2). Now it can only be positive (before I wasnt sure if we were following the same students, or the same class with a new intake of students). Probably still anticipating too much height growth with this prior, assuming these are university students. On the other hand Dennis Rodman grew like 8 inches one summer after high school apparently so want to keep open the possibility. 4M6 Question Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors? Answer I think my previous prior   Exponential(0.05) is probably still fine. If anything this question makes me think I wasnt being conservative enough with my first choice of priors. 4M7 Question Refit model m4.3 from the chapter, but omit the mean weight xbar this time. Compare the new models posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models. Answer data(Howell1) d &lt;- Howell1 d2 &lt;- d[ d$age &gt;= 18 , ] # define the average weight, x-bar xbar &lt;- mean(d2$weight) # fit model set.seed(100) m4.3 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b*( weight - xbar ) , a ~ dnorm( 178 , 20 ) , b ~ dlnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) , data=d2 ) set.seed(100) m4.3.2 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b*( weight ) , a ~ dnorm( 178 , 20 ) , b ~ dlnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) , data=d2 ) Chapter questions unfinished. Further Reading This is the first chapter that really refers to E. T. Jaynes in detail. I really want to read Probability Theory: The Logic of Science. Im not sure how mathematically dense it is but will maybe hold off until summer, once Ive seen some measure theory and worked through MacKays book on information theory. "],["many_variables.html", "Chapter 5 The Many Variables &amp; The Spurious Waffles 5.1 Chapter Notes 5.2 Questions Further Reading", " Chapter 5 The Many Variables &amp; The Spurious Waffles 5.1 Chapter Notes This chapter introduces multiple regression and causal models. Will try to race through it, just getting some practice visualising priors, using tidybayes to display model uncertainty etc. Model 5.1: Modelling divorce rate on age at marriage. \\[ \\begin{aligned} D_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_A A_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 0.2) \\\\ \\beta_A &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\\\ \\end{aligned} \\] Here are the variables well use in this model and later on: \\(D_i\\) - the 2009 divorce rate per 1,000 adults for each U.S. state. \\(A_i\\) - The 2005-2010 median age at marriage for each state Plot of these priors: # we generate 100 samples from our prior distributions prior_sim_5_1 &lt;- tibble(a=rnorm(100, mean = 0, sd=0.2), bA = rnorm(100, mean = 0, sd = 0.5)) # and plot the lines they imply ggplot(prior_sim_5_1)+ geom_abline(aes(slope = bA, intercept = a))+ xlim(-3,3)+ ylim(-3,3)+ xlab(&quot;Median Age of Marriage (std)&quot;)+ ylab(&quot;Divorce Rate (std)&quot;) Heres the model and the posterior mean/expected divorce rate: data(WaffleDivorce) data_divorce &lt;- WaffleDivorce%&gt;%as_tibble() data_divorce %&lt;&gt;% mutate( divorce_s = (Divorce - mean(Divorce))/sd(Divorce), marriage_s = (Marriage - mean(Marriage))/sd(Marriage), age_s = (MedianAgeMarriage - mean(MedianAgeMarriage))/sd(MedianAgeMarriage), ) m5_1 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bA * age_s , a ~ dnorm( 0 , 0.2 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_1 &lt;- m5_1 %&gt;% gather_draws(a, bA, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_1&quot;) data_grid(data_divorce,age_s=seq_range(age_s, n = 51)) %&gt;% add_linpred_draws(m5_1) %&gt;% # add draws for mean divorce rate mutate(MedianAgeMarriage= age_s * sd(data_divorce$MedianAgeMarriage) + mean(data_divorce$MedianAgeMarriage), Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%&gt;% ggplot(aes(x = MedianAgeMarriage, y=Divorce)) + stat_lineribbon() + geom_point(data = data_divorce, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) Heres the same plot for model 5.2 - which uses marriage rate as a predictor instead of age at marriage: m5_2 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bM * marriage_s , a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_2 &lt;- m5_2 %&gt;% gather_draws(a, bM, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_2&quot;) data_grid(data_divorce,marriage_s=seq_range(marriage_s, n = 51)) %&gt;% add_linpred_draws(m5_2) %&gt;% # add draws for mean divorce rate mutate(Marriage= marriage_s * sd(data_divorce$Marriage) + mean(data_divorce$Marriage), Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%&gt;% ggplot(aes(x = Marriage, y=Divorce)) + stat_lineribbon() + geom_point(data = data_divorce, colour = &quot;light blue&quot;) + scale_fill_brewer(palette = &quot;Greys&quot;) The chapter goes on to introduce DAGs and causal reasoning, before coming back to define a regression model that uses both marriage rate and median age at marriage as predictors. Here are a couple of possible DAGs for how marriage rate and median age of marriage influence the divorce rate: par(mar = c(4, 4, .1, .1)) dag_div1 &lt;- dagitty(&quot;dag{A -&gt; M; M -&gt; D; A -&gt; D }&quot;) coordinates(dag_div1) &lt;- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1)) drawdag(dag_div1) dag_div2 &lt;- dagitty(&quot;dag{A -&gt; M; A -&gt; D }&quot;) coordinates(dag_div2) &lt;- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1)) drawdag(dag_div2) We can use our data to test these proposed causal models, because they make different predictions. The first has no conditional independencies: impliedConditionalIndependencies(dag_div1) The second predicts that D is independent from M, conditional on A: impliedConditionalIndependencies(dag_div2) ## D _||_ M | A Heres our statistical model: \\[ \\begin{aligned} D_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_M M_i + \\beta_A A_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0, 0.2) \\\\ \\beta_M &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\beta_A &amp;\\sim \\text{Normal}(0, 0.5) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\\\ \\end{aligned} \\] m5_3 &lt;- quap( alist( divorce_s ~ dnorm( mu , sigma ) , mu &lt;- a + bM * marriage_s + bA * age_s , a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) , data = data_divorce ) summary_5_3 &lt;- m5_3 %&gt;% gather_draws(a, bM, bA, sigma) %&gt;% median_qi(.width=0.89)%&gt;% mutate(model = &quot;m5_3&quot;) summary_5_3%&gt;% print(digits = 3) ## # A tibble: 4 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 0.00307 -0.154 0.157 0.89 median qi m5_3 ## 2 bA -0.614 -0.861 -0.378 0.89 median qi m5_3 ## 3 bM -0.0661 -0.310 0.170 0.89 median qi m5_3 ## 4 sigma 0.784 0.662 0.908 0.89 median qi m5_3 Heres a plot for the three models: bind_rows(summary_5_1, summary_5_2,summary_5_3) %&gt;% ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) + geom_pointinterval(position = position_dodge(width = -0.3))+ ylab(&quot;parameters&quot;) As predicted by the second DAG, marriage rate is not associated with divorce rate once you condition on age of marriage. The chapter then describes three plots to help visualise inferences from more complicated models then the simple one predictor models from chapter 4. Predictor residual plots Posterior prediction plots Counterfactual plots For now Ill only recreate the posterior prediction plot. plot_data_m5_3 &lt;- data_divorce%&gt;% select(Loc, divorce_s, marriage_s, age_s)%&gt;% add_linpred_draws(m5_3)%&gt;% # add the predicted mean divorce rates group_by(Loc, divorce_s)%&gt;% mean_qi(x=.linpred, .width = 0.89)%&gt;% # summarise each group in the df print(digits = 3) ## # A tibble: 50 x 8 ## Loc divorce_s x .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AK 1.54 0.317 -0.00212 0.642 0.89 mean qi ## 2 AL 1.65 0.371 0.159 0.579 0.89 mean qi ## 3 AR 2.09 0.755 0.415 1.09 0.89 mean qi ## 4 AZ 0.611 0.122 -0.0386 0.284 0.89 mean qi ## 5 CA -0.927 -0.351 -0.544 -0.160 0.89 mean qi ## 6 CO 1.05 0.114 -0.118 0.346 0.89 mean qi ## 7 CT -1.64 -0.711 -0.983 -0.440 0.89 mean qi ## 8 DC -1.86 -1.76 -2.41 -1.13 0.89 mean qi ## 9 DE -0.433 -0.324 -0.633 -0.00115 0.89 mean qi ## 10 FL -0.652 -0.116 -0.335 0.104 0.89 mean qi ## # ... with 40 more rows head(plot_data_m5_3) # x is mean predicted divorce rate, .lower and .upper are 0.89 confidence interval limits ## # A tibble: 6 x 8 ## Loc divorce_s x .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AK 1.54 0.317 -0.00212 0.642 0.89 mean qi ## 2 AL 1.65 0.371 0.159 0.579 0.89 mean qi ## 3 AR 2.09 0.755 0.415 1.09 0.89 mean qi ## 4 AZ 0.611 0.122 -0.0386 0.284 0.89 mean qi ## 5 CA -0.927 -0.351 -0.544 -0.160 0.89 mean qi ## 6 CO 1.05 0.114 -0.118 0.346 0.89 mean qi ggplot(plot_data_m5_3)+ geom_pointinterval(mapping=aes(x=divorce_s, y=x,ymin=.lower,ymax=.upper), orientation = &quot;vertical&quot;, colour = &#39;#EE8866&#39;)+ xlab(&quot;Observed Divorce Rate (std)&quot;)+ ylab(&quot;Predicted Mean Divorce Rate (std)&quot;)+ geom_abline(slope=1,intercept=0)+ geom_text(data= plot_data_m5_3%&gt;% filter(Loc %in% c(&quot;ID&quot;,&quot;UT&quot;,&quot;RI&quot;,&quot;ME&quot;)), aes(x= divorce_s, y= x, label = Loc ), nudge_x = -0.1) States above the line have lower divorce rates than the model expects, given marriage rate and median age at marriage. States below the line have a higher divorce rate than the model expects. There is a section in the chapter on variables that mask the influence of each other on the outcome of interest. A case study of primate milk and brain size is used to illustrate. There is a section on using categorical data in regression models. The key point in this section is the argument for index variables over indicators variables when coding categorical variables. With indicator variables you might look at the !Kung San data from the previous chapter, and decide to code men as 1 and women as 0 in a variable called \\(m\\). Your model for height might look like: \\[ \\begin{aligned} h_i &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_m m_i \\end{aligned} \\] One problem with this is that the parameter \\(\\beta_m\\) now means the expected difference in height between men and women, and it may be difficult to set a prior. This approach also assumes that there is more uncertainty about male height than female height, since \\(\\beta_m\\) and its distribution doesnt enter into our predictions for the height of women. An alternative approach is to use index variables. You might code the !Kung San men as 1 and the women as 2 and your model might look like: \\[ \\begin{aligned} h_i &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha_{\\text{sex}[i]} \\end{aligned} \\] We have a vector of two intercept parameters and select either \\(\\alpha_1\\) or \\(\\alpha_2\\) depending on the index at each row of data. The same prior can be assigned to each, and neither is inherently more uncertain than the other. The index approach also extends much more simply to more than two categories. 5.2 Questions 5E1 Question Which of the linear models below are multiple linear regressions? µi =  + xi µi = x * xi + z * zi µi =  + (xi  zi) µi =  + x * xi + z * zi Answer Number 4 looks the most like the multiple regressions in the chapter: µ is regressed on both xi and zi with the intercept . (2) is just (4) with the  set to zero, so that counts too. Number 1 is just a bivariate regression. Number 3 is interesting. I think this is not really a multiple regression, even though there are two variables. Rather than attempting to determine separately the influence of x and z on µ, you are asserting in the model that they have and equal and opposite impact. I think this is not really what you want a multiple regression to do, but dont feel confident about my answer. 5E2 Question Write down a multiple regression to evaluate the claim: Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition. Answer Oh boy. This question immediately feels like a trap with animal diversity is linearly related to latitude. Surely if I choose to use a multiple linear regression with two variables, and control for one, the only relationships Ill observe will be linear. Im going to ignore the linearly part of the question from this point. It seems like the claim is that if I naively regress animal diversity on to latitude without accounting for plant diversity, I would find no relationship. I.e. that the relationship between latitude and animal diversity is masked. If that interpretation is correct, I would start with a bivariate model. $$ A_i Normal(, ) \\ _i = + _L*L $$ Where if the claim is true I would expect to see little relationship. I would then move on to a multiple regression including plant diversity: $$ A_i Normal(, ) \\ _i = + _L*L +_P * P $$ and examine whether it appears as if a relationship has now emerged. 5E3 Question Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. Write down the model definition and indicate which side of zero each slope parameter should be on. Answer $$ T_i Normal(, ) \\ _i = + _F*F +_S * S $$ T - time to PhD degree F - amount of funding S - size of laboratory 5E4 Question Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let \\(A_i\\) be an indicator variable that is 1 where case i is in category A. Also suppose \\(B_i\\), \\(C_i\\), and \\(D_i\\) for the other categories. Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when its possible to compute one posterior distribution from the posterior distribution of another model. \\[ \\begin{aligned} (1) \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i \\\\ (2) \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\ (3) \\mu_i &amp;= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\ (4) \\mu_i &amp;= \\alpha_A A_i + _B B_i + _C C_i + _D D_i \\\\ (5) \\mu_i &amp;= \\alpha_A(1  B_i  C_i D_i) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i \\end{aligned} \\] Answer 1 is the standard indicator variable approach. Where A, B and D are equal to 0, \\(\\alpha\\) is the mean \\(\\mu\\) where the predictor is at level C. 2 There is redundancy in two, surely it wouldnt be possible to estimate \\(\\alpha\\) - it can take any value and produce the same \\(\\mu\\) so long as the appropriate \\(\\beta_x\\) adjusts to compensate. I dont know if that means it is not inferentially equivalent though. 3 is clearly equivalent to (1), it doesnt make a difference (except for interpretation) which of the levels you label null. 4 Is equivalent also, just set \\(\\alpha_c\\) equal to \\(\\alpha\\) from (1). 5 Is an incredibly annoying way to set up your model, but can be pretty easily transformed into (3) with some algebra and relabelling: \\[ \\begin{aligned} \\mu_i &amp;= \\alpha_A(1  B_i  C_i D_i) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i \\\\ &amp;= \\alpha_A + (\\alpha_B-\\alpha_A)B_i + (\\alpha_C -\\alpha_A)C_i +(\\alpha_D-\\alpha_A)D_i \\\\ &amp;= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\end{aligned} \\] Further Reading "],["haunted_dag.html", "Chapter 6 The Haunted DAG &amp; The Causal Terror 6.1 Chapter Notes 6.2 Questions Further Reading", " Chapter 6 The Haunted DAG &amp; The Causal Terror 6.1 Chapter Notes The chapter opens with an example of a selection effect. Assuming that journal editors and peer reviewers care about both novelty and rigour, we can expect to see a negative correlation between the two in published papers. Heres a simulation and a chart: set.seed(1914) N &lt;- 200 # we simulate 200 papers p &lt;- 0.1 nw &lt;- rnorm(N) # we create a newsworthiness figure for each paper tw &lt;- rnorm(N) # we create a trustworthiness figure for each paper # note that there is no correlation between the two in the simualation s &lt;- nw + tw q &lt;- quantile( s , 1-p ) # the top 10% of papers are accepted selected &lt;- ifelse( s &gt;= q , &quot;Yes&quot; , &quot;No&quot; ) # are the papers selected? i.e. published data_pub &lt;- tibble(Trustworthiness = tw, Newsworthiness= nw,Selected =selected) ggplot(mapping=aes(Newsworthiness,Trustworthiness,colour=Selected))+ geom_point(data=data_pub)+ geom_smooth(data=filter(data_pub,Selected==&quot;Yes&quot;),method=lm,formula =&#39;y ~ x&#39;,se=FALSE) The blue line is the negative correlation between newsworthiness and trustworthiness induced by the selection process. This effect can occur inside of a multiple regression, where it is called collider bias. Multicollinearity How do multiple regression models behave when the predictor variables are highly correlated? The chapter contains a simulation for the effect of leg length on height. set.seed(909) N &lt;- 100 # we have 100 people height &lt;- rnorm(N,10,2) # we simulate their heights leg_prop &lt;- runif(N,0.4,0.5) #we simulate their leg length as a proportion of their height leg_left &lt;- leg_prop*height + rnorm( N , 0 , 0.02 ) # we calculate the length of each leg using the above leg_right &lt;- leg_prop*height + rnorm( N , 0 , 0.02 ) # making sure to include a little error data_leg &lt;- tibble(height,leg_left,leg_right) # we fit a model and plot the parameters m6_1 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + bl*leg_left + br*leg_right , a ~ dnorm( 10 , 100 ) , bl ~ dnorm( 2 , 10 ) , br ~ dnorm( 2 , 10 ) , sigma ~ dexp( 1 ) ) , data=data_leg ) gather_draws(m6_1,a, bl, br, sigma) %&gt;% median_qi(.width=0.89)%&gt;% ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper), )+ geom_pointinterval(colour = &quot;dark grey&quot;)+ ylab(&quot;parameters&quot;) Our simulation specified that leg length would be highly correlated with height. However, the model is very uncertain about the influence of either leg (\\(b_r\\) and \\(b_l\\)) on height. Why is this? The chapter contains a nice explanation. Our predictor variables are so highly correlated that it is almost as if we have only one predictor, used twice: \\[ \\begin{aligned} y_i &amp; \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_1 x_i + \\beta_2 x_i \\end{aligned} \\] or \\[ \\mu_i = \\alpha + (\\beta_1 + \\beta_2 )x_i \\] There are a huge number of combinations of \\(\\beta_1\\) and \\(\\beta_2\\) that could produce the data. Heres the chapter: Recall that a multiple linear regression answers the question: What is the value of knowing each predictor, after already knowing all of the other predictors? Here we sample the model parameters \\(b_l\\) and \\(b_r\\) and plot them and their sum. There are highly correlated, and their sum is centered on the true parameter value for regressing leg length on height. data_leg_post &lt;- spread_draws(m6_1,br,bl) %&gt;% mutate(`sum of bl and br`=br+bl) ggplot(data_leg_post)+ geom_point(aes(x=br,y=bl),col=&#39;#77AADD&#39;,alpha=0.2) ggplot(data_leg_post)+ geom_density(aes(x=`sum of bl and br`),col=&#39;#77AADD&#39;,size=1) The chapter reintroduces the primate milk example from chapter 5. # load data data_milk &lt;- read_delim(&quot;data/milk.csv&quot;, delim = &quot;;&quot;,col_names = TRUE)%&gt;% mutate(K=standardize(kcal.per.g), F=standardize(perc.fat), L=standardize(perc.lactose)) # kcal.per.g regressed on perc.fat m6_3 &lt;- quap( alist( K ~ dnorm( mu , sigma ), mu &lt;- a + bF*F, a ~ dnorm( 0 , 0.2 ), bF ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ) , data=data_milk ) # kcal.per.g regressed on perc.lactose m6_4 &lt;- quap( alist( K ~ dnorm( mu , sigma ) , mu &lt;- a + bL*L, a ~ dnorm( 0 , 0.2 ), bL ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ) , data=data_milk ) # kcal.per.g regressed on both m6_5 &lt;- quap( alist( K ~ dnorm( mu , sigma ) , mu &lt;- a + bF*F + bL*L, a ~ dnorm( 0 , 0.2 ), bF ~ dnorm( 0 , 0.5 ), bL ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ) , data=data_milk ) Here is the parameter summary for regression of kcal per gram of milk on the fat percentage of the milk: summary_6_3 &lt;- m6_3%&gt;% gather_draws(a,bF,sigma)%&gt;% mean_qi(.width = 0.89)%&gt;% mutate(model=&quot;Fat&quot;) summary_6_3%&gt;% print(digits = 3) ## # A tibble: 3 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 0.000384 -0.124 0.124 0.89 mean qi Fat ## 2 bF 0.861 0.725 0.999 0.89 mean qi Fat ## 3 sigma 0.450 0.356 0.545 0.89 mean qi Fat And heres the summary for the regression of kcals on lactose content: summary_6_4 &lt;- m6_4%&gt;% gather_draws(a,bL,sigma)%&gt;% mean_qi(.width = 0.89)%&gt;% mutate(model=&quot;Lactose&quot;) summary_6_4%&gt;% print(digits = 3) ## # A tibble: 3 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a -0.000894 -0.106 0.104 0.89 mean qi Lactose ## 2 bL -0.902 -1.01 -0.789 0.89 mean qi Lactose ## 3 sigma 0.380 0.300 0.458 0.89 mean qi Lactose The parameters are almost mirror images of each other: the more fat content of the milk the higher the calories, the higher lactose content the lower the calories. This makes sense since fat is more energy dense than carbohydrate and fat and lactose contents are negatively correlated. Heres what happens when both are included in the same model: summary_6_5 &lt;- m6_5%&gt;% gather_draws(a,bF,bL,sigma)%&gt;% mean_qi(.width = 0.89)%&gt;% mutate(model=&quot;Both&quot;) summary_6_5%&gt;% print(digits = 3) ## # A tibble: 4 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 0.000589 -0.108 0.104 0.89 mean qi Both ## 2 bF 0.245 -0.0567 0.541 0.89 mean qi Both ## 3 bL -0.676 -0.970 -0.386 0.89 mean qi Both ## 4 sigma 0.378 0.301 0.456 0.89 mean qi Both Heres a plot: bind_rows(summary_6_3, summary_6_4,summary_6_5) %&gt;% ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) + geom_pointinterval(position = position_dodge(width = -0.3))+ geom_vline(xintercept=0, linetype = &quot;dashed&quot;)+ ylab(&quot;parameters&quot;) Including both fat and lactose contents in the model pulls the parameter estimates \\(b_F\\) and \\(b_L\\) towards zero, and about doubles the uncertainty around both. Heres a pairs plot of the two variables: ggpairs(data=select(data_milk,kcal.per.g,perc.fat,perc.lactose)) The chapter summarises: These two variables are negatively correlated, and so strongly so that they are nearly redundant. Either helps in predicting kcal.per.g, but neither helps as much once you already know the other. The problem of multicollinearity is amember of a family of problems with fitting models, a family sometimes known as non-identifiability. When a parameter is non-identifiable, it means that the structure of the data and model do not make it possible to estimate the parameters value. And heres a DAG the chapter suggests for this case. D here is how dense the milk needs to be, and is related to how frequently the species nurses. Species that nurse more often tend to have relatively more lactose and less fat in their milk. Post-Treatment Bias The chapter describes bias that can result from the inclusion of varibels in a model that should be omitted. One example of this is post-treatment bias. The case study in the chapter concerns anti-fungal soil treatments, and their effect on plant height. Plants are sprouted and their initial heights are measured. Various treatments are applied, the plants final heights are measured, and the presence or absence of fungus is noted. The fungus should not be included in a model designed to estimate the effect of the anti-fungal treatment on plant height. The proposed mechanism by which anti-fungal treatment would lead to more plant growth is by killing fungus. By including the presence/absence in the model we are in effect asking how helpful is the anti-fungal treatment, relative to some default treatment, assuming fungal growth is the same in each case. This is not the question we need to answer. Heres a DAG: dag_fungus &lt;- dagitty(&quot;dag{H_0 -&gt; H_1; F -&gt; H_1; T -&gt; F; }&quot;) coordinates(dag_fungus) &lt;- list( x=c(H_0=0,H_1=1, F=2, T=3) , y=c(H_0=0,H_1=0,F=0, T=0) ) drawdag( dag_fungus) Here: \\(H_0\\) and \\(H_1\\) are initial and final heights \\(F\\) is presence or absence of fungus \\(T\\) is the treatment applied to the soil By including the fungus in the treatment we are blocking the causal path between treatment and final height. Heres a simulation from the chapter: set.seed(71) N &lt;- 1000 #1,000 plants # inital heights h0 &lt;- rnorm(N,10,2) treatment &lt;- rep( 0:1 , each=N/2 ) # 0 is no treatment, 1 is the anti-fungal treatment #anti-fungal treatment reduces probability of fungus fungus &lt;- rbinom( N , size=1 , prob=0.5 - treatment*0.4) # final height depends on initial height and fungus h1 &lt;- h0 + rnorm( N , 5 -3*fungus) data_fungus &lt;- tibble( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus ) And heres a linear model using the proportion of plant growth \\(p\\). \\[ \\begin{aligned} h_{1,i} &amp;\\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i &amp;= h_{0,i} \\times p\\\\ p &amp;= \\alpha + \\beta_T T_i + \\beta_F F_i \\\\ \\alpha &amp;\\sim \\text{Log-Normal}(0,0.2)\\\\ \\beta_T &amp;\\sim \\text{Normal}(0,0.5)\\\\ \\beta_F &amp;\\sim \\text{Normal}(0,0.5) \\\\ \\sigma &amp;=\\text{Exponential}(1) \\end{aligned} \\] And heres the model in quap, with the parameter estimates: m6_7 &lt;- quap( alist( h1 ~ dnorm( mu , sigma ), mu &lt;- h0 * p, p &lt;- a + bt*treatment + bf*fungus, a ~ dlnorm( 0 , 0.2 ) , bt ~ dnorm( 0 , 0.5 ), bf ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ), data=data_fungus ) summary_6_7 &lt;- m6_7%&gt;% gather_draws(a,bt,bf,sigma)%&gt;% mean_qi(.width=0.89)%&gt;% mutate(model=&quot;Including Fungus&quot;) summary_6_7%&gt;% print(digits = 3) ## # A tibble: 4 x 8 ## .variable .value .lower .upper .width .point .interval model ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 1.48 1.46 1.49 0.89 mean qi Including Fungus ## 2 bf -0.282 -0.298 -0.267 0.89 mean qi Including Fungus ## 3 bt 0.00238 -0.0115 0.0162 0.89 mean qi Including Fungus ## 4 sigma 1.30 1.25 1.35 0.89 mean qi Including Fungus The effect of the treatment, \\(b_t\\) is very close to zero which we know from the simulation is not correct. Heres the same model, omitting fungus as a variable: m6_8 &lt;- quap( alist( h1 ~ dnorm( mu , sigma ), mu &lt;- h0 * p, p &lt;- a + bt*treatment, a ~ dlnorm( 0 , 0.2 ), bt ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ), data=data_fungus ) summary_6_8 &lt;- m6_8%&gt;% gather_draws(a,bt,sigma)%&gt;% mean_qi(.width=0.89)%&gt;% mutate(model=&quot;Omitting Fungus&quot;) bind_rows(summary_6_7, summary_6_8) %&gt;% ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) + geom_pointinterval(position = position_dodge(width = -0.1))+ geom_vline(xintercept=0, linetype = &quot;dashed&quot;)+ ylab(&quot;parameters&quot;) Moving from the model that includes fungus to one that omits it, the model changes from being very confident that treatment doesnt make a difference, to being very confident that it does. Lets revisit the DAG from earlier, and look at the conditional independencies that it implies: impliedConditionalIndependencies(dag_fungus) ## F _||_ H_0 ## H_0 _||_ T ## H_1 _||_ T | F The third one here means that final height is independent from treatment, conditioning on fungus. This section in the chapter closes with an example where conditioning on a post-treatment variable gives the false impression that a treatment does work: dag_fungus_M &lt;- dagitty(&quot;dag{M [unobserved]; H_0 -&gt; H_1; M -&gt; H_1; M -&gt; F; T -&gt; F; }&quot;) coordinates(dag_fungus_M) &lt;- list( x=c(H_0=0,H_1=1,M=1.5, F=2, T=3) , y=c(H_0=0,H_1=0,M=1.1, F=0, T=0) ) drawdag( dag_fungus_M) The unobserved variable here is moisture. In this DAG, treatment does not affect plant growth at all, but both plant growth and fungal growth are encouraged by moisture. From the chapter: A regression of \\(H_1\\) on \\(T\\) will show no association between the treatment and plant growth. But if we include \\(F\\) in the model, suddenly there will be an association. This is an example of collider bias. The rest of the chapter walks through a couple of examples of collider bias, including one where a measured variable becomes a collider because of the influence of an unmeasured one. Collider Bias Here are the four elemental relations that make up DAGs: The Fork In a fork, \\(Z\\) is a common cause of \\(X\\) and \\(Y\\), and this creates a correlation between them. However \\(X\\) and \\(Y\\) are independent, conditional on \\(Z\\). The Pipe In a pipe, conditioning on \\(Z\\) blocks the causal path between \\(X\\) and \\(Y\\). This is like the fungus example from earlier. The Collider In a collider, there will be no association between \\(X\\) and \\(Y\\), unless you condition on \\(Z\\). Think about the academic paper example from the start of the chapter. The Descendant From the chapter: A descendent is a variable influenced by another variable. Conditioning on a descendent partly conditions on its parent  in [this DAG], conditioning on \\(D\\) will also condition, to a lesser extent, on \\(Z\\). The reason is that \\(D\\) has some information about \\(Z\\). In this example, this will partially open the path from \\(X\\) to \\(Y\\), because \\(Z\\) is a collider. But in general the consequence of conditioning on a descendent depends upon the nature of its parent. The chapter closes with a recipe for deciding which variables to condition on, along with some practice examples: List all of the paths connecting \\(X\\) (the potential cause of interest) and \\(Y\\) (the outcome). Classify each path by whether it is open or closed. A path is open unless it contains a collider. Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering \\(X\\). If there are any open backdoor paths, decide which variable(s) to condition on to close it (if possible). Heres an example from the chapter: dag_6_1 &lt;- dagitty(&quot;dag { U [unobserved] X -&gt; Y X &lt;- U &lt;- A -&gt; C -&gt; Y U -&gt; B &lt;- C}&quot;) coordinates(dag_6_1) &lt;- list( x=c(X=0,U=0,A=1,B=1, C=2, Y=2) , y=c(X=1,U=0.5,A=0.25,B=0.75, C=0.5, Y=1) ) drawdag(dag_6_1) We are interested in the causal effect of \\(X\\) on \\(Y\\). To close the open backdoor path through \\(A\\), we can condition on either \\(A\\) or \\(C\\). The dagitty package can tell us this: adjustmentSets( dag_6_1 , exposure=&quot;X&quot; , outcome=&quot;Y&quot; ) ## { C } ## { A } We must not condition on \\(B\\), this would open a backdoor path that is currently closed. 6.2 Questions 6E1 Question List three mechanisms by which multiple regression can produce false inferences about causal effects. Answer Multicollinearity - regression on highly correlated predictors can produce misleading parameters, as in the leg, or primate milk examples. Post-treatment bias - regression on post-treatment effects can make it appear that the treatment is not effective, as in the fungus example. Collider bias - regression on a collider can create the appearance of an association between two variables that does not exist. 6E3 Question List the four elemental confounds. Can you explain the conditional dependencies of each? Answer Fork - X and Y are independent, once we condition on Z. Pipe - X and Y are independent, once we condition on Z. Collider - Conditioning on Z creates an association between X and Y. Descendant - Conditioning on D partly conditions on Z. 6E4 Question How is a biased sample like conditioning on a collider? Think of the example at the open of the chapter. Answer Using the publishing example at the beginning of the chapter, we know that there is no causal relationship between trustworthiness and newsworthiness (because thats how the simulation is constructed). However, both cause selection for publication, creating a collider. We saw that sampling only the published papers created a negative correlation between the two predictors. Conditioning on publication would have the same effect - once we know newsworthiness and publication status, we can deduce some information about trustworthiness, or vice versa. For example, a study that was published but had low newsworthiness must be quite trustworthy. A study with high trustworthiness that wasnt published is probably not very newsworthy. Conditioning on the collider has the same result as only sampling from papers that were published: the creation of a spurious association. 6M1 Question Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y: C  V Y. Reanalyze the DAG. How many paths connect X to Y? Which must be closed? Which variables should you condition on now? Answer Heres the original DAG: Now we add the unobserved V: Previously, we could condition on either A or C to find the direct casual effect of X on Y. Now C is a collider, so we should condition on A. The dagitty package corrobates this: adjustmentSets( dag_6M1B , exposure=&quot;X&quot; , outcome=&quot;Y&quot; ) ## { A } 6M2 Question Sometimes, in order to avoid multicollinearity, people inspect pairwise correlations among predictors before including them in a model. This is a bad procedure, because what matters is the conditional association, not the association before the variables are included in the model. To highlight this, consider the DAG X  Z  Y. Simulate data from this DAG so that the correlation between X and Z is very large. Then include both in a model predicting Y. Do you observe any multicollinearity? Why or why not? What is different from the legs example in the chapter? Answer Modifying the leg example: N &lt;- 1000 set.seed(909) Y &lt;- rnorm(N,10,2) Y_prop &lt;- runif(N,0.4,0.5) Z &lt;- Y_prop*Y + rnorm( N , 0 , 0.02 ) Z_prop &lt;- runif(N,0.8,0.9) X &lt;- -Z_prop*Z + rnorm( N , 0 , 0.02 ) data_6M2 &lt;- bind_cols(X=X,Z=Z,Y=Y) ggpairs(data_6M2) set.seed(100) m6M2 &lt;- quap( alist( Y ~ dnorm( mu , sigma ) , mu &lt;- a + bX*X + bZ*Z , a ~ dnorm( 10 , 100 ) , bX ~ dnorm( -3 , 10 ) , bZ ~ dnorm( 2 , 10 ) , sigma ~ dexp( 1 ) ) , data=data_6M2 ) ggplot(data=precis(m6M2))+ geom_pointrange(aes(x=rownames(precis(m6M2)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+ geom_hline(yintercept = 0,col=&quot;red&quot;)+ xlab(&quot;parameter&quot;)+ coord_flip() ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning in sqrt(diag(vcov(model))): NaNs produced ## Warning: Removed 1 rows containing missing values (geom_segment). In the legs example, the model was very uncertain about the parameter values for both legs. This is not the case here, the parameter estimate for the influence of Z is about as expected, 2-2.5. The model has correctly identified that the only influence of X on Y is through Z, and so produces parameter estimates for X much smaller than expected. We have a pipe, and including Z blocks the path from X to Y, but we dont have the problems with identifiability that we had in the legs example. There is no way to tell this scenario apart from the legs example simply from looking at the pairwise correlations. 6M3 Question Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (condition on) to estimate the total causal influence of X on Y. Answer We should condition on Z to block the backdoor path. drawdag(dag_6M3.1) adjustmentSets(dag_6M3.1,exposure = &quot;X&quot;,outcome = &quot;Y&quot;,effect = &quot;total&quot;) ## { Z } We no longer want to condition on Z - it is a collider. drawdag(dag_6M3.2) adjustmentSets(dag_6M3.2,exposure = &quot;X&quot;,outcome = &quot;Y&quot;,effect = &quot;total&quot;) ## {} We no longer want to condition on Z - it is a collider. drawdag(dag_6M3.3) adjustmentSets(dag_6M3.3,exposure = &quot;X&quot;,outcome = &quot;Y&quot;,effect = &quot;total&quot;) ## {} We dont want to condition on Z here because we are looking for the total casual influence of X on Y - one route of this influence goes through Z. We should condition on A however, to block the backdoor path. drawdag(dag_6M3.4) adjustmentSets(dag_6M3.4,exposure = &quot;X&quot;,outcome = &quot;Y&quot;,effect = &quot;total&quot;) ## { A } 6H1 Question Use the Waffle House data to find the total causal influence of number of Waffle Houses on divorce rate. Justify your model or models with a causal graph. Answer Looking through the available variables, I think I want to consider the following: * Number of Waffle Houses (W) * Divorce rate (D) * Whether were in the South (S) * Marriage rate (M) * Median age of marriage (A) Heres my proposed DAG: dag_waf &lt;- dagitty( &quot;dag {S -&gt; W; S -&gt; M; S-&gt;A; W -&gt; D ; M -&gt; D; A -&gt; D;}&quot;) coordinates(dag_waf) &lt;- list( x=c(W=0,S=1,M=1,D=1,A=2) , y=c(S=0,W=1,M=1,A=1,D=2) ) drawdag(dag_waf ) adjustmentSets(dag_waf,exposure = &quot;W&quot;,outcome = &quot;D&quot;,effect = &quot;total&quot;) ## { A, M } ## { S } So we need to include S in the model, to block the backdoor path through A and M. After loading the data, standardising, and doing some prior simulation, I have the model below: set.seed(100) m6H1 &lt;- quap( alist( Divorce ~ dnorm( mu , sigma ) , mu &lt;- a[South] + bW*WaffleHouses, a[South] ~ dnorm( 0 , 0.6 ) , bW ~ dnorm( 0 , 0.2 ) , sigma ~ dexp( 1 ) ) , data=data_waf ) ggplot(data=precis(m6H1,depth = 2))+ geom_pointrange(aes(x=rownames(precis(m6H1,depth = 2)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+ geom_hline(yintercept = 0,col=&quot;red&quot;)+ xlab(&quot;parameter&quot;)+ coord_flip() This is consistent with the number of Waffle Houses having no causal effect on the divorce rate. 6H2 Question Build a series of models to test the implied conditional independencies of the causal graph you used in the previous problem. If any of the tests fail, how do you think the graph needs to be amended? Does the graph need more or fewer arrows? Feel free to nominate variables that arent in the data. Answer impliedConditionalIndependencies(dag_waf) ## A _||_ M | S ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S Well first test that the divorce rate is independent of being in the South, conditional on number of waffle houses, median age of marriage, and marriage rate. set.seed(100) m6H2.1 &lt;- quap( alist( Divorce ~ dnorm( mu , sigma ) , mu &lt;- a[South] + bW*WaffleHouses + bM*Marriage + bA*MedianAgeMarriage, a[South] ~ dnorm( 0 , 0.6 ) , bW ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 1 ) , bA ~ dnorm( 0 , 1 ) , sigma ~ dexp( 1 ) ) , data=data_waf ) m6H2.1_post &lt;- post &lt;- extract.samples(m6H2.1) m6H2.1_post$diff_south &lt;- m6H2.1_post$a[,1] - m6H2.1_post$a[,2] ggplot(data=precis( m6H2.1_post , depth=2 ))+ geom_pointrange(aes(x=rownames(precis( m6H2.1_post , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+ geom_hline(yintercept = 0,col=&quot;red&quot;)+ xlab(&quot;parameter&quot;)+ coord_flip() The values of the diff_south parameter are consistent with the conditional independence, but Im not completely happy that Ive caught all the ways that being in the South can influence the divorce rate. Perhaps I should add an arrow directly from the South to the divorce rate, or add in an unobserved variable to stand in for cultural/ religious attitudes towards divorce. Well test one more conditional independence, one that I think is more likely to be true. Lets see if the median age of marriage is independent of the number of waffle houses, once we condition on being in the south. set.seed(100) m6H2.2 &lt;- quap( alist( MedianAgeMarriage ~ dnorm( mu , sigma ) , mu &lt;- a[South] + bW*WaffleHouses, a[South] ~ dnorm( 0 , 0.6 ) , bW ~ dnorm( 0 , 1 ) , sigma ~ dexp( 1 ) ) , data=data_waf ) m6H2.2_post &lt;- extract.samples(m6H2.2) m6H2.2_post$diff_south &lt;- m6H2.2_post$a[,1] - m6H2.2_post$a[,2] ggplot(data=precis( m6H2.2_post , depth=2 ))+ geom_pointrange(aes(x=rownames(precis( m6H2.2_post , depth=2 )),y=mean,ymin=`5.5%`,ymax=`94.5%`))+ geom_hline(yintercept = 0,col=&quot;red&quot;)+ xlab(&quot;parameter&quot;)+ coord_flip() The parameter bW is estimated to be quite close to zero. 6H3 Question Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your models prior predictions stay within the possible outcome range. Answer We dont need to condition on any other parameters since were looking for the total causal effect. ## {} Unexpectedly, the total causal impact of area on weight appears to be zero, or slightly negative. Increasing area would not make foxes heavier. 6H4 Question Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food? Answer We dont need to condition on any other parameters assuming the DAG were given is correct. ## {} The total causal impact of food on weight again appears to be negative. Increasing food would not make foxes heavier. 6H5 Question Now infer the causal impact of group size. Which covariates do you need to adjust for? Looking at the posterior distribution of the resulting model, what do you think explains these data? That is, can you explain the estimates for all three problems? How do they go together? Answer Now we have to condition on food to block the backdoor path. ## { F } We see that the causal impact of group size is negative, and that the direct effect of food is zero or slightly positive. It is at least not so negative as the total causal effect. Id suggest that whats happening here is that the main effect of an increase in food (either directly or by an increase in area) would be to increase group size, which has a detrimental effect on weight. This effect seems to overwhelm any direct effect of increasing food on weight. Further Reading "],["ulysses.html", "Chapter 7 Ulysses Compass 7.1 Chapter Notes 7.2 Questions Further Reading", " Chapter 7 Ulysses Compass # Colours by Paul Tol. # Defined in R by Joachim Goedhart # Source: doi 10.5281/zenodo.3381072 tol_light &lt;- c(&#39;#BBCC33&#39;, &#39;#AAAA00&#39;, &#39;#77AADD&#39;, &#39;#EE8866&#39;, &#39;#EEDD88&#39;, &#39;#FFAABB&#39;, &#39;#99DDFF&#39;, &#39;#44BB99&#39;, &#39;#DDDDDD&#39;) 7.1 Chapter Notes This chapter is all about model selection. The title refers to our need to navigate between the Scylla of overfitting and the Charybdis of underfitting. Respectively learning too much and too little from the data. In the previous chapter about confounding, we learned that in some cases adding predictor variables to your model can makes inferences worse. However if we only care about prediction, and not inference, these variables can still help. So should we just add all plausible predictors to our models? This chapter answers no. Even if all we care about is prediction, we are at risk of overfitting to the training data set, which will make our model worse when predicting on new data. Heres an example from the chapter, on hominin brain volumes, and body mass: Brain Volume model data_hominin &lt;- tibble( species = c( &quot;afarensis&quot;,&quot;africanus&quot;,&quot;habilis&quot;,&quot;boisei&quot;, &quot;rudolfensis&quot;,&quot;ergaster&quot;,&quot;sapiens&quot;), brain_vol = c( 438 , 452 , 612, 521, 752, 871, 1350 ), # brain volume body_size = c( 37.0 , 35.5 , 34.5 , 41.5 , 55.5 , 61.0 , 53.5 ), # body mass vol_std = brain_vol/max(brain_vol), # keeping zero brain volume as a reference point mass_std = (body_size - mean(body_size))/sd(body_size) ) The structure of this section is that were going to regress brain volume on body mass using increasingly higher-order polynomials and see what happens to \\(R^2\\). Heres a simple linear regression: \\[ \\begin{aligned} b_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta m_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0.5,1)&amp;&amp; (\\text{Centred on approx. mean brain volume.})\\\\ \\beta &amp;\\sim \\text{Normal}(0,10)&amp;&amp; (\\text{Very flat.})\\\\ \\sigma &amp;\\sim \\text{Log-Normal}(0,1)\\\\ \\end{aligned} \\] set.seed(71) m7_1 &lt;- quap( alist( vol_std ~ dnorm( mu , exp(log_sigma) ), mu &lt;- a + b*mass_std, a ~ dnorm( 0.5 , 1 ), b ~ dnorm( 0 , 10 ), log_sigma ~ dnorm( 0 , 1 ) ), data=data_hominin ) We calculate the \\(R^2\\) of this model: # we define a function for the empirical variace # since var() in R uses the denominator n-1 var_n &lt;- function(vector){ mean((mean(vector)-vector)^2) } # we define a function to produce the R^2 of each model we&#39;ll be fitting hominin_r_squared &lt;- function(model){ pred_hominin &lt;- add_predicted_draws(data_hominin,model)%&gt;% # predict brain volume using observed body weights group_by(species,vol_std)%&gt;% summarise(pred_vol=mean(.prediction), .groups = &quot;keep&quot;)%&gt;% # we&#39;ll use the mean of our predictions mutate(residuals = pred_vol-vol_std) 1 - var_n(pred_hominin$residuals)/var_n(pred_hominin$vol_std) # our R^2 calc } hominin_r_squared(m7_1) ## [1] 0.4872818 We then do the same for polynomials of increasing order, starting with a model like this one: \\[ \\begin{aligned} b_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta_1 m_i + \\beta_2 m_i^2 &amp;&amp; (\\text{A second degree polynomial})\\\\ \\alpha &amp;\\sim \\text{Normal}(0.5,1)\\\\ \\beta &amp;\\sim \\text{Normal}(0,10)\\\\ \\sigma &amp;\\sim \\text{Log-Normal}(0,1)\\\\ \\end{aligned} \\] We fit all these models, up to a polynomial of degree six and calculate the \\(R^2\\). Ill hide this code because its pretty repetitive. Now we plot the models: # Calculating R^2 values_r_square &lt;- tibble(model = paste0(&quot;m7_&quot;,1:6))%&gt;% rowwise()%&gt;% mutate(r_squared = hominin_r_squared(get(model)), # get() allows us to refer to our models dynamically labels = paste(&quot;R^2 =&quot;,round(r_squared,2))) # labels for our plot later # Create our data frame of draws from the posterior grid_hominin &lt;- data_grid(data_hominin,mass_std=seq_range(mass_std, n = 51)) draws_hominin &lt;- add_predicted_draws(grid_hominin,m7_1)%&gt;% mutate(model = &quot;m7_1&quot;) # this loop be made more efficient I&#39;m sure for (i in 2:6){ # I&#39;ve also used get() here draws_int &lt;- add_predicted_draws(grid_hominin,get(paste0(&quot;m7_&quot;,i)))%&gt;% mutate(model = paste0(&quot;m7_&quot;,i)) draws_hominin &lt;- bind_rows(draws_hominin,draws_int) } draws_hominin%&gt;% mutate(brain_vol = .prediction * max(data_hominin$brain_vol), # undoing the previous transformations body_size = mass_std*sd(data_hominin$body_size)+mean(data_hominin$body_size))%&gt;% ggplot(aes(x = body_size, y=brain_vol)) + stat_lineribbon(aes(y = brain_vol)) + geom_point(data = data_hominin, colour = tol_light[[4]],size=2) + geom_text(data=values_r_square, # adding our R^2 lables from earlier aes(x=40,y=1900,label=labels),size=3)+ facet_wrap(~model)+ scale_fill_brewer(palette = &quot;Greys&quot;)+ xlab(&quot;body mass (kg)&quot;)+ ylab(&quot;brain volumne (cc)&quot;) With higher order polynomials we get an increasingly good fit to the data, eventually reaching \\(R^2 =1\\). But some of these models make no sense. Model m7_6 predicts negative brain volume at body mass 58kg. If a new hominin was discovered at around this weight, this model would make horrific predictions of brain volume, even though the model perfectly retrodicts all of the training data. Having set up the concern, the chapter goes on to outline some model selection tools: regularisation and information criteria. The first step is to define information entropy as a measure of uncertainty: \\[ H(p) = -E\\log(p_i) = -\\sum_{i=1}^n p_i \\log(p_i) \\] The chapter summarises: The uncertainty contained in a probability distribution is the average log-probability of an event. And then Kullback-Leibler divergence as a measure of the additional uncertainty induced by using probabilities from one distribution to describe another distribution: \\[ D_{KL}(p,q) = \\sum_{i} p_i ( \\log(p_i) - \\log(q_i))= \\sum_{i} p_i \\log(\\frac{p_i}{q_i}) \\] This is the average difference in log-probability between our target \\(p\\) and model \\(q\\), and we can use it to compare the accuracy of different models. Log Pointwise Predictive Density Revisit this section. Computing the log pointwise predictive density. With data \\(y = \\{y_i\\}\\) and posterior distribution \\(\\Theta\\): \\[ \\text{lppd}(y,\\Theta) = \\sum_i \\log \\left( \\frac{1}{S}\\sum_s p(y_i | \\theta_s) \\right) \\] Where \\(S\\) is the number of samples and each \\(\\theta_s\\) is a sample from \\(\\Theta\\). For an observation say \\(y_1\\), you calculate the average probability of seeing this observation given the posterior (i.e. you sum the conditional probabilities of the observation over each posterior sample, and divide by the number of samples). You repeat this process for each observation \\(y_i\\), take the log of each and sum them all together. Here is the calculation done in R, from the Overthinking: Computing the lppd box: set.seed(1) lppd1 &lt;- lppd( m7_1 , n=1e4 ) # Extracting posterior samples and calculating manually logprob &lt;- sim( m7_1 , ll=TRUE , n=1e4 ) # logprob contains 10,000 samples from the posterior distribution, each estimating the (log) probability of seeing each # of the seven observations. n &lt;- ncol(logprob) ns &lt;- nrow(logprob) average_post_prob &lt;- function( i ) { log_sum_exp( logprob[,i] ) - log(ns)} # this functions sums the log probabilities of the 10,000 samples and divides by the number of rows # except it is working on a log probability scale. lppd2 &lt;- sapply( 1:n , average_post_prob ) # the function is applied to each of the seven observations lppd1 ## [1] 0.6098673 0.6483443 0.5496098 0.6234939 0.4648151 0.4347613 -0.8444654 lppd2 ## [1] 0.6116789 0.6488210 0.5447935 0.6277960 0.4638950 0.4263109 -0.8522379 Summing over these seven observations will give the total lppd. Simulating in and out of sample deviance for varying parameter numbers The idea here is to simulate from the following model with two parameters: \\[ \\begin{aligned} y_i &amp; \\sim \\text{Normal}(\\mu_i,1) \\\\ \\mu_i &amp;= (0.15)x_{1,i} - (0.4)x_{2,i} \\end{aligned} \\] We fit linear regressions with between 1 and 5 parameters to the simulated data, and compare the in and out of sample deviance for these five models. N &lt;- 20 kseq &lt;- 1:5 # sim_train_test is a function from the rethinking package. # it simulates Gaussian data with N cases and fits a model with k parameters, # returning the in sample and out of sample lppd (can also do WAIC criteria) # data_dev contains the mean in sample deviance, mean out of sample deviance, # sd of in sample deviance, and sd of out of sample deviance for each model with 1-5 parameters. data_dev &lt;- map_dfr( kseq , function(k) { r &lt;- replicate( 100 , sim_train_test( N=N, k=k , cv.cores = 8)); tibble(mean_in = mean(r[1,]) ,mean_out= mean(r[2,]) ,sd_in= sd(r[1,]) ,sd_out= sd(r[2,]) ) } ) # Revisit this mess of a function, set eval to TRUE once it works again # Add commentary below the graph ggplot(data=data_dev)+ geom_pointrange(aes(x=1:5,y=mean_in,ymin=mean_in-sd_in,ymax=mean_in+sd_in), colour = &quot;blue&quot;)+ geom_pointrange(aes(x=1.1:5.1,y=mean_out,ymin=mean_out-sd_out,ymax=mean_out+sd_out))+ geom_text(aes(x=2.9, y = mean_in[3], label=&quot;in&quot;),colour=&quot;blue&quot;)+ geom_text(aes(x=3.2, y = mean_out[3], label=&quot;out&quot;))+ xlab(&quot;number of parameters&quot;)+ ylab(&quot;deviance&quot;)+ ggtitle(&quot;N = 20&quot;) There is a section on the role that regularising priors can play in preventing over-fitting. Predicting Predicitve Accuracy This section introduces cross-validation and information criteria as a way to estimating model predictive performance on out-of-sample data. Well end up at PSIS and WAIC respectively. Leave-one-out cross-validation is a way to get an estimate of out-of-sample accuracy. However it is computationally expensive. A key insight when trying to approximate LOOCV is that unlikely observations have a larger effect on the posterior distribution. This insight leads to the importance sampling approach, the approach highlighted by the chapter is Pareto-smoothed importance sampling or PSIS. It has a couple of nice features, including the ability to provide diagnostic information and to estimate the standard error of out-of-sample deviance. Revisit: Overthinking PSIS box Aside from cross-validation appraoches, we can also use information criteria to predict predictive accuracy. The Akaike information criterion estimates out-of-sample deviance: \\[ \\text{AIC}=D_\\text{train} + 2p = -2\\text{lppd}+2p \\] where \\(p\\) is the number of free parameters in the posterior distribution. From the chapter: [W]hat AIC tells us is that the dimensionality of the posterior distribution is a natural measure of the models overfitting tendency. More complex models tend to overfit more, directly in proportion to the number of parameters Mechanically, deriving AIC means writing down the goal, which is the expected KL divergence, and then making approximations. The expected bias turns out to be proportional to the number of parameters, provided a number of assumptions are approximately correct. AIC requires quite strong assumptions - flat priors, approximately Gaussian posterior, large sample size. The chapter introduces Watanabes widely applicable information criterion (WAIC), which does not make assumptions about the shape of the posterior and in a large sample converges to the cross-validation approximation. The formula for WAIC is: \\[ \\text{WAIC}(y,\\Theta) = -2 \\left( \\text{lppd} - \\sum_i \\text{var}_\\theta ( \\log p(y_i | \\theta)) \\right) \\] where the \\(y_i\\)s are the observations and \\(\\Theta\\) is the posterior distribution. The penalty term means, compute the variance in log-probabilities for each observation i, and then sum up these variances to get the total penalty. \\(\\text{var}_\\theta\\) means taking the variance over the set of posterior samples. Like PSIS, WAIC is also calculated pointwise - they both split up the data into independent observations. This has benefits - but its unclear how to interpret this is the case where some observations depend on previous observations (like in a time series). The book calculates WAIC for a simple model. This code is straight out of the book: # fit a linear model of stopping distance &amp; speed data(cars) set.seed(100) m &lt;- quap( alist( dist ~ dnorm(mu,sigma), mu &lt;- a + b*speed, a ~ dnorm(0,100), b ~ dnorm(0,10), sigma ~ dexp(1) ) , data=cars ) set.seed(94) cars_post &lt;- extract.samples(m,n=1000) # calculate (log) probability of seeing data in cars, assuming it comes from a normal distribution with mean and sd taken from the posterior samples. There are 1000 samples so logprob contains 1000 columns. There are 50 observations in cars so logprob has 50 rows. n_samples &lt;- 1000 cars_logprob &lt;- sapply( 1:n_samples , function(s) { mu &lt;- cars_post$a[s] + cars_post$b[s]*cars$speed dnorm( cars$dist , mu , cars_post$sigma[s] , log=TRUE ) } ) # we take the log of the average probabilities (with a little extra code since we&#39;re working with log probabilities) n_cases &lt;- nrow(cars) cars_lppd &lt;- sapply( 1:n_cases , function(i) log_sum_exp(cars_logprob[i,]) - log(n_samples) ) # we calculate the penalty term - taking the variance for each observation across all 1000 rows cars_pWAIC &lt;- sapply( 1:n_cases , function(i) var(cars_logprob[i,]) ) # we compute WAIC -2*( sum(cars_lppd) - sum(cars_pWAIC) ) ## [1] 423.3188 Model Comparison {-} The chapter revisits the plan growth/ fungus example from chapter 6. In that chapter, including fungus in the model bias inferences against the treatment - the fungus was a post-treatment variable. The models fit were; m6_6 - model that didnt include either fungus or treatment as predictors m6_7 - model including both fungus and treatment as predictors m6_8 - model omitting fungus as a predictor We compare the models using WAIC: set.seed(77) compare( m6_6 , m6_7 , m6_8 , func=WAIC ) ## WAIC SE dWAIC dSE pWAIC weight ## m6_7 361.8100 14.26082 0.00000 NA 3.737444 1.000000e+00 ## m6_8 403.0534 11.31622 41.24348 10.47324 2.766014 1.106859e-09 ## m6_6 405.9652 11.70605 44.15528 12.23512 1.600129 2.581088e-10 What are these figures? The chapter: Columns from left to right are: WAIC, standard error (SE) of WAIC, difference of each WAIC from the best model, standard error (dSE) of this difference, prediction penalty (pWAIC), and finally the Akaike weight. Smaller values of WAIC are better, and the compare function orders them from lowest to highest. The penalty figure in WAIC is sometimes called effective number of parameters. To tell if the models can be distinguished from each other in their expected prediction performance, we need to compare the WAIC differences in the dWAIC column to the standard error of this difference in the dSE column. Model 6_7 (the confounded model) is a lot better. As an aside, I originally used 1,000 plants in my simulation instead of the 100 the chapter uses, and I was very surprised to see my WAIC figures on the order of 10 x higher than those printed in the book. A nice, accidental lesson on when WAIC figures are (not) comparable. 7.2 Questions 7E1 Question State the three motivating criteria that define information entropy. Try to express each in your own words. Answer We want our measure of uncertainty to be: Continuous - a small change in the probabilities should lead to a small change in uncertainty. Increasing - uncertainty should increase as the number of events increases Additive - the uncertainty of two successive events should be the weighted sum of the uncertainties of each event. 7E2 Question Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin? Answer Since the textbook uses the natural log, Ill use it too. \\[\\begin{aligned} H(p) &amp;= -\\sum^n_{i=1}p_i \\log(p_i) \\\\ &amp;= -(0.7 \\log(0.7) + 0.3 \\log(0.3))\\\\ &amp;= 0.6109 \\end{aligned}\\] These are the same figures as the rain or shine example in the chapter. 7E3 Question Suppose a four-sided die is loaded such that, when tossed onto a table, it shows 1 20%, 2 25%, 3 25%, and 4 30% of the time. What is the entropy of this die? Answer \\[\\begin{aligned} H(p) &amp;= -\\sum^n_{i=1}p_i \\log(p_i) \\\\ &amp;= -(0.2 \\log(0.2) + 0.25 \\log(0.25) + 0.25 \\log(0.25) + 0.3 \\log(0.3))\\\\ &amp;= 1.3762 \\end{aligned}\\] 7E4 Question Suppose another four-sided die is loaded such that it never shows 4. The other three sides show equally often. What is the entropy of this die? Answer \\[\\begin{aligned} H(p) &amp;= -\\sum^n_{i=1}p_i \\ \\log(p_i) \\\\ &amp;= -(\\frac{1}{3} \\log(\\frac{1}{3}) + \\frac{1}{3} \\log(\\frac{1}{3}) + \\frac{1}{3} \\log(\\frac{1}{3}) + 0 \\log(0))\\\\ &amp;= -\\log(\\frac{1}{3}) &amp;&amp;\\text{using the convention that } 0 \\log(0) = 0 \\\\ &amp;= 1.0986 \\end{aligned}\\] 7M1 Question Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one? Answer AIC \\[ \\text{AIC}(y,\\Theta) = -2 \\text{lppd} + 2p \\] WAIC \\[ \\text{WAIC}(y,\\Theta) = -2 \\left( \\text{lppd} - \\sum_i \\text{var}_\\theta ( \\log p(y_i | \\theta)) \\right) \\] WAIC is more general, since the adjustment term \\(\\sum_i \\text{var}_\\theta ( \\log p(y_i | \\theta))\\) is approximately equal to the number of parameters when the following constraints are in place: the posterior is Gaussian there is a large sample size the prior is uninformative (or overwhelmed by the data) 7M2 Question Explain the difference between model selection and model comparison. What information is lost under model selection? Answer Model selection here involves comparing candidate models using a some criterion (say WAIC), choosing the model with the lowest WAIC, and discarding the rest. This approach involves throwing away information about the relative differences between models, which can give hints about how confident we should be about our models. 7M3 Question When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure. Answer We should be able to answer this question by inspecting the formula for lppd: \\[ \\text{lppd}(y,\\Theta) = \\sum_i \\log \\left( \\frac{1}{S}\\sum_s p(y_i | \\theta_s) \\right) \\] Changing the observations means changing the \\(y_i\\)s and therefore the calculation of the average probability of the observations given the data \\(\\frac{1}{S}\\sum_s p(y_i | \\theta_s)\\). Using a different set of observations for different models will make the resulting information criterion values uninterpretable. An easy way to see this is to consider that changing observations will lead to different information criterion values even with the same model. 7M4 Question What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure. Answer Before performing any experiments, Ill take a guess by looking at the definition of effective number of parameters for WAIC: \\[ \\sum_i \\text{var}_\\theta ( \\log p(y_i | \\theta))\\] Id expect the variance of the log probabilities here to decrease, since informative priors give a model more concentrated expectations for the data, i.e. they lead to unusual values of \\(y\\) being assigned less probability. So I expect that making priors more narrowly peaked decreases the effective number of parameters. Ill pick up the cars data set from earlier and fit a couple of models with increasingly concentrated priors. # previous model # m &lt;- quap( alist( # dist ~ dnorm(mu,sigma), # mu &lt;- a + b*speed, # a ~ dnorm(0,100), # b ~ dnorm(0,10), # sigma ~ dexp(1) # ) , data=cars ) cars_prior_sim_1 &lt;- tibble(a=rnorm(100,0,100),b=rnorm(100,0,10),mu=a+b) ggplot()+ geom_abline(data=cars_prior_sim_1, mapping=aes(slope=b,intercept=a))+ geom_hline(yintercept = 1.2*max(cars$dist) ,colour=&quot;red&quot;)+ geom_hline(yintercept = 0.8*min(cars$dist),colour=&quot;red&quot;)+ xlim(min(cars$speed),max(cars$speed)) These priors are very flat. Lets make them more concentrated. Logically, the intercept a should be near zero since if youre going zero mph it should take you zero feet to stop. Also wed expect b to be positive - the faster you go the further it takes you to stop. cars_prior_sim_2 &lt;- tibble(a=rnorm(100,0,5),b=rnorm(100,5,2.5),mu=a+b) ggplot()+ geom_abline(data=cars_prior_sim_2, mapping=aes(slope=b,intercept=a))+ geom_hline(yintercept = 1.2*max(cars$dist) ,colour=&quot;red&quot;)+ geom_hline(yintercept = 0.8*min(cars$dist),colour=&quot;red&quot;)+ xlim(min(cars$speed),max(cars$speed)) I think this is reasonable for our purposes. Now lets make the prior very concentrated. cars_prior_sim_3 &lt;- tibble(a=rnorm(100,0,0.5),b=rnorm(100,5,0.5),mu=a+b) ggplot()+ geom_abline(data=cars_prior_sim_3, mapping=aes(slope=b,intercept=a))+ geom_hline(yintercept = 1.2*max(cars$dist) ,colour=&quot;red&quot;)+ geom_hline(yintercept = 0.8*min(cars$dist),colour=&quot;red&quot;)+ xlim(min(cars$speed),max(cars$speed)) Now we fit two new models with our new priors and calculate the effective number of parameters. set.seed(100) m_moderate &lt;- quap( alist( dist ~ dnorm(mu,sigma), mu &lt;- a + b*speed, a ~ dnorm(0,5), b ~ dnorm(5,2.5), sigma ~ dexp(1) ) , data=cars ) set.seed(100) m_concentrated &lt;- quap( alist( dist ~ dnorm(mu,sigma), mu &lt;- a + b*speed, a ~ dnorm(0,0.5), b ~ dnorm(5,0.5), sigma ~ dexp(1) ) , data=cars ) moderate_post &lt;- extract.samples(m_moderate,n=1000) concentrated_post &lt;- extract.samples(m_concentrated,n=1000) n_samples &lt;- 1000 moderate_logprob &lt;- sapply( 1:n_samples , function(s) { mu &lt;- moderate_post$a[s] + moderate_post$b[s]*cars$speed dnorm( cars$dist , mu , moderate_post$sigma[s] , log=TRUE ) } ) concentrated_logprob &lt;- sapply( 1:n_samples , function(s) { mu &lt;- concentrated_post$a[s] + concentrated_post$b[s]*cars$speed dnorm( cars$dist , mu , concentrated_post$sigma[s] , log=TRUE ) } ) n_cases &lt;- nrow(cars) moderate_pWAIC &lt;- sapply( 1:n_cases , function(i) var(moderate_logprob[i,]) ) concentrated_pWAIC &lt;- sapply( 1:n_cases , function(i) var(concentrated_logprob[i,]) ) sum(cars_pWAIC) ## [1] 4.780675 sum(moderate_pWAIC) ## [1] 3.732736 sum(concentrated_pWAIC) ## [1] 3.662483 #compare(m,m_moderate,m_concentrated, func = PSIS) As expected, the effective number of parameters (in the WAIC calculation) decreases as the prior becomes more concentrated. Running the commented-out compare function shows the same for PSIS. 7M5 Question Provide an informal explanation of why informative priors reduce overfitting. Answer Informative priors make a model more sceptical of the data, since the model has narrower expectations of plausible parameter values before it even sees the data. Overfitting occurs when a model is too wedded to the particular data set it is trained on. It encodes features of this data set that are unlikely to be present in future data, Because of their scepticism, informative priors reduce the risk of overfitting. The aim is that only regular features - those that you might expect to occur in a future data set - will be encoded into the model. 7M6 Question Provide an informal explanation of why overly informative priors result in underfitting. Answer If the priors make the model too sceptical of the data, it will fail to capture some of the regular features of the data. The model would do a better job of predicting future data if it were allowed to learn more from the training data. 7H1 Question In 2007, The Wall Street Journal published an editorial (Were Number One, Alas) with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in, seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in data(Laffer). Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue? Answer data(Laffer) #laffer_prior1 &lt;- tibble(a=rnorm(100,1,0.5),b=rnorm(100,0.3,0.2),mu=a+b) #ggplot()+ # geom_abline(data=laffer_prior1, mapping=aes(slope=b,intercept=a))+ # geom_hline(yintercept = 1.2*max(Laffer$tax_revenue) ,colour=&quot;red&quot;)+ # geom_hline(yintercept = 0.8*min(Laffer$tax_revenue),colour=&quot;red&quot;)+ # xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) set.seed(100) m_laffer1 &lt;- quap( alist( tax_revenue ~ dnorm(mu,sigma), mu &lt;- a + b*tax_rate, a ~ dnorm(1,0.5), b ~ dnorm(0.3,0.2), sigma ~ dexp(1) ) , data=Laffer ) laffer1_post &lt;- extract.samples(m_laffer1) #laffer_prior2 &lt;- tibble(a=rnorm(100,1,0.5),b=rnorm(100,0.01,1),c=rnorm(100,0.01,1)) #ggplot()+ # purrr::map(1:nrow(laffer_prior2), ~geom_function(fun = function(x) laffer_prior2$a[.x] + laffer_prior2$b[.x] * x + laffer_prior2$c[.x] * x^2 ))+ # geom_hline(yintercept = 1.2*max(Laffer$tax_revenue) ,colour=&quot;red&quot;)+ # geom_hline(yintercept = 0.8*min(Laffer$tax_revenue),colour=&quot;red&quot;)+ # xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) set.seed(100) m_laffer2 &lt;- quap( alist( tax_revenue ~ dnorm(mu,sigma), mu &lt;- a + b*tax_rate +c*tax_rate^2, a ~ dnorm(1,0.5), b ~ dnorm(0.1,1), c ~ dnorm(0.1,1), sigma ~ dexp(1) ) , data=Laffer ) laffer2_post &lt;- extract.samples(m_laffer2) laffer2_mean &lt;- tibble(a=mean(laffer2_post$a),b=mean(laffer2_post$b),c=mean(laffer2_post$c),d=mean(laffer2_post$d)) set.seed(101) m_laffer3 &lt;- quap( alist( tax_revenue ~ dnorm(mu,sigma), mu &lt;- a + b*tax_rate +c*tax_rate^2 +d*tax_rate^3, a ~ dnorm(1,0.5), b ~ dnorm(0.1,1), c ~ dnorm(0.1,1), d ~ dnorm(0.1,1), sigma ~ dexp(1) ) , data=Laffer ) laffer3_post &lt;- extract.samples(m_laffer3) laffer3_mean &lt;- tibble(a=mean(laffer3_post$a),b=mean(laffer3_post$b),c=mean(laffer3_post$c),d=mean(laffer3_post$d)) ggplot()+ geom_point(data=Laffer, mapping=aes(tax_rate,tax_revenue))+ geom_abline(data=laffer1_post, mapping=aes(slope=mean(b),intercept=mean(a)),colour=&quot;red&quot;)+ geom_function(fun = function(x) laffer2_mean$a + laffer2_mean$b*x + laffer2_mean$c*x^2, colour = &quot;blue&quot;)+ geom_function(fun = function(x) laffer3_mean$a + laffer3_mean$b*x + laffer3_mean$c*x^2 + laffer3_mean$d*x^3, colour = &quot;green&quot;)+ xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) compare(m_laffer1,m_laffer2,m_laffer3) ## WAIC SE dWAIC dSE pWAIC weight ## m_laffer1 124.1184 23.65623 0.000000 NA 6.365137 0.4870995 ## m_laffer2 125.1519 25.68798 1.033533 3.284146 7.971944 0.2905286 ## m_laffer3 125.6866 25.18629 1.568234 3.537762 8.963676 0.2223720 The linear model predicts an increasing relationship between tax rate and tax revenue. The quadratic model predicts a decreasing relationship between tax rate and tax revenue, after reaching a maximum. They are with one standard error of each other by WAIC so I wouldnt want to draw strong conclusions about the relationship between tax rate and revenue from this data. Tax revenue appears to increase with tax rate, with a possible leveling out or decline after a certain rate. 7H2 Question In the Laffer data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Students t distribution to revisit the curve fitting problem. How much does a curved relationship depend upon the outlier point? Answer We get warnings about high Pareto k values when using PSIS to compare the models: compare(m_laffer1,m_laffer2,m_laffer3, func=PSIS) ## Some Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points. ## Some Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points. ## Some Pareto k values are very high (&gt;1). Set pointwise=TRUE to inspect individual points. ## PSIS SE dPSIS dSE pPSIS weight ## m_laffer1 126.3576 26.21755 0.0000000 NA 7.411442 0.55631136 ## m_laffer2 127.1612 28.19643 0.8036119 3.263653 9.026825 0.37223382 ## m_laffer3 130.4621 30.29146 4.1045255 5.320317 11.297844 0.07145482 set.seed(24071847) PSIS_m_laffer1 &lt;- PSIS(m_laffer1,pointwise=TRUE) WAIC_m_laffer1 &lt;- WAIC(m_laffer1,pointwise=TRUE) ggplot()+ geom_point(aes(x=PSIS_m_laffer1$k , y=WAIC_m_laffer1$penalty), colour=if_else(PSIS_m_laffer1$k&gt;0.5,&quot;red&quot;,&quot;dark blue&quot;))+ geom_vline(aes(xintercept=0.5),linetype=2)+ xlab(&quot;PSIS Pareto k&quot;)+ ylab(&quot;WAIC penalty&quot;) We alter the models fit above to use a Students t distribution with shape parameter 2. data(Laffer) set.seed(100) m_laffer1_t &lt;- quap( alist( tax_revenue ~ dstudent(2,mu,sigma), mu &lt;- a + b*tax_rate, a ~ dnorm(1,0.5), b ~ dnorm(0.3,0.2), sigma ~ dexp(1) ) , data=Laffer ) laffer1_post_t &lt;- extract.samples(m_laffer1_t) set.seed(100) m_laffer2_t &lt;- quap( alist( tax_revenue ~ dstudent(2,mu,sigma), mu &lt;- a + b*tax_rate +c*tax_rate^2, a ~ dnorm(1,0.5), b ~ dnorm(0.1,1), c ~ dnorm(0.1,1), sigma ~ dexp(1) ) , data=Laffer ) laffer2_post_t &lt;- extract.samples(m_laffer2_t) laffer2_mean_t &lt;- tibble(a=mean(laffer2_post_t$a),b=mean(laffer2_post_t$b),c=mean(laffer2_post_t$c),d=mean(laffer2_post_t$d)) ggplot()+ geom_point(data=Laffer, mapping=aes(tax_rate,tax_revenue))+ geom_abline(data=laffer1_post_t, mapping=aes(slope=mean(b),intercept=mean(a)),colour=&quot;red&quot;)+ geom_function(fun = function(x) laffer2_mean_t$a + laffer2_mean_t$b*x + laffer2_mean_t$c*x^2, colour = &quot;blue&quot;)+ xlim(min(Laffer$tax_rate),max(Laffer$tax_rate)) #compare(m_laffer1_t,m_laffer2_t) compare(m_laffer1_t,m_laffer2_t, func = PSIS) ## PSIS SE dPSIS dSE pPSIS weight ## m_laffer1_t 108.2617 13.64436 0.0000000 NA 3.077506 0.6003157 ## m_laffer2_t 109.0752 12.20443 0.8135611 2.118027 3.755171 0.3996843 We no longer receive the message about high Pareto k values Now the predictions of the linear and quadratic models are very close over the whole range. The quadratic model appears to show some leveling off but there is no evidence of a decreasing relationship over the range where we have data. 7H3 Question Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species: Species A Species B Species C Species D Species E Island 1 0.20 0.20 0.20 0.200 0.200 Island 2 0.80 0.10 0.05 0.025 0.025 Island 3 0.05 0.15 0.70 0.050 0.050 Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each islands bird distribution. Interpret these entropy values. Second, use each islands bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why? Answer First we calculate the entropy of the bird distributions: island_birds_en &lt;- island_birds%&gt;%rowwise()%&gt;% mutate(Entropy= -sum( c_across(contains(&quot;Species&quot;)) * log(c_across(contains(&quot;Species&quot;))) )) kable(island_birds_en,digits=3) Species A Species B Species C Species D Species E Entropy Island 1 0.20 0.20 0.20 0.200 0.200 1.609 Island 2 0.80 0.10 0.05 0.025 0.025 0.743 Island 3 0.05 0.15 0.70 0.050 0.050 0.984 Entropy here can be interpreted as a measure of biodiversity. Island 1 has the highest entropy, with no one species dominating any other. Island 2, where species A accounts for 80% of all birds, has the lowest entropy. island_KL &lt;- function(a,b){ sum( island_birds[a,-1] * log( island_birds[a,-1] / island_birds[b,-1]) )} # island_KL(a,b) estimates the divergence of distribution in row a using the distribution in row b # Divergence from Island 1 island_KL(2,1) ## [1] 0.866434 island_KL(3,1) ## [1] 0.6258376 # Divergence from Island 2 island_KL(1,2) ## [1] 0.9704061 island_KL(3,2) ## [1] 1.838845 # Divergence from Island 3 island_KL(1,3) ## [1] 0.6387604 island_KL(2,3) ## [1] 2.010914 island_birds_KL &lt;- island_birds_en%&gt;% bind_cols( `Sum of Divergences` = c(sum(island_KL(2,1),island_KL(3,1)), sum(island_KL(1,2),island_KL(3,2)), sum(island_KL(1,3),island_KL(2,3)))) kable(island_birds_KL,digits=3) Species A Species B Species C Species D Species E Entropy Sum of Divergences Island 1 0.20 0.20 0.20 0.200 0.200 1.609 1.492 Island 2 0.80 0.10 0.05 0.025 0.025 0.743 2.809 Island 3 0.05 0.15 0.70 0.050 0.050 0.984 2.650 Island 1 is the best at predicting the other islands, because it has the highest entropy. It is the least surprised by the other islands. 7H4 Question Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models m6.9 and m6.10 again. Compare these two models using WAIC (or PSIS, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree? Answer compare(m6.9, m6.10) ## WAIC SE dWAIC dSE pWAIC weight ## m6.9 2713.759 37.50349 0.0000 NA 3.614323 1.000000e+00 ## m6.10 3102.039 27.84842 388.2804 35.39789 2.419089 4.852645e-85 Model 6.9 is expected to make better predictions, but model 6.10 provides the correct causal inference. Model 6.9 includes marriage status in the model, which is a collider of age and happiness. Including it misleads about their relationship. However, both age and marriage status are associated with happiness, and so including them both allows the model to make better predictions, hence the lower WAIC. 7H5 Question Revisit the urban fox data, data(foxes), from the previous chapters practice problems. Use WAIC or PSIS based model comparison on five different models, each using weight as the outcome, and containing these sets of predictor variables: avgfood + groupsize + area avgfood + groupsize groupsize + area avgfood area Can you explain the relative differences in WAIC scores, using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences (dSE). Answer compare(m7H5.1,m7H5.2,m7H5.3,m7H5.4,m7H5.5) ## WAIC SE dWAIC dSE pWAIC weight ## m7H5.3 323.3126 19.58726 0.000000 NA 4.566105 0.608394357 ## m7H5.1 324.4759 19.50192 1.163270 0.6420368 5.139118 0.340082478 ## m7H5.2 328.7308 18.31676 5.418239 4.0042524 4.166766 0.040516271 ## m7H5.4 331.8269 17.87634 8.514287 5.0679471 3.241450 0.008616508 ## m7H5.5 334.3913 18.30572 11.078738 5.5154867 3.561676 0.002390385 drawdag( dag_foxes ) Surprising! Comparing the WAIC difference and the standard errors of the difference, it looks like 3 is expected to be a more accurate model than 1. Both contain group size and area, but 1 also contains food. Adding a predictor can lower the expected out-of-sample performance if the predictor has little association with the outcome, but this is unusual in light of the DAG in chapter 6, which suggests that the only influence of area is via food. Its not clear why adding area would be useful if adding food is not useful. Perhaps the DAG is incorrect, and there is also a direct line from area to group size. Ill revisit this later and see if I can make sense of it. Further Reading "],["conditional.html", "Chapter 8 Conditional Manatees 8.1 Chapter Notes 8.2 Questions", " Chapter 8 Conditional Manatees # Colours by Paul Tol. # Defined in R by Joachim Goedhart # Source: doi 10.5281/zenodo.3381072 tol_light &lt;- c(&#39;#BBCC33&#39;, &#39;#AAAA00&#39;, &#39;#77AADD&#39;, &#39;#EE8866&#39;, &#39;#EEDD88&#39;, &#39;#FFAABB&#39;, &#39;#99DDFF&#39;, &#39;#44BB99&#39;, &#39;#DDDDDD&#39;) 8.1 Chapter Notes This model introduces interaction effects. The bulk of the chapter is made up of an example about the relationship between terrain ruggedness and GDP, and how this relationship changes with continent. For countries outside of Africa, terrain ruggedness is associated with a lower GDP. This relationship is reversed for countries within Africa. Heres a DAG: dag_rugged &lt;- dagitty( &quot;dag {U [unobserved];R -&gt; G; C -&gt; G; U-&gt;R; U -&gt; G}&quot;) coordinates(dag_rugged) &lt;- list( x=c(R=0,G=1,U=1,C=2) , y=c(R=0,G=0,C=0,U=1) ) drawdag(dag_rugged) R - terrain ruggedness G - GDP C - continent U - unobserved confounds that well ignore for now Note that the DAG is neutral on whether theres an interaction effect here. Continent and ruggedness could independently effect GDP, or there could be some interaction (i.e. that the way ruggedness effects GDP is dependent on continent). How do we model interaction effects? The chapter warns against splitting the data by continent, for these reasons: Some parameters (like \\(\\sigma\\)) dont depend on continent. Splitting the data makes the model less confident about these parameters unnecessarily. Without including continent in the model, you cant quantify the value of distinguishing between African and non-African nations. If we want to compare models with e.g. information criteria, the models need to all be trained on the same data set. There are other advantages to pooling data that will become clear once we reach multi-level models in chapter 13. Alright lets get started with some modelling. We start by loading and processing the data. Ruggedness &amp; GDP Model data(rugged) data_rugged &lt;- rugged %&gt;% mutate(log_gdp = log(rgdppc_2000))%&gt;% filter(!is.na(log_gdp)) %&gt;% mutate(log_gdp_std = log_gdp/ mean(log_gdp), # standardising rugged_std = rugged/ max(rugged), # keeping zero ruggedness as a reference point cid= if_else(cont_africa==1,1,2), # continent ID cid = factor(cid)) Heres our first pass model, no interaction effects: \\[ \\begin{aligned} \\log(y_i) &amp;\\sim \\text{Normal}(\\mu_i,\\sigma)\\\\ \\mu_i &amp;= \\alpha + \\beta(r_i - \\bar{r}) \\\\ \\alpha &amp;\\sim \\text{Normal}(1,0.1)\\\\ \\beta &amp;\\sim \\text{Normal}(0,0.3)\\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] The chapter does a bit of prior predictive simulation to compare the following priors: Prior Set 1 - \\(\\alpha \\sim \\text{Normal}(1.1), \\beta \\sim \\text{Normal}(0,1)\\) Prior Set 2 - \\(\\alpha \\sim \\text{Normal}(1.0.1), \\beta \\sim \\text{Normal}(0,0.3)\\) and settles on the latter. Heres a plot of 100 prior samples from each: # we generate 100 samples from each of our prior distributions priors_rugged &lt;- tibble(a=c(rnorm(100, mean = 1, sd=1),rnorm(100, mean = 1, sd=0.1)), b=c(rnorm(100, mean = 0, sd=1),rnorm(100, mean = 0, sd=0.3)), prior_set = c(rep(&quot;prior_set_1&quot;,100),rep(&quot;prior_set_2&quot;,100))) # we plot the lines they imply ggplot(priors_rugged)+ geom_abline(aes(slope = b, intercept = a - b*mean(data_rugged$rugged_std) ),alpha=0.5)+ # setting intercept so that x-axis ranges over (0,1) xlim(0,1)+ ylim(0.5, 1.5)+ geom_hline(yintercept = max(data_rugged$log_gdp_std), colour = &quot;red&quot;)+ geom_hline(yintercept = min(data_rugged$log_gdp_std), colour = &quot;red&quot;)+ xlab(&quot;ruggedness&quot;)+ ylab(&quot;log GDP (standardised)&quot;)+ facet_wrap(~prior_set) The red lines are the maximum and minimum GDP observed. Heres the model with quap, and its posterior: # regression of log gdp on ruggedness with no interactions set.seed(100) m8_1 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a + b*( rugged_std - 0.215 ) , a ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp(1) ) , data=data_rugged ) # posterior draws with tidybayes m8_1 %&gt;% gather_draws(a, b, sigma) %&gt;% mean_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 3 x 7 ## .variable .value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 a 1.00 0.983 1.02 0.89 mean qi ## 2 b 0.00150 -0.0867 0.0870 0.89 mean qi ## 3 sigma 0.137 0.125 0.149 0.89 mean qi Looking at \\(b\\), we can see that theres not much of an association between ruggedness and GDP. Now to interaction effects. We start by letting the intercept vary with continent (in Africa or not in Africa). That looks like this: \\[ \\mu_i = \\alpha_{\\text{CID}[i]} + \\beta(r_i - \\bar{r}) \\] where CID is an index variable - 1 for for African nations, 2 otherwise. Heres the model, and the posterior: set.seed(100) m8_2 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=data_rugged ) m8_2 %&gt;% gather_draws(a[CID], b, sigma) %&gt;% # is this the best way to display index variables with tidybayes? revisit mean_qi(.width=0.89)%&gt;% print(digits = 3) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 4 x 8 ## CID .variable .value .lower .upper .width .point .interval ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 a 0.880 0.854 0.905 0.89 mean qi ## 2 2 a 1.05 1.03 1.07 0.89 mean qi ## 3 NA b -0.0461 -0.118 0.0275 0.89 mean qi ## 4 NA sigma 0.112 0.103 0.122 0.89 mean qi The intercept for African nations seems to be notably lower. Now we compare the two models using WAIC: compare( m8_1 , m8_2 ) ## WAIC SE dWAIC dSE pWAIC weight ## m8_2 -251.8309 15.30478 0.00000 NA 4.442314 1.000000e+00 ## m8_1 -188.7003 13.33426 63.13055 15.18786 2.720143 1.956026e-14 The varying intercept model gets all of the model weight. Heres the posterior plot for this model. Revisit this - wrote this last year and it could be much cleaner. Would prefer to recreate it using tidybayes, ggdist. rugged_seq &lt;- seq( from=-0.1 , to=1.1 , length.out=30 ) # compute mu over samples, fixing cid=2 and then cid=1 mu_not_africa &lt;- as_tibble(link( m8_2 , data=tibble( cid=2 , rugged_std=rugged_seq ) )) mu_africa &lt;- as_tibble(link( m8_2 , data=tibble( cid=1 , rugged_std=rugged_seq ) )) not_africa_lower &lt;- purrr::map_dbl(mu_not_africa,quantile,probs=0.025,names=FALSE) not_africa_mean &lt;- purrr::map_dbl(mu_not_africa,mean) not_africa_upper &lt;- purrr::map_dbl(mu_not_africa,quantile,probs=0.975,names=FALSE) africa_lower &lt;- purrr::map_dbl(mu_africa,quantile,probs=0.025,names=FALSE) africa_mean &lt;- purrr::map_dbl(mu_africa,mean) africa_upper &lt;- purrr::map_dbl(mu_africa,quantile,probs=0.975,names=FALSE) shaded_interval &lt;- tibble(rugged = rugged_seq, na_lower = not_africa_lower, na_mean = not_africa_mean, na_upper = not_africa_upper, a_lower = africa_lower, a_mean = africa_mean, a_upper = africa_upper) rugged_precis &lt;- precis(m8_2,depth=2) ggplot(data = shaded_interval)+ geom_point(data = data_rugged, mapping = aes(x=rugged_std, y = log_gdp_std, colour = cid))+ # geom_line(aes(x=rugged,y=na_mean),colour=&quot;#00BFC4&quot;)+ # geom_line(aes(x=rugged,y=a_mean),colour=&quot;#F8766D&quot;)+ geom_abline(data=rugged_precis, aes(intercept = mean[2],slope = mean[3]), colour= &quot;#00BFC4&quot;)+ geom_abline(data=rugged_precis, aes(intercept = mean[1],slope = mean[3]), colour= &quot;#F8766D&quot;)+ geom_ribbon(aes(x=rugged,ymin=na_lower,ymax=na_upper),alpha=0.1,fill=&quot;#00BFC4&quot;)+ geom_ribbon(aes(x=rugged,ymin=a_lower,ymax=a_upper),alpha=0.1,fill=&quot;#F8766D&quot;)+ xlim(0, 1)+ xlab(&quot;Ruggedness&quot;)+ ylab(&quot;Log GDP&quot;)+ theme(legend.position = &quot;none&quot;) Our model does not allow the slope to vary by continent. Heres what we need for that: \\[ \\mu_i = \\alpha_{\\text{CID}[i]} + \\beta_{\\text{CID}[i]}(r_i - \\bar{r}) \\] And heres the model with the posterior: # model allowing slopes to vary set.seed(100) m8_3 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b[cid]*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b[cid] ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=data_rugged ) m8_3 %&gt;% gather_draws(a[CID], b[CID], sigma) %&gt;% # is this the best way to display index variables with tidybayes? revisit mean_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 5 x 8 ## CID .variable .value .lower .upper .width .point .interval ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 a 0.887 0.861 0.912 0.89 mean qi ## 2 1 b 0.133 0.0171 0.252 0.89 mean qi ## 3 2 a 1.05 1.03 1.07 0.89 mean qi ## 4 2 b -0.141 -0.225 -0.0541 0.89 mean qi ## 5 NA sigma 0.109 0.0999 0.119 0.89 mean qi Compare the slope \\(\\beta\\) for countries inside Africa (CID 1) to the slope for non-African countires (CID 2). It is reversed. We compare the models using PSIS: compare( m8_1 , m8_2 , m8_3 , func=PSIS ) ## Some Pareto k values are high (&gt;0.5). Set pointwise=TRUE to inspect individual points. ## PSIS SE dPSIS dSE pPSIS weight ## m8_3 -258.9662 15.25083 0.000000 NA 5.221461 9.716247e-01 ## m8_2 -251.8993 15.42071 7.066903 6.574883 4.430118 2.837527e-02 ## m8_1 -188.6182 13.27438 70.347931 15.482071 2.752669 5.148010e-16 As well as the table attributing almost all of the weight to the interacting slopes model, we also get a warning about some high-influence countries. Heres a plot: grid_rugged &lt;- data_grid(data_rugged, rugged_std=seq_range(rugged_std, n = 11), cid) add_predicted_draws(grid_rugged,m8_3, value = &quot;log_gdp_std&quot;)%&gt;% # naming the draw outputs to be consistent with data_rugged for convenience mutate(nation = if_else(cid==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;))%&gt;% # for plot titles ggplot(aes(x=rugged_std, y = log_gdp_std))+ stat_lineribbon()+ geom_point(data=data_rugged%&gt;% mutate(nation = if_else(cid==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;)) # need to also add nation here for facetting , aes(colour = nation))+ facet_wrap(~nation)+ scale_fill_brewer(palette = &quot;Greys&quot;)+ guides(colour=&quot;none&quot;) Now our intercepts and slopes are allowed to vary. I am a bit concerned that the uncertainty around the mean doesnt seem to change very much over the range of ruggedness. There are a lot more flat countries than rugged ones. Im pretty sure stat_lineribbon does not force the homogeneity here. Need to revisit. Cf. the plots in the chapter, page 250. Pretty confident this shouldnt be happening, and it doesnt happen when I recreate this same plot with Stan and draw samples (see the version of the same plot in the next chapter). The chapter notes that from the perspective of the model, the interactions are symmetrical. The questions: How much the association between ruggedness and GDP depend on whether the nation is in Africa?, and How much does the association between whether the nation is in Africa and GDP depend on ruggedness? cannot be distinguished by the model. The next example in the chapter concerns interaction effects between two or more continuous variables. The categorical case weve just seen means we create a new slope and intercept for each category. The continuous case is harder - we must allow the slope to vary continuously with a continuous variable. The example in the chapter concerns the size of tulip blooms under different soil and lighting conditions. We load in the data: data(tulips) data_tulips &lt;- as_tibble(tulips) %&gt;% mutate(blooms_std = blooms / max(blooms), # setting between 0 and 1 water_cent = water - mean(water), # centering on 0 shade_cent = shade - mean(shade)) Heres the model where both water and shade predict bloom size, but there are no interactions: \\[ \\begin{aligned} B_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_WW_i + \\beta_SS_i \\end{aligned} \\] After some thinking about priors, heres the model: m8_4 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) , data=data_tulips ) But we want interactions. We want the impact of changing either water or shade to be conditional on the level of the other variable. As the chapter puts it: if water is low, then decreasing the shade cant help as much as when water is high. Well end up with something like this: \\[ \\begin{aligned} B_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\gamma_{W,i}W_i + \\beta_SS_i \\\\ \\gamma_{W,i} &amp; = \\beta_W + \\beta_{WS}S_i \\end{aligned} \\] Our parameter \\(\\gamma_{W,i}\\) is now the slope that defines how the size of the blooms change with the level of water, and this depends partly on shade. The point about symmetry from earlier still holds, we can expand out the above to get: \\[ \\begin{aligned} B_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_W W + \\beta_SS_i + \\beta_{WS}S_iW_i\\\\ \\end{aligned} \\] Heres the model: m8_5 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , bws ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) , data=data_tulips ) Lets try to plot from the posterior. The way the chapter recommends doing this is by creating multiple charts to show the slopes for different levels of one of the variables. Facetting by this variable, in ggplot terms. grid_tulips &lt;- data_grid(data_tulips, # using 11 grid points for water # there are really only 3 levels in the data but I want to be able to extend this to other continuous variables water_cent=seq_range(water_cent, n = 11), # however I only need as many grid points for shade as facets I want in the plot. I&#39;ve chosen 3 shade_cent=seq_range(shade_cent, n = 3)) add_predicted_draws(grid_tulips,m8_5, value = &quot;blooms_std&quot;)%&gt;% # naming the draw outputs to be consistent with data_tulips for convenience mutate(shade_level = paste(&quot;Shade =&quot;,shade_cent))%&gt;% ggplot(aes(x=water_cent, y = blooms_std))+ stat_lineribbon()+ geom_point(data=data_tulips%&gt;% mutate(shade_level = paste(&quot;Shade =&quot;,shade_cent)) # need to create shade_level here also for facetting to work. bit clumsy. , colour = tol_light[[4]],size=2)+ facet_wrap(~shade_level)+ scale_fill_brewer(palette = &quot;Greys&quot;) Nice. When shade is lowest, water has its highest effect on bloom size. 8.2 Questions 8E1 Question For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect. Bread dough rises because of yeast. Education leads to higher income. Gasoline makes a car go. Answer Here are three hypotheses about interaction effects: The amount that yeast causes bread dough to rise depends on temperature. The effect that education has on income depends on the industry you work in. That effect that gasoline has on car speed depends on whether the engine is running. 8E2 Question Which of the following explanations invokes an interaction? Caramelizing onions requires cooking over low heat and making sure the onions do not dry out. A car will go faster when it has more cylinders or when it has a better fuel injector. Most people acquire their political beliefs from their parents, unless they get them instead from their friends. Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.). Answer This is an interaction effect. The effect of low heat on caramelisation depends on moisture. I dont know enough about car engines to know if this is an interaction effect. The question doesnt suggest that the effect of additional cylinders depends on fuel injector quality, so this wouldnt be an interaction effect. Can interpret this sentence as an interaction effect: the effect a persons parents political beliefs on them depends on whether they have adopted their friends beliefs. Dont think this suggests an interaction effect. 8E3 Question For each of the explanations in 8E2, write a linear model that expresses the stated relationship. Answer Caramelizing onions requires cooking over low heat and making sure the onions do not dry out. \\[ \\begin{aligned} &amp;\\text{Caramelisation} \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu = \\alpha + \\beta_{h} * \\text{heat} + \\beta_w * \\text{water} + \\beta_i * \\text{heat}*\\text{water} \\end{aligned} \\] A car will go faster when it has more cylinders or when it has a better fuel injector. \\[ \\begin{aligned} &amp;\\text{Speed} \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu = \\alpha + \\beta_{c} * \\text{cylinders} + \\beta_f * \\text{fuel injector quality} \\\\ \\end{aligned} \\] Most people acquire their political beliefs from their parents, unless they get them instead from their friends. \\[ \\begin{aligned} &amp;\\text{beliefs} \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu_i = \\alpha + \\beta_{p} * \\text{parents&#39; beliefs}_i + \\beta_f * \\text{friends&#39; beliefs}_i + \\beta_i * \\text{parents&#39; beliefs}_i*\\text{friends&#39; beliefs}_i \\\\ \\end{aligned} \\] Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.). \\[ \\begin{aligned} &amp;\\text{intelligence} \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu_i = \\alpha + \\beta_{s} * \\text{sociality}_i + \\beta_a * \\text{appendages}_i \\\\ \\end{aligned} \\] The statement above doesnt suggest that sociality or manipulative appendages cause intelligence. But that if you want to predict a species intelligence, having information about either of those two traits will help you. 8M1 Question Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature? Answer The effect of water and shade on the development of blooms depends on temperature. At hot temperatures, no amount of light and water will cause blooms. 8M2 Question Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot? Answer Heres the original model: \\[ \\begin{aligned} &amp;\\text{B}_i \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu = \\alpha + \\beta_{W} * \\text{W}_i + \\beta_S * \\text{S}_i + \\beta_{WS} * \\text{W}_i* \\text{S}_i \\\\ \\end{aligned} \\] Heres one that produces no blooms whenever temperature is hot: \\[ \\begin{aligned} &amp;\\text{B}_i \\sim \\text{Normal}(\\mu,\\sigma) \\\\ &amp;\\mu = \\left( alpha + \\beta_{W} * \\text{W}_i + \\beta_S * \\text{S}_i + \\beta_{WS} * \\text{W}_i* \\text{S}_i \\right) * \\text{cold} \\\\ \\end{aligned} \\] Where the cold variable can take two values, 1 for cold and 0 for hot. 8M3 Question In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a species interaction. Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not? Answer What is the outcome variable of interest? Is it population of ravens? Raven population size Wolf population size Prey population size It may not be linear - perhaps when there are few wolves, increasing the number of wolves permits the raven population to increase with the number of prey animals "],["markov_chain.html", "Chapter 9 Markov Chain Monte Carlo 9.1 Chapter Notes 9.2 Questions Further Reading", " Chapter 9 Markov Chain Monte Carlo # Colours by Paul Tol. # Defined in R by Joachim Goedhart # Source: doi 10.5281/zenodo.3381072 tol_light &lt;- c(&#39;#BBCC33&#39;, &#39;#AAAA00&#39;, &#39;#77AADD&#39;, &#39;#EE8866&#39;, &#39;#EEDD88&#39;, &#39;#FFAABB&#39;, &#39;#99DDFF&#39;, &#39;#44BB99&#39;, &#39;#DDDDDD&#39;) 9.1 Chapter Notes A Metropolis Algorithm This chapter introduces Markov chain Monte Carlo conceptually, but Im also using this chapter of my notes to try to introduce myself to working directly with Stan. The book uses a wrapper for Stan called ulam and introduces more and more direct Stan code as the book goes on. In these notes though Ill try to figure out my workflow for prior predictive checks, sampling from the posterior, prediction from the posterior, plotting, diagnostics and so on when working with stanfit models. The chapter opens with an implementation of the Metropolis algorithm, through a parable about the king of a ring of ten islands. Each week, the king decides whether to remain on his current island, or move to a neighbouring island. A proposal island is chosen by flipping a coin - either the next island clockwise or anti-clockwise from the current one. Whether the king moves to the proposal island or stays put depends on a random draw, with the probability weighted by the relative population of the island. In this example island 2 has twice as many people as island 1, island 3 has three times as many as island 1 etc. Heres the code from the book: set.seed(71) num_weeks &lt;- 1e5 # 100,000 steps in the chain positions &lt;- rep(0,num_weeks) # empty position vector current &lt;- 10 # we start on island 10 for ( i in 1:num_weeks ) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample( c(-1,1) , size=1 ) # propose either the +1 or -1 island # now make sure he loops around the archipelago if ( proposal &lt; 1 ) proposal &lt;- 10 if ( proposal &gt; 10 ) proposal &lt;- 1 # move? prob_move &lt;- proposal/current # ratio of population sizes current &lt;- ifelse( runif(1) &lt; prob_move , proposal , current ) # automatically moves if population(propsal) &gt; population(current) } position_data &lt;- tibble(week = 1:100000, position = positions) The proportion of time the king spends on each island is in proportion to its population: ggplot(position_data)+ geom_histogram(aes(x=position), bins=30) The chapter also displays the first 100 weeks so you can see the path that the king takes. Heres a little animation: anim_metrop &lt;- ggplot(position_data[1:100,])+ geom_line(aes(x=week, y=position),colour=tol_light[[4]],size=1)+ transition_reveal(week) anim_metrop &lt;- animate(anim_metrop,nframes=350,fps=20, end_pause = 50) anim_metrop # anim_save(&quot;animations\\\\09_anim_metrop.gif&quot;, animation = anim_metrop) In the context of sampling from a target distribution, the islands are parameter values, and the populations of each island are the probabilities assigned to that parameter value. Each week is a sample. Gibbs Sampling and Hamiltonian Monte Carlo The chapter briefly touches on Gibbs sampling. The proposals in the algorithm above were decided by coin flip. But too many rejected proposals can make our algorithm computationally inefficient. Gibbs sampling is more thoughtful about its proposals and is more efficient as a result. Gibbs sampling involves adaptive proposals that alters the distribution of proposed parameter values based on the current parameter values. It does this by solving for the posterior distribution of individual parameters using conjugate pairs - specific combinations of prior distributions and likelihoods. The chapter is pretty short on mathematical detail here since well be very quickly moving towards using Hamiltonian Monte Carlo. In high-dimensional problems that may well have high correlation between parameters (and therefore narrow ridges of high posterior probability), Gibbs sampling can become very inefficient. It may make too many proposals that are likely to be rejected. Gibbs sampling will also run into the problem of concentration of measure. At high dimensions most of the probability mass is far from the mode of the distribution - the combination of parameter values that maximises posterior probability. We have a high-dimensional, reasonably narrow shell of high-probability. This is exactly the kind of space that Gibbs and Metropolis sampling approaches struggle with. Hamiltonian Monte Carlo methods use local knowledge of the shape (i.e. gradient) of the posterior to make smart proposals. Imagine a skateboarder in a bowl. To get a sample from the distribution (bowl shape), the skateboarder kicks in a random direction with a random momentum. The rest of the work is done by gravity. After a bit of time the skateboard stops and records their position. This is a sample. Well collect more sample near the bottom of the bowl than near the top - this is the high-probability region. In order the skateboard to move according to the bowl-shape, it doesnt need to know anything about the entire shape of the bowl, just the local gradient. Each proposal should be accepted, but in practice checks are done to ensure conservation of e.g. the total energy in the system. If it has changed, something has gone wrong somewhere. HMC needs two settings: Number of leapfrog steps - each path between samples is broken up into a number of steps where the trajectory is (re-)calculated. Step size - these steps can be shorter or longer. In Stan, there is a long warm-up phase which will decide these two parameters. Another thing Stan can do is notice when the path is turning around and arriving very close to the original point - this is called U-turning and it can make the sampling inefficient. Stan notices this happening and tunes the number of leapfrog steps to prevent it. Revisit - there is a long code implementation of HMC in the chapter. Give it a try. When working with Stan well need to start doing two things: Pre-processing any variable transformations to save computation Removing columns from the data frame if they will not be included in the model. The chapter introduces the ulam tool for fitting Hamiltonian Monte Carlo (HMC) models in Stan. However Id like to try working in raw Stan. We load the ruggedness data from chapter 8 and fit the interaction model, this time using HMC instead of quadratic approximation. ## Loading data and processing data(rugged) data_rugged &lt;- rugged %&gt;% mutate(log_gdp = log(rgdppc_2000))%&gt;% filter(!is.na(log_gdp)) %&gt;% mutate(log_gdp_std = log_gdp/ mean(log_gdp), # standardising rugged_std = rugged/ max(rugged), # keeping zero ruggedness as a reference point cid= if_else(cont_africa==1,1,2), # continent ID cid = factor(cid))%&gt;% select(log_gdp_std, rugged_std, cid) # removing columns that won&#39;t be in the model The model in chapter 8, fit using quadratic approximation looks like this: set.seed(100) m8_3 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b[cid]*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b[cid] ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=data_rugged ) Here is the code for the same model in Stan: code_m9_1 &lt;- &quot;data{ vector[170] log_gdp_std; vector[170] rugged_std; int cid[170]; } parameters{ vector[2] a; vector[2] b; real&lt;lower=0&gt; sigma; } model{ vector[170] mu; sigma ~ exponential( 1 ); b ~ normal( 0 , 0.3 ); a ~ normal( 1 , 0.1 ); for ( i in 1:170 ) { mu[i] = a[cid[i]] + b[cid[i]] * (rugged_std[i] - 0.215); } log_gdp_std ~ normal( mu , sigma ); }&quot; What is this code doing? We have three blocks: a data block, a parameters block, and a model block. The Data Block The observed variables are named, their data types are specified, and so are their sizes. The line vector[170] log_gdp_std means that we have a vector of real values with length 170, and we are naming it log_gdp_std. The line int cid[170] means we have an integer variable with length 170 named CID. Each line is ended by a semi-colon. The Parameters Block In our interaction model, we specified two intercepts and two slopes. Thats why we have vector[2] in this block for parameters \\(a\\) and \\(b\\). We have assigned an exponential prior to sigma, which has the support set of the positive real-numbers. The line real&lt;lower=0&gt; constrains sigma to be positive. The Model Block This block computes the log-probability of the data. It runs from top to bottom, first introducing \\(\\mu\\). Each line with ~ in it adds a term to the log-probability. Then it runs a loop to define our 170 \\(\\mu\\)s. The final line for log_gdp_std is vectorised. Heres the code for compiling the model and sampling: # stan_model compiles the model # the data is not used and no samples are drawn m9_1 &lt;- stan_model(model_name = &quot;m9_1&quot;,model_code=code_m9_1)%&gt;% # compose_data is a tidybayes function sampling(data = compose_data(data_rugged), chains=4, cores=parallel::detectCores())%&gt;% # sampling draws samples from the posterior recover_types() # tidybayes function useful when we have more complex indexing than this model # The output is now a stanfit object that contains the draws from the posterior # The function stan() does both the compiling and fitting but I&#39;ll keep them separate while I&#39;m still new to Stan. # saveRDS(m9_1,file=&quot;models\\\\m9_1.rds&quot;) We can use tidybayes to extract the draws in tidy format: # Extracting Draws m9_1 %&gt;% spread_draws(a[CID], b[CID], sigma)%&gt;% head(10) ## Warning: `gather_()` was deprecated in tidyr 1.2.0. ## Please use `gather()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## # A tibble: 10 x 7 ## # Groups: CID [1] ## CID a .chain .iteration .draw b sigma ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.886 1 1 1 0.0773 0.113 ## 2 1 0.870 1 2 2 0.214 0.110 ## 3 1 0.893 1 3 3 0.0645 0.114 ## 4 1 0.888 1 4 4 0.0449 0.107 ## 5 1 0.867 1 5 5 -0.0146 0.114 ## 6 1 0.920 1 6 6 0.193 0.105 ## 7 1 0.851 1 7 7 0.0157 0.112 ## 8 1 0.858 1 8 8 -0.0334 0.109 ## 9 1 0.885 1 9 9 0.300 0.115 ## 10 1 0.887 1 10 10 0.266 0.123 The summarise_draws() function can be used to quickly get convergence diagnostics like rhat, ess_bulk and ess_tails see here for details on these diagnostics. m9_1 %&gt;% spread_draws(a[CID], b[CID], sigma)%&gt;% summarise_draws()%&gt;% print(digits = 3) ## # A tibble: 6 x 11 ## # Groups: CID [2] ## CID variable mean median sd mad q5 q95 rhat ess_bulk ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 a 0.887 0.887 0.0164 0.0163 0.860 0.914 1.00 4558. ## 2 1 b 0.132 0.132 0.0776 0.0771 0.00389 0.261 1.00 4192. ## 3 1 sigma 0.112 0.111 0.00621 0.00617 0.102 0.122 1.00 4462. ## 4 2 a 1.05 1.05 0.0101 0.0101 1.03 1.07 1.00 4599. ## 5 2 b -0.144 -0.144 0.0558 0.0546 -0.236 -0.0503 1.00 4528. ## 6 2 sigma 0.112 0.111 0.00621 0.00617 0.102 0.122 1.00 4462. ## # ... with 1 more variable: ess_tail &lt;dbl&gt; We can also summarise the posterior uses any of the point interval functions tidybayes provides: m9_1 %&gt;% gather_draws(a[CID], b[CID], sigma)%&gt;% mean_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 5 x 8 ## CID .variable .value .lower .upper .width .point .interval ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 a 0.887 0.861 0.913 0.89 mean qi ## 2 1 b 0.132 0.00794 0.258 0.89 mean qi ## 3 2 a 1.05 1.03 1.07 0.89 mean qi ## 4 2 b -0.144 -0.233 -0.0530 0.89 mean qi ## 5 NA sigma 0.112 0.102 0.122 0.89 mean qi Compare against the model fit with quadratic approximation: m8_3 %&gt;% gather_draws(a[CID], b[CID], sigma) %&gt;% # is this the best way to display index variables with tidybayes? revisit mean_qi(.width=0.89)%&gt;% print(digits = 3) ## # A tibble: 5 x 8 ## CID .variable .value .lower .upper .width .point .interval ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 a 0.887 0.861 0.912 0.89 mean qi ## 2 1 b 0.133 0.0171 0.252 0.89 mean qi ## 3 2 a 1.05 1.03 1.07 0.89 mean qi ## 4 2 b -0.141 -0.225 -0.0541 0.89 mean qi ## 5 NA sigma 0.109 0.0999 0.119 0.89 mean qi They are very similar for this model. Posterior Predictive Checks with Stan Weve just seen the information we can extract using spread_draws() and gather_draws() from tidybayes. Its a little bit harder to sample from the linear predictor or the posterior predictive distribution with stanfit models. Im sure Ill refine (or totally overhaul) this as I get more experience (e.g. I believe the generated quantities block can do some of the below) but heres what Im doing just now: # Create a grid of data points to predict over grid_rugged &lt;- data_grid(data_rugged, rugged_std=seq_range(rugged_std, n = 21)) # Create draws of the parameters in the model # You can use the tidybayes function get_variables() to list all the variables you can include draws_rugged &lt;- m9_1 %&gt;% spread_draws(a[CID], b[CID], sigma)%&gt;% mutate(CID=as.factor(CID),ndraws=50) # Stitch these two data frames together. pred_rugged &lt;- inner_join(grid_rugged, draws_rugged, by=character())%&gt;% # cross product of the two data frames rowwise()%&gt;% # Rowwise for rnorm to work mutate(linpred = a + b*(rugged_std-0.215), # create the linear predictor prediction = withr::with_seed(71,rnorm(1,linpred,sigma)), # sample from the posterior predictive. seed = 71 nation = if_else(CID==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;))%&gt;% ungroup() We can then use this data frame of posterior predictions in plots as before. This code is more or less copied from the code I wrote in chapter 8 for the quap model. ggplot(data=pred_rugged,aes(x=rugged_std, y = prediction))+ stat_lineribbon()+ geom_point(data=data_rugged%&gt;% mutate(nation = if_else(cid==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;), prediction = log_gdp_std) # need to also add nation here for faceting , aes(colour = nation))+ facet_wrap(~nation)+ scale_fill_brewer(palette = &quot;Greys&quot;)+ guides(colour=&quot;none&quot;) Prior Predictive Checks with Stan I put this section after the posterior checks because one of the approaches Ill take requires extracting samples. Again Im sure Ill adapt or totally overhaul this approach as I get more experience. There arent a lot of very detailed guides on how to actually code prior predictive checks for raw Stan models. As far as I can tell there are two ways of doing prior predictive checks: Build the possibility into your model and then use MCMC to sample from the prior Write a model with similar structure using the generated quantities block only and use Stans fixed parameter sampler Heres an example of the first approach: code_m9_1_adapted &lt;- &quot;data{ vector[170] log_gdp_std; vector[170] rugged_std; int cid[170]; int&lt;lower = 0, upper = 1&gt; run_estimation; // a switch to evaluate the likelihood } parameters{ vector[2] a; vector[2] b; real&lt;lower=0&gt; sigma; } model{ vector[170] mu; sigma ~ exponential( 1 ); b ~ normal( 0 , 0.3 ); a ~ normal( 1 , 0.1 ); for ( i in 1:170 ) { mu[i] = a[cid[i]] + b[cid[i]] * (rugged_std[i] - 0.215); } if(run_estimation==1){ log_gdp_std ~ normal( mu , sigma ); // here I&#39;ve implemented the switch } }&quot; The difference between the original code for this model above (code_m9_1) and this code is the switch to turn off the likelihood. I read this approach on the Modern Statistical Workflow blog. When the likelihood is turned off the model doesnt condition on the data (in this case log_gdp_std) at all and so just returns samples from the prior. Heres how that works in the code: # We compile the model - this could be used for either prior or posterior prediction compile_m9_1_adapted &lt;- stan_model(model_name = &quot;m9_1_adapted&quot;,model_code=code_m9_1_adapted) # We sample from the prior. In the data argument of sampling(), we need to add the new variable run_estimation # If we want to sample from the prior we set it to 0, from the posterior we set it to 1. m9_1_adapted &lt;- sampling(object = compile_m9_1_adapted, data = c(compose_data(data_rugged),run_estimation = list(0)), # set run_estimation to 0 chains=4,cores=parallel::detectCores())%&gt;% recover_types() # saveRDS(m9_1_adapted,file=&quot;models\\\\m9_1_adapted.rds&quot;) Then we can more or less use the same code as in the posterior predictive section to take a look at the implications of these priors: # Create a grid of data points to predict over grid_rugged &lt;- data_grid(data_rugged, rugged_std=seq_range(rugged_std, n = 21)) # Create draws of the parameters in the model # You can use the tidybayes function get_variables() to list all the variables you can include draws_prior_rugged &lt;- m9_1_adapted %&gt;% spread_draws(a[CID], b[CID], sigma)%&gt;% mutate(CID=as.factor(CID),ndraws=50) # Stitch these two data frames together. pred_prior_rugged &lt;- inner_join(grid_rugged, draws_prior_rugged, by=character())%&gt;% # cross product of the two data frames rowwise()%&gt;% # Rowwise for rnorm to work mutate(linpred = a + b*(rugged_std-0.215), # create the linear predictor prediction = rnorm(1,linpred,sigma), nation = if_else(CID==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;))%&gt;% ungroup() ggplot(data=pred_prior_rugged,aes(x=rugged_std, y = prediction))+ stat_lineribbon()+ geom_point(data=data_rugged%&gt;% mutate(nation = if_else(cid==1,&quot;African Nations&quot;,&quot;Non-African Nations&quot;), prediction = log_gdp_std) # need to also add nation here for faceting , aes(colour = nation))+ facet_wrap(~nation)+ scale_fill_brewer(palette = &quot;Greys&quot;)+ guides(colour=&quot;none&quot;) Compare this plot to the one for the posterior above. The benefit of this approach is that you only need to maintain one version of model code, instead of separate code for prior prediction and for your actual model. The downside is that its less efficient to sample using MCMC than just drawing from a model coded for this purpose using Stan _rng functions and the fixed parameter sampler. Thats the approach well look at now. The second approach is to make use of the generated quantities block in the Stan code. This one will take me a while to start doing competently, at the moment Ive written almost no Stan code. The Stan User Guide contains an example of doing this. Heres some code. Its not working just now, will revisit. code_m9_1_prior &lt;- &quot;data{ vector[170] rugged_std; } generated quantities{ real a = normal_rng(1, 0.1); real b = normal_rng(0, 0.3); real sigma = exponential_rng(1); array[170] real y_sim = normal_rng(a + b * (rugged_std-0.215),sigma); }&quot; compile_m9_1_prior &lt;- stan_model(model_name = &quot;m9_1_prior&quot;,model_code=code_m9_1_prior) Stan Forum Question Cross Validated Example Diagnostics &amp; Comparison - Trace Plots, WAIC &amp; PSIS The loo package contains functions for WAIC and PSIS. Heres the vignette. Revisit this and add in an example. The rstan package contains some functions for easy plotting. See here. One of these function plots the trace plot for a model. The trace plot plots the samples for each parameter in order and connects them with a line. Each chain is shown in a different colour. stan_trace(m9_1) Heres what were looking for, according to the chapter: Stationarity - the path of each chain should stay within the same high-probability region Good Mixing - the chain should rapidly explore the full region, zig-zagging around instead of meandering Convergence - each of the chains should be exploring the same region The chapter includes an example of a model with very flat priors and very little data, in order to demonstrate how you may be able to tell if your attempt at model fitting has gone wrong somewhere. code_m9_2 &lt;- &quot;data{ vector[2] y; } parameters{ real a; real&lt;lower=0&gt; sigma; } model{ real mu; sigma ~ exponential( 0.0001 ); a ~ normal( 0 , 1000 ); mu = a; y ~ normal( mu , sigma ); }&quot; y &lt;- c(-1,1) # our data m9_2 &lt;- stan_model(model_name = &quot;m9_2&quot;,model_code=code_m9_2)%&gt;% sampling(data = compose_data(y), chains=4, cores=detectCores())%&gt;% recover_types() # saveRDS(m9_2,file = &quot;models\\\\m9_2.rds&quot;) Heres the trace plot: stan_trace(m9_2) The chains here are not stationary, and they do not converge to the same region of high probability. They are a warning that something has gone wrong. In this case the issue can be fixed by using even slightly informative priors. Another way that model fitting can go wrong is with non-identifiable parameters. We saw this in the leg length example in chapter 6. Revisit - page 293. Care and Feeding of Your Markov Chain This section of the chapter includes a helpful outline for using Hamiltonian Monte Carlo. How many samples are needed? What really matters is the number of effective samples - accounting for autocorrelation. Stan provides a measure for effective sample size with bulk and tail ESS. The number of samples required also depends on what we want to know. Well need more samples if we want detailed information on the tails of the distribution than if we only care about the posterior means. It also depends on the shape of the distribution, with approximately normal distributions needing fewer. The recommendation here suggests a minimum of 100 for bulk and tail ESS per chain but you may want quite a bit more. What warm-up setting should be used? If youre having issues, you might try increasing the number of warm-ups. The default is the number of samples (default 2000) divided by two. How many chains do you need? The chapter suggests three answers: First use a single, short chain when initially debugging a model. There are apparently some error messages that will only display if there is only one chain. Second, when deciding whether the chains are valid, you need more than one. E.g. for a traceplot or other convergence diagnostics. Third, when youre sure that your model is working and you want to make the final run that youll draw inferences from, you only need one chain. You can still use more if you want. Just note that multiple chains will duplicate the warm-up effort, however youll also be able to split the work over multiple cores. 9.2 Questions Revisit. 9E1 Further Reading This paper introduces \\(\\hat{R}\\), for diagnosing convergence: Gelman and Rubin (1992) Inference from iterative simulation using multiple sequences. "],["big_entropy.html", "Chapter 10 Big Entropy and the Generalized Linear Model 10.1 Chapter Notes 10.2 Questions Further Resources", " Chapter 10 Big Entropy and the Generalized Linear Model # Colours by Paul Tol. # Defined in R by Joachim Goedhart # Source: doi 10.5281/zenodo.3381072 tol_light &lt;- c(&#39;#BBCC33&#39;, &#39;#AAAA00&#39;, &#39;#77AADD&#39;, &#39;#EE8866&#39;, &#39;#EEDD88&#39;, &#39;#FFAABB&#39;, &#39;#99DDFF&#39;, &#39;#44BB99&#39;, &#39;#DDDDDD&#39;) 10.1 Chapter Notes The chapter title is pretty straightforward: this one discusses maximum entropy arguments and introduces GLMs. Maximum Entropy The chapter introduces a justification for maximum entropy approaches that appears in Jaynes Probability Theory. Jaynes attributes the approach to Graham Wallis. We have \\(m\\) different possibilities, and we want to assign probabilities \\(\\{ p_1, \\dots, p_m \\}\\) to them, with the probabilities summing to 1. We want to do this by making use of some information \\(I\\) that we have. The following is outlined in an Overthinking box in the chapter. Theres a footnote there to page 352 of Jaynes Probability Theory, which is where the detail below (and more!) comes from. Jaynes describes a thought experiment in which a blindfolded person throws pennies into \\(m\\) equal boxes, so that any penny has an equal chance of landing in any of the boxes. The person throws some large number \\(n &gt;&gt; m\\) of pennies and at the end we count up all the pennies in each box, divide by the total number of pennies and take this to be the probability assigned to the boxes by our experiment. For each box \\(i = 1,2,\\dots,m\\) \\[ p_i = \\frac{n_i}{n} \\] where \\(n_i\\) is the observed number of pennies in box \\(i\\). This is our candidate probability distribution. The probability of any particular assignment is given by the multinomial distribution: \\[ m^{-n} \\frac{n!}{n_1! \\dots n_m!}. \\] After the experiment, we check whether the candidate probability distribution is consistent with our information \\(I\\). If not, we ask the blindfolded person to try again until a distribution is accepted. The most likely probability distibution to result from this game we say is the fairest, subject to our information constraints. What is the most likely probability distribution to be chosen by this experiment? The answer is whatever one maximises \\[ W = m^{-n} \\frac{n!}{n_1! \\dots n_m!} \\] subject to the constraints of \\(I\\). This is equivalent to finding the distribution which maximises \\(\\frac{1}{n} \\log(W)\\): \\[ \\begin{aligned} \\frac{1}{n} \\log(W) &amp;= \\frac{1}{n} \\left( \\log(n!) - \\log(n_1!) - \\dots - \\log(n_m!) \\right)\\\\ &amp;= \\frac{1}{n} \\left( n \\log(n) - n + \\sqrt{2 \\pi n} + \\frac{1}{12n} + \\mathcal{O}\\left(\\frac{1}{n^2}\\right) \\right) &amp;&amp; (\\text{Stirling approximation for } \\log(n!))\\\\ &amp;\\quad - \\frac{1}{n} \\left( n_1 \\log(n_1) - n_1 + \\sqrt{2 \\pi n_1} + \\frac{1}{12n_1} + \\mathcal{O}\\left(\\frac{1}{n_1^2}\\right) \\right) \\\\ &amp;\\vdots \\\\ &amp;\\quad- \\frac{1}{n} \\left( n_m \\log(n_m) - n_m + \\sqrt{2 \\pi n_m} + \\frac{1}{12n_m} + \\mathcal{O}\\left(\\frac{1}{n_m^2}\\right) \\right) \\\\ &amp;= \\left( \\log(n) - 1 + \\sqrt{2 \\pi \\frac{1}{n}} + \\mathcal{O}\\left(\\frac{1}{n^2}\\right) \\right) \\\\ &amp;\\quad- \\left( p_1 \\log(np_1) - p_1 + \\sqrt{2 \\pi \\frac{1}{n}p_1} + \\frac{1}{12n^2p_1} + \\mathcal{O}\\left(\\frac{1}{n_1^2}\\right) \\right) \\\\ &amp;\\vdots \\\\ &amp;\\quad- \\left( p_m \\log(np_m) - p_m + \\sqrt{2 \\pi \\frac{1}{n}p_m} + \\frac{1}{12n^2p_m} + \\mathcal{O}\\left(\\frac{1}{n_m^2}\\right) \\right) \\\\ &amp;\\to -\\sum p_i \\log(p_i) +\\log(n) -\\sum p_i \\log(n) - 1 + \\sum p_i \\\\ &amp;= -\\sum p_i \\log(p_i) \\end{aligned} \\] with the limit taken as \\(n \\to \\inf\\) and \\(n_i \\to \\inf\\) so that \\(p_i\\) remains constant. Weve recovered the formula for information entropy introduced in Chapter 7. The chapter then goes on to introduce proofs that the Gaussian distribution is the maximum entropy distribution given only a finite variance, and that the binomial is the maximum entropy distribution given only some constant expected value and two unordered possible events. First the Gaussian. Heres the probability density function of the Gaussian: \\[ p(x) = (2 \\pi \\sigma^2)^{-1/2} \\exp \\left( - \\frac{(x- \\mu)^2}{2 \\sigma^2} \\right) \\] and its entropy: \\[ H(p) = - \\int p(x) \\log p(x) dx = \\frac{1}{2} \\log(2 \\pi e \\sigma^2) \\] We want to consider \\(q(x)\\), some other probability density function with the same variance \\(\\sigma^2\\). The basic structure of this proof is that we reintroduce KL divergence from Chapter 7 Revisit. Generalized Linear Models This part of the chapter extends the notion of a linear model weve been working with so far to include non-Gaussian likelihoods. There is an introduction to the exponential family, and then a discussion of two common link functions that well be using over the rest of the book: the logit link and the log link. So far we have assumed a Gaussian distribution for the outcome of our models like so: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta x_i \\end{aligned} \\] but this is not the best choice when our outcome variable is bounded (like a proportion or a probability), or when it is discrete. Otherwise we could have a model that predicts that the Earth is 130% covered in water, or similar. General linear model approaches generalise the linear models weve seen so far. Well let our outcome variable be, say, binomially distributed and our linear predictor will we some function of, say, the \\(p\\) parameter of our binomial: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Binomial}(n,p_i) \\\\ f(p_i) &amp;= \\alpha + \\beta x_i \\end{aligned} \\] The chapter recommends a maximum entropy approach to selecting the probability distribution that youll use for the outcome. The other new item in the model above is the function \\(f()\\). This is the link function. We need this because the parameter that were feeding into our new distribution may not represent the mean value any more, and it also may have constraints that our linear model \\(\\alpha + \\beta x_i\\) may not satisfy. For example, we cant feed a \\(p_i\\) to our binomial distribution unless it is between zero and 1. The job of the link function is to map the linear space of \\(\\alpha + \\beta x_i\\) onto the non-linear space of the parameter were using. The logit link is used for parameters that represent probabilities, and that therefore must be between 0 and 1. Our models will look like this: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Binomial}(n,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha + \\beta x_i \\end{aligned} \\] with the logit function representing the log odds like so: \\[ \\text{logit}(p_i) = \\log \\frac{p_i}{1 - p_i} \\] So in this model, our parameter \\(p_i\\) is the inverse-logit transform of the linear model: \\[ p_i = \\frac{\\exp(\\alpha + \\beta x_i)}{1 + \\exp(\\alpha + \\beta x_i)}. \\] This function is called the logistic, or in GLM contexts, the inverse logit. Heres a plot: data_logit &lt;- tibble(n=1:101, predictor_value = seq(from = -1, to = 1, length.out=101), without_link = 2*predictor_value, # simple linear model logit_link = exp(2*predictor_value)/(1+exp(2*predictor_value))) %&gt;% # applying the inverse logit function pivot_longer(c(without_link,logit_link), names_to= &quot;space&quot;,values_to = &quot;parameter_value&quot;) ggplot(data=data_logit,aes(x=predictor_value,y=parameter_value))+ geom_path(colour=tol_light[[4]],size=1)+ geom_hline(data = data_logit %&gt;% filter(n%%3==0), # display every third row in the data as a horizontal line aes(yintercept=parameter_value), alpha=0.4)+ facet_wrap(~fct_rev(space), scales=&quot;free&quot;) The linear model \\(2x\\) ranges over the log odds space (LHS of plot): \\[ \\text{logit}(p_i) = \\log \\frac{p_i}{1-p_i}= 2x_i \\] After applying the inverse logit, we can see that our parameter \\(p\\) ranges over the probability space \\([0,1]\\) (RHS of plot). The horizontal lines are there to show the compression that happens as we get closer to either 0 or 1. From the chapter: This compression does affect interpretation of parameter estimates, because no longer does a unit change in a predictor variable produce a constant change in the mean of the outcome variable. Instead, a unit change in \\(x_i\\) may produce a larger or smaller change in the probability \\(p_i\\), depending upon how far from zero the log-odds are When an event is almost guaranteed to happen, its probability cannot increase very much, no matter how important the predictor may be. The other link function introduced in this chapter is the log link. The log link function is for parameters that are only defined over positive real numbers. E.g. \\[ \\begin{aligned} y_i &amp;\\sim \\text{Normal}(\\mu,\\sigma) \\\\ \\log(\\sigma_i) &amp;= \\alpha + \\beta x_i \\end{aligned} \\] By definition, \\(\\sigma\\) cannot be negative, and the log transform keeps this from happening. In the model above, our \\(sigma\\) is modelled as the exponentiation of the linear model, since our inverse link function is exponentiation. Heres a plot similar to the one for the logit: data_log &lt;- tibble(n=1:101, predictor_value = seq(from = -1, to = 1, length.out=101), without_link = 2*predictor_value, # simple linear model log_link = exp(2*predictor_value)) %&gt;% # applying the inverse log function exp() pivot_longer(c(without_link,log_link), names_to= &quot;space&quot;,values_to = &quot;parameter_value&quot;) ggplot(data=data_log,aes(x=predictor_value,y=parameter_value))+ geom_path(colour=tol_light[[4]],size=1)+ geom_hline(data = data_log %&gt;% filter(n%%3==0), # display every third row in the data as a horizontal line aes(yintercept=parameter_value), alpha=0.4)+ facet_wrap(~fct_rev(space), scales=&quot;free&quot;) We can see potential trouble here - when asking a model to predict outside of the range of the data it was trained on, the log link can create implausible outputs because of the exponential relationship. The chapter ends with a couple of 10.2 Questions There are no questions at the end of this chapter. Further Resources On the link between Bayesian conditioning and entropy maximisation: Williams (1980): Bayesian Conditionalisation and the Principle of Minimum Information (http://www.yaroslavvb.com/papers/williams-conditionalization.pdf) Caticha, A. and Griffin, A. (2007). Updating probabilities. In Mohammad-Djafari, A., editor, Bayesian Inference and Maximum Entropy Methods in Science and Engineering, volume 872 ofAIP Conf. Proc. Griffin (2008): Maximum Entropy: The Universal Method for Inference (https://arxiv.org/ftp/arxiv/papers/0901/0901.2987.pdf) Conrads paper deriving various maximum entropy distributions. https://kconrad.math.uconn.edu/blurbs/analysis/entropypost.pdf Work through this and fill out the Gaussian and Binomial arguments above. "],["god_spiked.html", "Chapter 11 God Spiked the Integers 11.1 Chapter Notes 11.2 Questions Further Resources", " Chapter 11 God Spiked the Integers 11.1 Chapter Notes Binomial Regression The chapter introduces a case where we might want to use logistic regression. It describes an experiment in which chimpanzees were given the option to pull one of two levers (left or right). Each lever would deliver a tray to the chimpanzee, and also to the opposite end of the table where a partner chimpanzee may or may not be sitting. In all cases the trays delivered to the lever-pulling chimpanzee would contain food, but only one of the levers would deliver food to the partner. The aim of the experiment was to determine whether chimpanzees were more likely to pull the lever that delivers food to the other end of the table if a partner chimpanzee was present. We start by loading the data and defining an index variable (treatment) that takes digits 1-4 with the following meaning: Right-hand lever delivers food to both ends of the table, and no partner is present. Left-hand lever delivers food to both ends of the table, and no partner is present. Right-hand lever delivers food to both ends of the table, and a partner is present. Left-hand lever delivers food to both ends of the table, and a partner is present. We model the outcome that the left lever is pulled like so: \\[ \\begin{aligned} L_i &amp;\\sim \\text{Binomial}(1,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha_{actor[i]} + \\beta_{treatment[i]} \\\\ \\alpha_j &amp;\\sim \\text{Normal}(0,1.5) \\\\ \\beta_k &amp;\\sim \\text{Normal}(0,0.5) \\end{aligned} \\] In R: set.seed(100) m11.4 &lt;- ulam( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a[actor] + b[treatment] , a[actor] ~ dnorm( 0 , 1.5 ), b[treatment] ~ dnorm( 0 , 0.5 ) ) , data=data_chimp , chains=4 , log_lik=TRUE, cmdstan = TRUE ) After transforming them into the outcome scale, we can plot the parameters that represent each chimpanzee: ## mean sd 5.5% 94.5% ## a1 0.3906872 0.07593051 0.2713121 0.5170867 ## a2 0.9742894 0.01716667 0.9429495 0.9940104 ## a3 0.3222523 0.06929863 0.2169651 0.4433149 ## a4 0.3234947 0.07037946 0.2213532 0.4426394 ## a5 0.3900974 0.07652739 0.2717665 0.5146987 ## a6 0.6101797 0.07560688 0.4854886 0.7234966 ## a7 0.8694970 0.04508907 0.7920538 0.9310104 Here, values close to zero indicate a preference for the right lever, and values close to one a preference for the left lever. And here is the same graph for the treatment effects: Here R and L refer to which level was the pro-social option - right or left. N and P refer to whether a partner was present. If the chimps in this experiment exhibited pro-social behaviour, wed expect that the pro-social lever would be pulled more often in the presence of a partner. I.e. we want to compare R/P against R/N, and L/P against L/N. We can see that when right is the pro-social option there is a slight tendency for the chimps to pull the right lever more when a partner is present. There is no similar tendency to pull the left lever more when a partner is present when the left lever is the pro-social choice. Revisit: Recreate Figure 11.4? 11.1.1 Poisson Regression The chapter introduces the Poisson distribution, and then sets up an example model using data on tool use among historical societies in Oceania. We want to model tool use among these societies, with predictors populations size, and amount of contact with other populations. Our model is: \\[ \\begin{aligned} T_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\log \\lambda_i &amp;= \\alpha_{\\text{CID}[i]}+ \\beta_{\\text{CID}[i]} \\log P_i \\\\ \\alpha_j &amp;\\sim \\text{Normal}(3,0.5) \\\\ \\beta_j &amp;\\sim \\text{Normal}(0,0.2)\\\\ \\end{aligned} \\] Here it is in R: set.seed(100) m11.10 &lt;- ulam( alist( T ~ dpois( lambda ), log(lambda) &lt;- a[cid] + b[cid]*P, a[cid] ~ dnorm( 3 , 0.5 ), b[cid] ~ dnorm( 0 , 0.2 ) ), data=data_tool , chains=4 , log_lik=TRUE, cmdstan = TRUE ) We plot the posterior predictions: Here blue dots are high contact, and red low contact societies. The size of the points is scaled by Pareto k-value. Revisit: Theory-based model - include Negative Binomial Models The chapter introduces an extension of the Poisson generalised linear model that uses the negative binomial distribution. This adds the ability to adjust our model for data over varying exposures. A toy example is introduced to explain. We own a monastery that produces manuscripts at a rate \\(\\lambda\\) of 1.5 per day. We simulate data over a month: set.seed(47) num_days &lt;- 30 y &lt;- rpois(num_days, 1.5) We are considering acquiring a new monastery, and want to compare its productivity. However this one does not keep a daily record of manuscript production, but instead a weekly one. The exposure is different: seven days instead of one. Our task will be to model the rate of manuscript production at each monastery in order to inform our purchasing decision. The (unknown to us) daily rate of the second monastery is actually 0.5 manuscripts per day, and we simulate 4 weeks worth of data on that basis: set.seed(47) num_weeks &lt;- 4 y_new &lt;- rpois(num_weeks, 0.5*7) We collect these two sets of data into one data frame. data_manu &lt;- tibble(y_all = c(y,y_new), exposure = c(rep(1,30),rep(7,4)), monastery = c(rep(0,30),rep(1,4))) # monastery indicator The introduction of a new term into our model allows us to compare rates across our varying exposures. This term is the logarithm of the exposure. data_manu &lt;- data_manu %&gt;% mutate(log_exp = log(exposure)) set.seed(100) m11.12 &lt;- quap( alist( y ~ dpois( lambda ), log(lambda) &lt;- log_exp + a + b*monastery, a ~ dnorm( 0 , 1 ), b ~ dnorm( 0 , 1 ) ), data=data_manu ) Why does the addition of this term adjust for the varying exposures? If we think about \\(\\lambda\\) as a rate we can express it as a number of manuscripts \\(\\mu\\) produced over a number of days \\(\\tau\\): \\(\\lambda = \\mu/\\tau\\). If we return to the definition of the Poisson GLM with the log link function we can see how this helps us to scale our rate parameter to adjust for the varying exposures: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\log \\lambda_i &amp;= \\log \\left( \\frac{\\mu_i}{\\tau_i} \\right)= \\alpha + \\beta x_i \\\\ \\implies \\log \\lambda_i &amp;= \\log\\mu_i - \\log\\tau_i = \\alpha + \\beta x_i \\\\ \\implies \\log\\mu_i &amp;= \\log\\tau_i + \\alpha + \\beta x_i \\\\ \\end{aligned} \\] We define a new model with the exposures on the daily scale. \\[ \\begin{aligned} y_i &amp;\\sim \\text{Poisson}(\\mu_i) \\\\ \\log \\mu_i &amp;= \\log \\tau_i + \\alpha + \\beta x_i \\\\ \\end{aligned} \\] When the exposure \\(\\tau_i\\) equals one, \\(\\log(\\tau_i) = 0\\) and we get back the initial model. We can now compare the production of the two monasteries: ## mean sd 5.5% 94.5% ## lambda_old 1.2398507 0.2001562 0.9464707 1.5844527 ## lambda_new 0.4151877 0.1213752 0.2502575 0.6281128 These are daily rates. We can see that the new monastery is about a third as productive as the old, and we can adjust the price were willing to pay accordingly. Multinomial and Categorical Models The chapter introduces the multi-nomial distribution as an extension of the binomial. It has probability mass function: \\[ \\text{Pr}(y_i, \\dots , y_K | n, p_i, \\dots , p_K) = \\frac{n!}{\\prod_i y_i!} \\prod^K_{i=1}p_i^{y_i} \\] Here there are \\(K\\) kinds of events (not just two) and we observe \\(y_i\\) events of each type \\(i\\) over \\(n\\) total trials. Imagine an urn filled with balls of \\(K\\) different colours. We pull \\(n\\) balls from the urn with replacement and count up how many of each colour we get. The \\[ \\frac{n!}{\\prod_i y_i!} \\] term is analogous to the \\[ {n \\choose y} = \\frac{n!}{y! (n-y)!} \\] term in the binomial PMF. The equivalent to the inverse logit function we used in the binomial case is called the softmax function, and it looks like this: \\[ \\text{Pr}(k| s_1, s_2, \\dots, s_K) = \\frac{\\exp(s_k)}{\\sum^K_{i=1} \\exp(s_i)} \\] where \\(s_i\\) is a score assigned to event type \\(i\\). To illustrate, the chapter introduces a simulated example. We are trying to model career choice in 500 young adults. There are three career options, each comes with its own expected income. The following code assigns an income to each career option, converts this to a score, and converts the score to a set of probabilities using the softmax function. Then the 500 individuals pick one of the three options, with the choice weighted by the calculated probabilities. We end up with a vector of length 500, where each entry is one of the three career options. N &lt;- 500 income &lt;- c(1,2,5) score &lt;- income*0.5 p &lt;- softmax(score[1],score[2],score[3]) career &lt;- rep(0,N) set.seed(34302) for (i in 1:N){ career[i] &lt;- sample( 1:3, size =1, prob = p) } The chapter presents the code for the multi-nomial model in raw Stan code. This is the first model written in raw Stan in the book. code_m11.13 &lt;- &quot; data{ int N; // number of individuals int K; // number of possible careers int career[N]; // outcome vector[K] career_income; } parameters{ vector[K-1] a; // intercepts real&lt;lower=0&gt; b; // association of income with choice } model{ vector[K] p; vector[K] s; a ~ normal( 0 , 1 ); b ~ normal( 0 , 0.5 ); s[1] = a[1] + b*career_income[1]; s[2] = a[2] + b*career_income[2]; s[3] = 0; // pivot p = softmax( s ); career ~ categorical( p ); } &quot; The string of code is fed to Stan like so: data_career &lt;- list( N=N , K=3 , career=career , career_income=income ) set.seed(100) m11.13 &lt;- stan( model_code=code_m11.13 , data=data_career , chains=4 ) precis( m11.13 , 2 ) Revisit: I got lost here. Return after attempting some questions. 11.2 Questions 11E1 Question If an event has probability 0.35, what are the log-odds of this event? Answer We expect a ratio of 35 successes to 65 failures, which equates to odds of \\(\\frac{35}{65} = \\frac{7}{13}\\). Taking the natural log of this value gives \\(-0.62\\). 11E2 Question If an event has log-odds 3.2, what is the probability of this event? Answer \\[ \\begin{aligned} \\exp(3.2) = 24.53 &amp;= \\frac{p}{1-p} \\\\ \\implies p &amp;= 0.96 \\end{aligned} \\] 11E3 Question Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome? Answer This question asks us to compute the relative effect of a parameter. If we exponentiate the coefficient then we get the proportional odds: $ exp(1.7) = 5.47 $ which suggests a 447% increase in the odds of the outcome when we increase the parameter in question by one unit. As outlined in the Overthinking box on page 337, this works because the ratio in odds that we get with a one unit increase in the parameter is: \\[ \\begin{aligned} q &amp;= \\frac{\\exp(\\alpha + \\beta(x_i + 1) )}{\\exp(\\alpha + \\beta(x_i))} = \\frac{\\exp(\\alpha) \\exp(\\beta x_i) \\exp(\\beta)}{\\exp(\\alpha) \\exp(\\beta x_i)} \\\\ &amp;= \\exp(\\beta) \\end{aligned} \\] 11E4 Question Why do Poisson regressions sometimes require the use of an offset? Provide an example. Answer Sometimes we get count data reported with varying exposures. The example in the chapter in one monastery reports daily counts of manuscripts produced, and one reports weekly. The offset allows us to compare rates across varying exposures. The offset is the logarithm of the exposure. Why does the offset adjust for the varying exposures? If we think about \\(\\lambda\\) as a rate we can express it as a number of manuscripts \\(\\mu\\) produced over a number of days \\(\\tau\\): \\(\\lambda = \\mu/\\tau\\). If we return to the definition of the Poisson GLM with the log link function we can see how this helps us to scale our rate parameter to adjust for the varying exposures: \\[ \\begin{aligned} \\log \\lambda_i &amp;= \\log \\left( \\frac{\\mu_i}{\\tau_i} \\right)= \\alpha + \\beta x_i \\\\ \\implies \\log \\lambda_i &amp;= \\log\\mu_i - \\log\\tau_i = \\alpha + \\beta x_i \\\\ \\implies \\log\\mu_i &amp;= \\log\\tau_i + \\alpha + \\beta x_i \\\\ \\end{aligned} \\] We can then define a new model with the exposures on the daily scale. \\[ \\begin{aligned} y_i &amp;\\sim \\text{Poisson}(\\mu_i) \\\\ \\log \\mu_i &amp;= \\log \\tau_i + \\alpha + \\beta x_i \\\\ \\end{aligned} \\] 11M1 Question As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why? Answer Lets follow the example explanation in the chapter (page 339) and talk about 9 trials with 6 successes. The likelihood of this data in the aggregate model is \\[ \\text{Pr}(6|9,p) = \\frac{6!}{6!(9-6)!}p^6(1-p)^{9-6} \\] The fraction on the right hand side is \\({9 \\choose 6}\\) which multiplies the likelihood by the number of different ways you could see 6 successes in 9 trials. The joint probability of the same disaggregated data is \\[ \\text{Pr}(1,1,1,1,1,1,0,0,0,p) = p \\times p \\times p \\times p \\times p \\times p \\times (1-p) \\times (1-p) \\times (1-p) =p^6(1-p)^{9-6}. \\] 11M2 Question If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome? Answer In a Possion regression with a log link our parameter is the exponentiation of the linear model: \\[ \\log(\\mu_i) = \\alpha + \\beta x_i \\] with a one unit increase in the parameter we get \\[ \\begin{aligned} \\frac{\\exp(\\alpha + \\beta(x_i + 1) )}{\\exp(\\alpha + \\beta(x_i))} = \\frac{\\exp(\\alpha) \\exp(\\beta x_i) \\exp(\\beta)}{\\exp(\\alpha) \\exp(\\beta x_i)} = \\exp(\\beta) \\end{aligned} \\] Our outcome value has been increased by a factor of \\(\\exp(\\beta)\\). In this case if the coefficient has value 1.7, then an increase of one unit in the parameter translates to an outcome value that has increased by a factor of 5.47. 11M3 Question Explain why the logit link is appropriate for a binomial generalized linear model. Answer In a binomial GLM we have observed a number of trials where there are two possible outcomes, and we are looking to make inferences about the unobserved underlying probabilities that influence these outcomes. These probabilities must be between zero and one, and an inverse-logit transform of the linear model will constrain the parameter to these values. 11M4 Question Explain why the log link is appropriate for a Poisson generalized linear model. Answer In a Poisson GLM our observed outcomes are counts that occur over time or space, and we are looking to make inferences about the unobserved underlying rates that influence these outcomes. A rate must be non-negative, and exponentiation (inverse log) of the linear model will constrain the parameter to these values. 11M5 Question What would it imply to use a logit link for the mean of a Poisson generalized linear model? Can you think of a real research problem for which this would make sense? Answer You would be constraining the mean rate to be between 0 and 1. If your research question is considering many small intervals, where for each interval the probability of observing an event is low then a logit function would be suitable. 11M6 Question State the constraints for which the binomial and Poisson distributions have maximum entropy. Are the constraints different at all for binomial and Poisson? Why or why not? Answer The binomial distribution has maximum entropy with constraints: two unordered events constant expected value The Poisson distribution is the binomial distribution as \\(n \\to \\infty\\) and \\(p \\to 0\\) as \\(np\\) remains constant. If \\(n\\) is large and \\(p\\) small enough to model with a Poisson distribution, it will have maximum entropy under the same constraints. 11M7 Question Use quap to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, m11.4. Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the approximate and the MCMC distributions? Relax the prior on the actor intercepts to Normal(0,10). Re-estimate the posterior using both ulam and quap. Do the differences increase or decrease? Why? Answer Heres m11.4 using quap instead of ulam: set.seed(100) m11.4.quap &lt;- quap(alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a[actor] + b[treatment] , a[actor] ~ dnorm( 0 , 1.5 ), b[treatment] ~ dnorm( 0 , 0.5 ) ), data=data_chimp) And here are the two posterior plots side by side: Here is the same chart after relaxing the actor intercept prior to Normal(0,10). Cant quite figure out whats going on here. Dont think Ive made a coding error but will have to revisit later. 11M8 Question Revisit the data(Kline) islands example. This time drop Hawaii from the sample and refit the models. What changes do you observe? Answer Here is a comparison of the posterior predictions with and without Hawaii: A couple things have changed here. The model is now a lot less confident about high population, low contact societies - the red compatibility interval gets much wider. Also, as before the model expects that low contact societies will develop fewer tools than high contact societies, except now the prediction is uniform across the data range. Previously the model predicted that over a certain population size more tools would be produced by low contact societies. We now have reason to believe that this crossover point is an artifact of including Hawaii, with its large population and large number of tools, and the lack of data on any large population, high contact societies. One annoying thing about removing Hawaii is that it changes the automatic scaling on the data point sizes - I should revisit this later to set the scaling manually for consistency. 11H1 Question Use WAIC or PSIS to compare the chimpanzee model that includes a unique intercept for each actor, m11.4 (page 330), to the simpler models fit in the same section. Interpret the results. Answer Here are the models well be comparing: 11.1 - model with no predictors and flat priors 11.2 - model includes treatment (but not actor) as predictor, flat priors 11.3 - model includes treatment (but not actor) as predictor, more informative priors 11.4 - model includes treatment and actor as predictors. more informative priors ## WAIC SE dWAIC dSE pWAIC weight ## m11.4.quap 532.4388 18.538215 0.0000 NA 8.166138 1.000000e+00 ## m11.3 682.3300 9.019331 149.8912 18.02068 3.548242 2.828404e-33 ## m11.2 682.9577 9.689085 150.5189 18.12860 3.951869 2.066520e-33 ## m11.1 688.0776 7.155477 155.6388 18.58770 1.068659 1.597607e-34 It looks like including treatment effect does improve expected accuracy of the model, but its nowhere close to as important as including actor. This suggests that the most important predictor of a chimp pulling the left lever is simply handedness, rather than presence / absence of a partner and food for them. Comparing model 2 to 3, it also looks like we have enough data here to overwhelm even very bad priors. 11H2 Question The data contained in library(MASS);data(eagles) are records of salmon pirating attempts by Bald Eagles in Washington State. See ?eagles for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the victim and the thief the pirate. Use the available data to build a binomial GLM of successful pirating attempts. Consider the following model: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Binomial}(n_i,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha + \\beta_P P_i + \\beta_V V_i + \\beta_A A_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0,1.5) \\\\ \\beta_P, \\beta_V, \\beta_A &amp;\\sim \\text{Normal}(0,0.5) \\end{aligned} \\] where \\(y\\) is the number of successful attempts, \\(n\\) is the total number of attempts, \\(P\\) is a dummy variable indicating whether or not the pirate had large body size, \\(V\\) is a dummy variable indicating whether or not the victim had large body size, and finally \\(A\\) is a dummy variable indicating whether or not the pirate was an adult. Fit the model above to the eagles data, using both quap and ulam. Is the quadratic approximation okay? Now interpret the estimates. If the quadratic approximation turned out okay, then its okay to use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions. Compute and display both: the predicted probability of success and its 89% interval for each row in the data, as well as the predicted success count and its 89% interval. What different information does each type of posterior prediction provide? Now try to improve the model. Consider an interaction between the pirates size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret. Answer Here the first letter refers to the size of the pirate, large or small. The second refers to whether the pirate is adult or immature. The third refers to the size of the victim. The plot of probabilities contains information about the proportion of successes expected in each scenario. The count predictions contain information on the number of attempts. The count plot can be thought of as predicting the number of successes for each scenario for 160 trials total. Heres a model with an interaction effect between the pirates size and age: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Binomial}(n_i,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha + (\\beta_P + \\beta_A A_i)P_i + \\beta_V V_i \\\\ \\alpha &amp;\\sim \\text{Normal}(0,1.5) \\\\ \\beta_P, \\beta_V, \\beta_A &amp;\\sim \\text{Normal}(0,0.5) \\end{aligned} \\] Heres a comparison of the two models by WAIC: ## WAIC SE dWAIC dSE pWAIC weight ## m11.H2b 58.97597 11.46574 0.000000 NA 8.280315 0.765901 ## m11.H2c 61.34659 15.25543 2.370618 5.891377 7.982785 0.234099 11H3 Question The data contained in data(salamanders) are counts of salamanders (Plethodon elongatus) from \\(47\\) different \\(49m^2\\) plots in northern California. The column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. You will model SALAMAN as a Poisson variable. Model the relationship between density and percent cover, using a log-link (same as the example in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing quap to ulam. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? A bad job? Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction? Answer This is the second questions that asks me to check the performance of the quadratic approximation against Hamiltonian Monte Carlo in ulam before looking at parameter estimates, and Im not sure how to do this. Ive been warned against model comparison using WAIC or PSIS when using two different algorithms. I could use a pairs plot to check whether the posterior distribution looks broadly Gaussian. Need to revisit this. Also Ive been a bit lazy with my use of ulam over this chapter. I should really be pre-processing all of my variables and only feeding ulam a list of the data I want it to use, rather than a data frame that contains unnecessary columns. Hopefully this will speed up and set me up well for using data with varying lengths once I get to multi-level models. Alright, this starts off as a fairly straightforward-looking Poisson model, with only percentage ground cover as a predictor. I think it should look like this: \\[ \\begin{aligned} y_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\log(\\lambda_i) &amp;= \\alpha + \\beta_{C}(x_i-\\bar{x}) \\end{aligned} \\] We use the log link since we want our salamander estimates to be non-negative. Here is my data prep: data(&quot;salamanders&quot;) # scaling PCTCOVER to be between 0 and 1 and then centering. data_sal &lt;- as_tibble(salamanders)%&gt;% mutate(cov_cen = scale(PCTCOVER)) list_sal &lt;- with(data_sal,list(sal =SALAMAN,cov_cen = cov_cen)) And now for some prior simulation. After some messing around I settled on Normal(1.2,1) for the intercept, and Normal(0,0.2) for \\(\\beta_C\\). These prior simulations are displayed two different ways. On the left is a density plot of the number of salamanders in the intercept only model when \\(a \\sim \\text{Normal}(1.2,1)\\). The plot on the right shows 50 simulations of how the number of salamanders might vary with the amount of ground cover, when \\(a\\) is as above and \\(b \\sim \\text{Normal}(0,0.2)\\). Now to fit the model and plot the results. I think the model does a good job of capturing the broad relationship here: salamanders like ground cover. The model does a poor job of capturing the variation above 75%. The next part of the question asks us to add age of trees into the model, using any model you think useful. Just for fun, I might create a new index variable for cover (high or low, with the boundary at 75%) and then plot salamander population against forest age. Here red is low cover, blue is high cover. The slope should be able to vary here, it looks like forest age doesnt seem to have much effect no matter the level of cover. 11H4 Question The data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 20102012 (see van der Lee and Ellemers (2015) for data and context). These data have a very similar structure to the UCBAdmit data discussed in the chapter. I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWOs goal is to equalize rates of funding between men and women, what type of intervention would be most effective? Answer G - Gender D - Department A - Award Ill start with a model that only includes gender, and not department. This will give us an estimate of the total effect of gender. Here are the results: ## mean sd 5.5% 94.5% ## diff_a 0.20646480 0.10435218 0.039312400 0.37026665 ## diff_p 0.02810227 0.01410845 0.005329711 0.05045711 On the probability scale, applications from women are 1-5% less likely to succeed. Now we add department to the model, blocking the pipe to estimate the direct effects on gender. And the results: ## mean sd 5.5% 94.5% ## diff_a 0.1415749 0.10123312 -0.021963000 0.30244495 ## diff_p 0.0247863 0.01901147 -0.003853548 0.05708239 See comment above, unsure about this one. revisit this. 11H5 Question Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from the previous problem. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the backdoor criterion. If you have trouble thinking this though, try simulating fake data, assuming your DAG is true. Then analyze it using the model from the previous problem. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence? Answer Further Resources On the link between Bayesian conditioning and entropy maximisation: Williams (1980): Bayesian Conditionalisation and the Principle of Minimum Information (http://www.yaroslavvb.com/papers/williams-conditionalization.pdf) Caticha, A. and Griffin, A. (2007). Updating probabilities. In Mohammad-Djafari, A., editor, Bayesian Inference and Maximum Entropy Methods in Science and Engineering, volume 872 ofAIP Conf. Proc. Griffin (2008): Maximum Entropy: The Universal Method for Inference (https://arxiv.org/ftp/arxiv/papers/0901/0901.2987.pdf) Conrads paper deriving various maximum entropy distributions. https://kconrad.math.uconn.edu/blurbs/analysis/entropypost.pdf Work through this and fill out the Gaussian and Binomial arguments above. An example of multinomial logistic regression in the literature: See Koster and McElreath (2017) for a published Stan example with varying effects, applied to behavioral choice. (https://pure.mpg.de/rest/items/item_2479179_5/component/file_2479178/content) Subject of question 11.H.2 on logistic regression: Knight, R. L. and Skagen, S. K. (1988) Agonistic asymmetries and the foraging ecology of Bald Eagles. Ecology 69, 11881194. "],["monsters_mixtures.html", "Chapter 12 Monsters and Mixtures 12.1 Chapter Notes 12.2 Questions Further Resources", " Chapter 12 Monsters and Mixtures 12.1 Chapter Notes Over-Dispersed Counts The chapter opens with a discussion of over-dispersion in count data - when the data exhibits more variation than can be explained by a binomial or Poisson distribution. Well try to address this using two types of continuous mixture models - beta-binomial and negative-binomial models. The beta-binomial distribution is the binomial distribution, except that instead of the probability of success p being fixed, it is drawn from some beta distribution. The chapter example returns to the UCB admissions data from the previous chapter, except this time we allow each row of the data (i.e. each department / gender combination) is allowed to have a different probability of admission - drawn from a beta distribution. Id previously seen beta distributions with \\(\\alpha\\) and \\(\\beta\\) parametrisation, but the chapter uses \\(\\bar{p}\\) and \\(\\theta\\), with \\(\\alpha = \\bar{p}\\theta\\) and \\(\\beta = (1-\\bar{p})\\theta\\). Here \\(\\bar{p}\\) is the average probability and \\(\\theta\\) is a shape parameter. Here is the model used for the UCB data: \\[ \\begin{aligned} A_i &amp;\\sim \\text{BetaBinomial}(N_i,\\bar{p_i},\\theta) \\\\ \\text{logit}(\\bar{p_i}) &amp;= \\alpha_{\\text{gen}[i]} \\\\ \\alpha_j &amp;\\sim \\text{Normal}(0,1.5) \\\\ \\theta &amp; \\sim \\phi + 2 \\\\ \\phi &amp; \\sim \\text{Exponential}(1) \\end{aligned} \\] The higher the value of \\(\\theta\\), the more concentrated the probability. When \\(\\bar{p}_i\\) is 0.5, a \\(\\theta\\) of 2 gives a completely flat distribution. This is why \\(\\theta\\) is assigned a minimum of two in the model above. We fit the model, and examine the posterior: ## mean sd 5.5% 94.5% n_eff Rhat4 ## a[1] -0.4378644 0.4127803 -1.12690970 0.2218039 1646.246 0.9991368 ## a[2] -0.3299027 0.4180197 -0.99721864 0.3426233 1615.236 0.9988416 ## phi 0.9966260 0.7846515 0.07746409 2.3980537 1659.046 1.0004995 ## theta 2.9966260 0.7846514 2.07745965 4.3980537 1659.046 1.0004995 The probability of admission increases with the value of \\(\\alpha\\). The difference between \\(\\alpha\\) for men and women is mean(post_UCB$diff_a) ## [1] -0.1079617 suggesting the model believes women are more likely to be admitted. However the standard deviation of this value is 0.5710928;the model is very uncertain. We contrast this with model m11.7 in the last chapter, which predicted that men were more likely to be admitted, and was quite a bit more confident about this. Even though we havent included department in the model, allowing \\(p\\) to vary by department / gender combination has captured some of the variation between departments. Heres a plot: The chapter then moves on to the use of negative binomial (or gamma-Poisson) continuous mixture models to address over-dispersion. These are Poisson models, where the rate is allowed to vary across observations by drawing it from a gamma distribution. The gamma-Poisson distribution has two parameters, one is a rate parameter \\(\\lambda\\) and one (\\(\\phi\\)) controls the variance. The distribution has \\(\\text{var} = \\lambda + \\frac{\\lambda^2}{\\phi}\\) so smaller \\(\\phi\\) implies larger variance. The chapter refits the tool data from chapter 11 with a gamma-Poisson distribution, the idea is that we expect an outlier point like Hawaii to become less influential, because the model can accommodate more variation (in a Poisson distribution the variance necessarily equals the mean). Here are the posterior plots of the tools model using a Poisson distribution, and using the gamma-Poisson: Here blue dots are high contact, and red low contact societies. The size of the points is scaled by Pareto k-value. The gamma-Poisson is less influenced by Hawaii, and consequently much more uncertain in large populations. Zero-Inflated Outcomes The zero-inflated Poisson model is introduced as an example of a mixture model: models that use multiple probability distribution to measure the influence of more than one cause. With zero-inflation, we aim to model a count variable where zeros can be produced in more than one way. In the monastery example in the chapter, each day monks have a fixed probability of taking the day off (maybe they spend the day drinking wine). On these days they will produce zero manuscripts. If they do work, they will produce some (low) number of manuscripts over the course of the day, and this might also be zero (maybe they just finished a bunch). So a zero can be produced two ways (broadly, as the outcome of a binomial process, or a Poisson process). The chapter introduces the zero-inflated Poisson distribution: a binomial / Poisson mixture. The probability of a zero is: \\[ \\begin{aligned} \\text{Pr}(0|p,\\lambda) &amp; = \\text{Pr}(\\text{drink}|p) + \\text{Pr}(\\text{work}|p) \\times \\text{Pr}(0|\\lambda) \\\\ &amp;= p + (1-p)\\exp(-\\lambda) \\end{aligned} \\] and the probability of some non-zero figure is: \\[ \\begin{aligned} \\text{Pr}(y &gt; 0|p,\\lambda) &amp; = \\text{Pr}(\\text{work}|p) \\times \\text{Pr}(y|\\lambda) \\\\ &amp;= (1-p)\\frac{\\lambda^y \\exp(-\\lambda)}{y!} \\end{aligned} \\] The formulas here come the Poisson likelihood (rate \\(\\lambda\\)) and the binomial (probability \\(p\\) of taking the day off). A zero-inflated Poisson model will look something like this, for some predictor x: \\[ \\begin{aligned} y_i &amp; \\sim \\text{ZIPoisson}(p_i,\\lambda_i) \\\\ \\text{logit}(p_i)&amp;= \\alpha_p + \\beta_p x_i\\\\ \\log(\\lambda_i)&amp;= \\alpha_\\lambda + \\beta_\\lambda x_i\\\\ \\end{aligned} \\] The chapter expands on zero-inflation Poisson models by simulating some data from our fictional monastery, fitting a model, and attempting to recover the data-generating process. Ordered Categorical Outcomes Here the outcome we want to predict is made up of some number of categories, like a multinomial. Except that the categories are ordered, e.g. an approval rating from 1 (strongly disapprove) to 5 (strongly approve). The ordering is important, but the scale is not necessarily linear, and so shouldnt be modelled as a continuous outcome. The way of dealing with this described in the chapter is to use a log cumulative odds function, as we have used the log odds link in previous chapters. The chapter introduces a trolley problem example where respondents grade the moral permissability of action in a scenario on a scale of 1 to 7. Here Ive reproduced some charts in the chapter that show the counts of each response, the cumulative proportion, and then the log cumulative odds. data(Trolley) data_trol &lt;- as_tibble(Trolley)%&gt;% mutate(response = as_factor(response), prop = ) plot_trol_hist &lt;- ggplot(data_trol)+ geom_histogram(aes(x = response), stat=&quot;count&quot;) hist_trol &lt;- data_trol%&gt;% group_by(response)%&gt;% summarise(n = n())%&gt;% mutate(prop = n / sum(n), prop_cum = cumsum(prop), log_cum_odds = log(prop_cum / (1- prop_cum))) plot_trol_prop &lt;- ggplot(hist_trol, mapping = aes(x = response, y = prop_cum,group = 1))+ geom_line()+ geom_point()+ ylab(&quot;cumulative proportion&quot;) plot_trol_odd &lt;- ggplot(hist_trol%&gt;%filter(log_cum_odds!= Inf), mapping = aes(x = response, y = log_cum_odds,group = 1))+ geom_line()+ geom_point()+ ylab(&quot;log cumulative odds&quot;) plot_grid(plot_trol_hist,plot_trol_prop,plot_trol_odd,nrow =1) A model with no predictors is introduced, to check that we can recover the cumulative proportions in the data in the posterior distribution: m12.4 &lt;- ulam( alist( R ~ dordlogit( 0 , cutpoints ), cutpoints ~ dnorm( 0 , 1.5 ) ) , data=list( R=data_trol$response ), chains=4 , cores=4,cmdstan = TRUE ) # cumulative proportions in the data round(hist_trol$prop_cum, 3) # model expectations for cumulative proportions round( inverse_logit(coef(m12.4)) , 3 ) Then the chapter explains how to include predictors in this kind of model. The log cumulative odds for each response k is modelled as a linear combination of its intercept \\(\\alpha_k\\) and a standard linear model: \\[ \\begin{aligned} \\log\\frac{\\text{Pr}(y_i \\leq k)}{1 - \\text{Pr}(y_i \\leq k)} &amp;= \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta x_i \\end{aligned} \\] The subtraction is conventional, to ensure that positive \\(\\beta\\) means the predictor \\(x\\) is positively associated with the outcome \\(y\\). The model actually used for the trolley data looks like this: \\[ \\begin{aligned} \\log\\frac{\\text{Pr}(y_i \\leq k)}{1 - \\text{Pr}(y_i \\leq k)} &amp;= \\alpha_k - \\phi_i \\\\ \\phi_i &amp; = \\beta_A A_i + \\beta_C C_i + \\beta_{I,i} I_i \\\\ \\beta_{I,i} &amp;= \\beta_I + \\beta_{IA} A_i + \\beta_{IC} C_i \\end{aligned} \\] Here: * \\(A_i\\) is the value of action on row \\(i\\), 0 or 1. * \\(I_i\\) is the value of intention on row \\(i\\), 0 or 1. * \\(C_i\\) is the value of contact on row \\(i\\), 0 or 1. * \\(B_I,i\\) introduces an interaction effect between intention and action, and intention and contact. list_trol &lt;- with(data_trol, list(R = response, A = action, C = contact, I = intention)) m12.5 &lt;- ulam( alist( R ~ dordlogit( phi , cutpoints ), phi &lt;- bA*A + bC*C + BI*I , BI &lt;- bI + bIA*A + bIC*C , c(bA,bI,bC,bIA,bIC) ~ dnorm( 0 , 0.5 ), cutpoints ~ dnorm( 0 , 1.5 ) ) , data=list_trol , chains=4 , cores=4, cmdstan = TRUE ) ggplot(data=precis(m12.5))+ geom_pointrange(aes(x=rownames(precis(m12.5)),y=mean,ymin=`5.5%`,ymax=`94.5%`))+ xlab(&quot;parameter&quot;)+ coord_flip() We can see that all of the predictors (action - bA, contact - bC, intention - bI) are all negatively associated with permissability. Need to revisit this for posterior plots. And also the section on ordered categorical predictors. 12.2 Questions 12E1 Question Answer Further Resources "],["models_memory.html", "Chapter 13 Models With Memory 13.1 Chapter Notes 13.2 Questions Further Resources", " Chapter 13 Models With Memory 13.1 Chapter Notes This chapter introduces multi-level models, starting with an example using tadpole mortality data. Each row in the data set is a bucket that starts off with some number of tadpoles, each bucket has its own experimental conditions (number of tadpoles, presence / absence of predators etc.) and at the end the number of surviving tadpoles are counted. The chapter explains that we do not want to assign the same intercept estimate to each of the buckets - we dont want to accidentally mask any variation that may be due to some of our measured variables. But we also dont want to assign each bucket its own independent intercept - learning about one bucket should tell us something about the next. This is the motivation for multi-level models - in particular we start off with a varying intercepts model. Compare the kind of model we would try to fit in previous chapters: \\[ \\begin{aligned} S_i &amp;\\sim \\text{Binomial}(N_i,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha_{\\text{TANK}[i]} \\\\ \\alpha_j &amp;\\sim \\text{Normal}(0,1.5) \\end{aligned} \\] With the varying intercepts model: \\[ \\begin{aligned} S_i &amp;\\sim \\text{Binomial}(N_i,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha_{\\text{TANK}[i]} \\\\ \\alpha_j &amp;\\sim \\text{Normal}(\\bar{\\alpha},\\sigma) \\\\ \\bar{\\alpha} &amp; \\sim \\text{Normal}(0,1.5) \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] In the first model survival is modelled as binomial, each tank is assigned its own intercept, with each of these intercepts sharing the same fixed prior. In the multi-level model, the intercept prior is a function of two hyperpriors, \\(\\bar{\\alpha}\\) and \\(\\sigma\\). The model updates both levels of the model as it sees the data. We fit both models and compare. compare(m13.1,m13.2) ## WAIC SE dWAIC dSE pWAIC weight ## m13.2 199.8403 7.029993 0.00000 NA 20.81333 0.9995300343 ## m13.1 215.1651 4.569440 15.32476 3.704447 25.86906 0.0004699657 The measure of effective parameters, pWAIC is lower for the multi-level model (m13.2), because of the stronger regularising effect of the hyperpriors. Here is a plot of the posterior of the multi-level model: The blue points are the survival proportions in the raw data, the black circles are the survival proportions estimates by the model. The dashed line is the average survival proportion across all tanks. The survival estimates are pulled towards the mean, and this effect is particularly strong when: a point is far from the mean in small tanks, where there are fewer tadpoles and so the model is more sceptical of the data (in light of the experience of the other tanks). More Than One Type of Cluster The chapter reintroduces the chimpanzee example. We will add clustering according to actor (the chimp pulling levers) and experimental block. Heres the model: \\[ \\begin{aligned} L_i &amp;\\sim \\text{Binomial}(1,p_i) \\\\ \\text{logit}(p_i) &amp;= \\alpha_{\\text{ACTOR}[i]} + \\gamma_{\\text{BLOCK}[i]} + \\beta_{\\text{TREATMENT}[i]} \\\\ \\beta_j &amp;\\sim \\text{Normal}(0,0.5) &amp;&amp; \\text{for } j = 1 \\dots 4 \\\\ \\alpha_j &amp;\\sim \\text{Normal}(\\bar{\\alpha},\\sigma_\\alpha) &amp;&amp; \\text{for } j = 1 \\dots 7\\\\ \\gamma_j &amp;\\sim \\text{Normal}(0,\\sigma_\\gamma) &amp;&amp; \\text{for } j = 1 \\dots 6\\\\ \\bar{\\alpha} &amp; \\sim \\text{Normal}(0,1.5) \\\\ \\sigma_\\alpha &amp;\\sim \\text{Exponential}(1) \\\\ \\sigma_\\gamma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] Whats happening here? The chapter explains: Each cluster gets its own vector of parameters. For actors, the vector is \\(\\alpha\\), and it has length 7, because there are 7 chimpanzees in the sample. For blocks, the vector is \\(\\gamma\\), and it has length 6, because there are 6 blocks. Each cluster variable needs its own standard deviation parameter that adapts the amount of pooling across units, be they actors or blocks. These are \\(\\sigma_\\alpha\\) and \\(\\sigma_\\gamma\\), respectively. Finally, note that there is only one global mean parameter \\(\\bar{\\alpha}\\). We cant identify a separate mean for each varying intercept type, because both intercepts are added to the same linear prediction. We fit the model and plot the posterior: We can see that there is much more variation among actors (\\(\\sigma_\\alpha\\)) than among blocks (\\(\\sigma_\\gamma\\)). Divergent Transitions and Non-Centered Priors Divergent transitions occur quite frequently in multi-level models. The chapter introduces two methods of dealing with these: Increasing Stans target acceptance rate, which results in a smaller step size. Reparameterisation of the model, to use non-centered priors We start by increasing the target acceptance rate, to 99% compared to the ulam default 95%: set.seed(13) m13.4b &lt;- ulam( m13.4 , chains=4 , cores=4 , control=list(adapt_delta=0.99), cmdstan = TRUE ) divergent(m13.4b) Creating a non-centered version of the chimp models requires taking \\(\\bar{\\alpha}\\), \\(\\sigma_\\alpha\\) and \\(\\sigma_\\gamma\\) out of the intercepts: \\[ \\begin{aligned} \\alpha_j &amp;\\sim \\text{Normal}(\\bar{\\alpha},\\sigma_\\alpha)\\\\ \\gamma_j &amp;\\sim \\text{Normal}(0,\\sigma_\\gamma) \\end{aligned} \\] Heres the non-centered parameterisation of the model: \\[ \\begin{aligned} L_i &amp;\\sim \\text{Binomial}(1,p_i) \\\\ \\text{logit}(p_i) &amp;= \\bar{\\alpha} + z_{\\text{ACTOR}[i]} \\sigma_\\alpha + x_{\\text{BLOCK}[i]} \\sigma_\\gamma + \\beta_{\\text{TREATMENT}[i]} \\\\ \\beta_j &amp;\\sim \\text{Normal}(0,0.5) &amp;&amp; \\text{for } j = 1 \\dots 4 \\\\ z_j &amp;\\sim \\text{Normal}(0,1) \\\\ x_j &amp;\\sim \\text{Normal}(0,1) \\\\ \\bar{\\alpha} &amp; \\sim \\text{Normal}(0,1.5) \\\\ \\sigma_\\alpha &amp;\\sim \\text{Exponential}(1) \\\\ \\sigma_\\gamma &amp;\\sim \\text{Exponential}(1) \\end{aligned} \\] The actor and block intercepts have been standardised, and are now transformed in the linear model instead. We plot the effective number of parameters of the centred and non-centred models against each other: Each point is a parameter. Points above the line suggest the non-centered model performed better. 13.2 Questions 13E1 Question Answer Further Resources "],["covariance.html", "Chapter 14 Adventures in Covariance 14.1 Chapter Notes 14.2 Questions Further Resources", " Chapter 14 Adventures in Covariance 14.1 Chapter Notes Varying Slopes by Construction The chapter introduces a simulation exercise to explain varying effects models. We have a population of cafes, and are interested in waiting times. As in the previous chapter, well allow intercepts to vary, with partial pooling across cafes. But were also interested in the effect of the predictor afternoon (i.e. whether you are getting coffee in the morning or afternoon). We want to also allow the slopes to vary, and to pool across cafes. This is a varying effects strategy. More than this, the key addition here is that we also want to allow our intercepts and slopes to covary, pooling information across intercepts and slopes. Were going to use a multi-variate normal distribution to generate a population of cafes. We need a vector of means and a variance-covariance matrix: a &lt;- 3.5 b &lt;- (-1) sigma_a &lt;- 1 sigma_b &lt;- 0.5 rho &lt;- (-0.7) Mu &lt;- c(a,b) Where \\(a\\) is average morning wait time \\(b\\) is average difference in wait time between morning and afternoon we have the standard deviations in the intercepts and slopes \\(\\rho\\) is correlation between intercepts and slopes \\(\\mu\\) is the vector of means We could build the variance covariance matrix directly it should look like this: \\[ \\begin{pmatrix} \\sigma_\\alpha^2 &amp; \\sigma_\\alpha \\sigma_\\beta \\rho \\\\ \\sigma_\\alpha \\sigma_\\beta \\rho &amp; \\sigma_\\beta^2 \\end{pmatrix} \\] Instead we decompose it, in a way that treats the standard deviations and correlations separately, because this will become useful in setting priors sigmas &lt;- c(sigma_a,sigma_b) Rho &lt;- matrix( c(1,rho,rho,1) , nrow=2 ) Sigma &lt;- diag(sigmas) %*% Rho %*% diag(sigmas) i.e. \\[ \\begin{pmatrix} \\sigma_\\alpha^2 &amp; \\sigma_\\alpha \\sigma_\\beta \\rho \\\\ \\sigma_\\alpha \\sigma_\\beta \\rho &amp; \\sigma_\\beta^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\begin{pmatrix} 1 &amp; \\rho \\\\ \\rho &amp; 1 \\end{pmatrix} \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} \\] Thats the setup, heres the simulation part, with a plot of the data, that shows how the intercepts and slopes covary. N_cafes &lt;- 20 set.seed(5) vary_effects &lt;- as_tibble(mvrnorm(N_cafes, Mu, Sigma))%&gt;% rename(intercepts = V1, slopes = V2) vary_effects &lt;- bind_cols(cafe = 1:N_cafes,vary_effects) plot_cafe_data &lt;- ggplot(data = vary_effects, aes(x = intercepts, y = slopes))+ geom_point(col = &quot;blue&quot;, shape = 1) for (l in c(0.1,0.3,0.5,0.8,0.99)){ plot_cafe_data &lt;- plot_cafe_data + stat_ellipse(type = &quot;norm&quot;,level = l)} plot_cafe_data Each point is a cafe. We now simulate 10 visits to each cafe: set.seed(22) data_cafe &lt;- tibble( cafe= rep( 1:N_cafes , each=10 ), afternoon=rep(0:1,10*N_cafes/2))%&gt;% left_join(vary_effects, by = &quot;cafe&quot;)%&gt;% mutate(wait = rnorm(200,mean = intercepts + slopes * afternoon , sd = 0.5 )) Now we fit a model to see if we can get back the data generating process. The model looks like this: \\[ \\begin{aligned} W_i &amp;\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\ \\mu_i &amp;= \\alpha_{\\text{CAFE}[i]} + \\beta_{\\text{CAFE}[i]}A_i \\\\ \\\\ \\begin{bmatrix} \\alpha_{\\text{CAFE}[i]} \\\\ \\beta_{\\text{CAFE}[i]} \\end{bmatrix} &amp;\\sim \\text{MVNormal}\\left( \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix} , S \\right) &amp;&amp; \\text{population of varying effects}\\\\ \\\\ S &amp;= \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} &amp;&amp; \\text{construct covariance matrix} \\\\ \\\\ \\alpha &amp;\\sim \\text{Normal}(5,2) &amp;&amp; \\text{prior for average intercept}\\\\ \\beta &amp;\\sim \\text{Normal}(-1,0.5) &amp;&amp; \\text{prior for average slope} \\\\ \\sigma &amp;\\sim \\text{Exponential}(1) &amp;&amp; \\text{prior std dev within cafes} \\\\ \\sigma_\\alpha &amp;\\sim \\text{Exponential}(1) &amp;&amp; \\text{prior std dev among intercepts}\\\\ \\sigma_\\beta &amp;\\sim \\text{Exponential}(1) &amp;&amp; \\text{prior std dev among slopes}\\\\ R &amp;\\sim \\text{LKJcorr}(2) &amp;&amp; \\text{prior for correlation matrix} \\end{aligned} \\] After running, we plot the posterior correlation between intercepts and slopes. In our simulation data, there is a negative correlation: busy cafes have larger differences in wait times between morning and afternoon. Our model reflects this: Revisit: The book includes a section on constructing a model with more than two varying effects, using the chimp example. This section is especially useful because it demonstrates a non-centered parameterisation for this kinds of model using Cholesky decomposition. Instruments and Causal Designs We return to the problem of estimating the effect of education on wages. We expect there to be some unobserved factors that may confound inference: We cant close the backdoor path, because we have not observed U. But we might be able to use an instrumental variable to make inferences. An instrumental variable \\(Q\\) must be: Independent of U Not independent of E Q must have no influence on W except through E The book notes that 1 and 3 in particular, are not testable, and can be strong assumptions. Assuming we have an instrumental variable, our DAG now looks like: How do we use \\(Q\\). The book suggesting thinking of Q in this example as the quarter of the year a person is born in, which has an influence on how much education a person receives. The chapter simulates some data: set.seed(73) N &lt;- 500 U_sim &lt;- rnorm( N ) Q_sim &lt;- sample( 1:4 , size=N , replace=TRUE ) E_sim &lt;- rnorm( N , U_sim + Q_sim ) W_sim &lt;- rnorm( N , U_sim + 0*E_sim ) data_edu_sim &lt;- list(W=standardize(W_sim) , E=standardize(E_sim) , Q=standardize(Q_sim) ) You can see that in the simulated data, education has no causal effect on wages. The first model attempted is a straightforward regression of wages on education: \\[ \\begin{aligned} W &amp;\\sim N(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha_W + \\beta_{EW}E\\\\ \\alpha_W &amp;\\sim N(0,0.2)\\\\ \\beta_{EW} &amp;\\sim N(0,0.5)\\\\ \\sigma &amp;\\sim \\text{Exp}(1) \\end{aligned} \\] The model believes that education leads to higher wages (you can see that \\(b_{EW}\\) is very far from 0): ## mean sd 5.5% 94.5% n_eff Rhat4 ## aW -0.001419763 0.04103712 -0.0668210 0.0614210 2058.644 0.9999916 ## bEW 0.397312055 0.04067676 0.3330058 0.4587898 1922.806 1.0005218 ## sigma 0.918514205 0.02924823 0.8724036 0.9659672 1900.922 1.0034956 Next we add \\(Q\\) as a predictor: \\[ \\begin{aligned} W &amp;\\sim N(\\mu_i, \\sigma)\\\\ \\mu_i &amp;= \\alpha_W + \\beta_{EW}E + \\beta_{QW}Q\\\\ \\alpha_W &amp;\\sim N(0,0.2)\\\\ \\beta_{EW} &amp;\\sim N(0,0.5)\\\\ \\beta_{QW} &amp;\\sim N(0,0.5)\\\\ \\sigma &amp;\\sim \\text{Exp}(1) \\end{aligned} \\] And the results are worse: ## mean sd 5.5% 94.5% n_eff Rhat4 ## aW -0.0002878362 0.03732442 -0.05972346 0.05953473 1940.322 0.9988731 ## bEW 0.6355551170 0.04867284 0.55926847 0.71532664 1195.392 1.0002084 ## bQW -0.4058662475 0.04678752 -0.47877007 -0.33126016 1205.308 0.9991839 ## sigma 0.8575132195 0.02795544 0.81467929 0.90481446 1552.985 1.0008698 The estimated effect of education on wages is even larger, and the model also thinks that \\(Q\\) is correlated with wages even when \\(E\\) is included in the model. We know from the simulation that \\(Q\\) has no effect on wages except through E; the error comes from the fact that E is a collider of \\(Q\\) and \\(U\\). The chapter goes on to describe how \\(Q\\) should be used, starting by writing the generative version of the model (assuming the DAG). According to the DAG, wages are a function of education, and our unobserved confound: \\[ \\begin{aligned} W_i &amp;\\sim N(\\mu_{W,i},\\sigma_W)\\\\ \\mu_{W,i} &amp;= \\alpha_W + \\beta_{EW}E_i + U_i \\end{aligned} \\] Education is a function of quarter of birth and the unobserved confound: \\[ \\begin{aligned} E_i &amp;\\sim N(\\mu_{E,i},\\sigma_E)\\\\ \\mu_{E,i} &amp;= \\alpha_E + \\beta_{QE}Q_i + U_i \\end{aligned} \\] We assume even numbers of people born in each quarter: \\[ Q \\sim \\text{Categorical}([0.25,0.25,0.25,0.25]) \\] For now we assume U is normally distributed with mean 0 and standard deviation 1: \\[ U_i \\sim N(0,1) \\] In order to create a statistical model out of all of this, we use a multivariate linear model: \\[ \\begin{aligned} \\begin{pmatrix} W_i \\\\ E_i \\end{pmatrix} &amp;\\sim \\text{MVNormal}(\\begin{pmatrix} \\mu_{W,i} \\\\ \\mu_{E,i} \\end{pmatrix},S)\\\\ \\mu_{W,i} &amp;= \\alpha_W + \\beta_{EW}E_i \\\\ \\mu_{E,i} &amp;= \\alpha_E + \\beta_{QE}Q_i \\end{aligned} \\] Whats happening here is that wages and education are both simultaneously outcomes of our regression. The \\(S\\) here is analogous to \\(\\sigma\\) in the above simple linear regressions - its meant to capture residual correlations between pairs of \\(W\\) and \\(E\\) (e.g. from the action of our unobserved confound). Here are the results: ## mean sd 5.5% 94.5% n_eff Rhat4 ## aE 0.0008915069 0.03433353 -0.05523862 0.05541958 1480.0990 0.9996156 ## aW 0.0006545851 0.04377868 -0.07079820 0.07273627 1401.6666 1.0010062 ## bQE 0.5891858335 0.03498143 0.53501613 0.64443475 1411.4764 0.9993527 ## bEW -0.0463663122 0.07308362 -0.16794360 0.06406582 777.4085 1.0020241 ## Rho[1,1] 1.0000000000 0.00000000 1.00000000 1.00000000 NaN NaN ## Rho[1,2] 0.5402627720 0.05073347 0.45610957 0.62018750 750.5671 1.0018643 ## Rho[2,1] 0.5402627720 0.05073347 0.45610957 0.62018750 750.5671 1.0018643 ## Rho[2,2] 1.0000000000 0.00000000 1.00000000 1.00000000 NaN NaN ## Sigma[1] 1.0241176895 0.04590810 0.95699685 1.10006935 767.6010 1.0025145 ## Sigma[2] 0.8085983475 0.02547070 0.76929886 0.85086392 1633.9111 1.0003105 The model now correctly believes that the causal effect of education on wages is close to zero. The residual correlation between wages and education, \\(\\rho_{1,2}\\), is positive, which reflects the influence of \\(U\\). Endnotes 208 and 209 point to some real-world attempts to use instrumental variables for inference. Revisit: The chapter includes a short discussion of the front-door criterion, which I first read about in Judea Pearls Book of Why. Then there is a second example that uses a custom covariance matrix. This time to make inferences about social relations in a community in Nicaragua. Continuous Categories and the Gaussian Process The challenge of this section is to extend our application of varying effects from unordered categories to continuous variables. To do this the chapter introduces Gaussian process regression. The chapter returns to the chapter 11 data set of tool use in historic Oceanic societies, this time adding a measure of geographic distance to the model. ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 Heres the model well be using: \\[ \\begin{aligned} T_i &amp;\\sim \\text{Poisson}(\\lambda_i) \\\\ \\lambda_i &amp;= \\exp(k_{\\text{SOC}[i]}) \\alpha P_i^\\beta / \\gamma\\\\ \\\\ \\begin{pmatrix} k_1 \\\\ k_2 \\\\ k_3 \\\\ \\dots \\\\ k_{10} \\end{pmatrix} &amp;\\sim \\text{MVNormal}\\left( \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\dots \\\\ 0 \\end{pmatrix} , K \\right) &amp;&amp; \\text{prior for intercepts}\\\\ \\\\ K_{ij} &amp;= \\eta^2 \\exp(-\\rho^2D^2_{ij}) + \\delta_{ij}\\sigma^2 &amp;&amp; \\text{covariance matrix} \\end{aligned} \\] Here the \\(\\lambda_i\\) term is the model from chapter 11 with an additional term for varying intercept \\(k_{\\text{SOC}[i]}\\). Negative values of \\(k_\\text{SOC[i]}\\) will reduce \\(\\lambda\\), and positive values will increase it. Why are the entries of the covariance matrix defined like that? The \\(\\exp(-\\rho^2D^2_{ij})\\) term means that covariance between societies \\(i\\) and \\(j\\) declines exponentially with the square of the distance between them. The exact rate is controlled by \\(\\rho\\). \\(\\eta^2\\) is the maximum covariance between any two societies. In the \\(\\delta_{ij}\\sigma^2\\) term, \\(\\delta_{ij}\\) is the Kronecker delta. This term would be used if we had more than one observation for society and wanted to allow for additional covariance when \\(i=j\\). Here are the parameter results: ## mean sd 5.5% 94.5% n_eff Rhat4 ## k[1] -0.148402659 0.30998543 -0.63955874 0.33646864 709.9680 1.005093 ## k[2] 0.002684859 0.30056597 -0.45053691 0.49759916 633.2755 1.004251 ## k[3] -0.050550296 0.28174896 -0.47895698 0.40597849 598.4924 1.004261 ## k[4] 0.366865490 0.26729507 -0.02193611 0.81699311 657.3474 1.005273 ## k[5] 0.094786551 0.26006232 -0.28204444 0.53608437 697.9420 1.002621 ## k[6] -0.360295041 0.27160317 -0.80016738 0.03490379 818.9144 1.003139 ## k[7] 0.157534273 0.25409633 -0.20737781 0.56643565 678.2727 1.003498 ## k[8] -0.192210414 0.25940191 -0.58630755 0.22291150 748.3814 1.003753 ## k[9] 0.277474804 0.24967771 -0.08590088 0.69005578 705.6751 1.003134 ## k[10] -0.150614677 0.34512273 -0.70713486 0.39019117 1112.6261 1.000693 ## g 0.602461754 0.57198391 0.07419879 1.71741660 1607.5003 1.003247 ## b 0.278706970 0.08672352 0.14322593 0.42380396 1306.5750 1.002605 ## a 1.390190386 1.10746784 0.21896477 3.40762055 2166.0644 1.000575 ## etasq 0.193184148 0.20287420 0.02551770 0.55632958 989.9122 1.001779 ## rhosq 1.387124588 1.72623165 0.08278087 4.67127800 1864.7903 1.001385 These are a little difficult to interpret, but we can plot the Gaussian process function to get a sense of how the model expects covariance to change with increasing distance: Ive drawn 250 lines for each plot. The bold line in the right hand plot is the posterior mean. Because each society is assigned a parameter, and the model includes a covariance matrix, we can also make inferences about which societies are correlated. The chapter produces a matrix of correlations and plots them. Revisit: The chapter closes with a fun rundown of phylogenetic regression. Here we use Gaussian processes in a model that includes phylogenetic distance, as opposed to physical distance. 14.2 Questions 14E1 Question Answer Further Resources Endnote 204 lists a handful of resources for non-centered parameterisation: Model determination using sampling-based methods - Gelfand (1996) Updating schemes, correlation structure, blocking and parameterisation for the Gibbs sampler - Roberts and Sahu (1997) A general framework for the parametrization of hierarchical models - Papaspiliopoulos et al. (2007) Hamiltonian Monte Carlo for hierarchical models - Betancourt and Girolami (2013) "],["missing_data.html", "Chapter 15 Missing Data and Other Opportunities 15.1 Chapter Notes 15.2 Questions Further Reading", " Chapter 15 Missing Data and Other Opportunities 15.1 Chapter Notes Measurement Error We want to build models that allow for measurement error. The book returns to the Waffle House / Divorce example from chapter 5. Both the marriage and divorce columns in the data come with standard errors that we did not make use of back when we first saw this example. The plot on the left here is a straightforward plot of the data, including error bars, on divorce against age of marriage. Theres one data point per U.S. state. The plot on the right is meant to demonstrate that the standard error is much larger for states with small populations as youd expect. This is important, because variation in the size of the error among states is likely to introduce biases. In order to motivate the approach to incorporating measurement data, the chapter draws the following graph of the data generating processes: As usual, variables in circles are unobserved. Here the DAG assumes that the marriage rate (\\(M\\)) and age at marriage (\\(A\\)) influence the divorce rate (\\(D\\)). But we dont observe the divorce rate, we observe \\(D_obs\\) which is also influenced by measurement error \\(e_D\\). We can attempt to recover \\(D\\) by assuming a distribution for it, and assigning it a parameter in our model with a specified error. E.g: \\[ D_{\\text{OBS},i} \\sim \\text{Normal}(D_{\\text{TRUE},i},D_{\\text{SE},i}) \\] Our model will look like this: \\[ \\begin{aligned} D_{\\text{OBS},i} &amp;\\sim \\text{Normal}(D_{\\text{TRUE},i},D_{\\text{SE},i}) \\\\ D_{\\text{TRUE},i} &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_M M_i\\\\ \\end{aligned} \\] Heres the posterior for (some of) the model parameters: ## mean sd 5.5% 94.5% n_eff Rhat4 ## a -0.05604315 0.09815946 -0.2131211 0.1010110 1555.4378 0.9994692 ## bA -0.61774701 0.16306287 -0.8669412 -0.3612279 989.0135 1.0027799 ## bM 0.04557230 0.16689614 -0.2227070 0.3179405 1128.1817 1.0005704 ## sigma 0.59257881 0.10724280 0.4310783 0.7785063 722.5359 1.0001266 Compared to the chapter 5 model, \\(bA\\) has almost halved. In this case the impact of measurement error was to exaggerate the effect of marriage age on divorce. However you cant assume that measurement error will always increase the effects of interest, sometimes it can obscure them. Endnote 223 points to some papers on this. What if there is also measurement error on the predictor variables e.g. marriage rate? Heres the DAG: and heres the model: \\[ \\begin{aligned} D_{\\text{OBS},i} &amp;\\sim \\text{Normal}(D_{\\text{TRUE},i},D_{\\text{SE},i}) \\\\ D_{\\text{TRUE},i} &amp;\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\ \\mu_i &amp;= \\alpha + \\beta_A A_i + \\beta_M M_{\\text{TRUE},i}\\\\ M_{\\text{OBS},i} &amp;\\sim \\text{Normal}(M_{\\text{TRUE},i},M_{\\text{SE},i}) \\\\ M_{\\text{TRUE},i} &amp;\\sim \\text{Normal}(0,1) \\\\ \\end{aligned} \\] Standardising the observed marriage rate helps us choose a sensible prior distribution for the true marriage rate. Although later in the chapter (and in an exercise) a prior more informed by the data generating process is trialled. Revisit: Fit the model, plot figure 15.3. Missing Data Sometimes data is simply missing. We want a principled approach that considers the data generating process. The chapter introduces a simple example about dogs eating homework to demonstrate: \\(S\\) is the amount a student studies. It influences homework quality. \\(D\\) is whether a dog has eaten the homework. \\(H_\\text{obs}\\) is the quality of observed homework. It is influenced by true homework quality, but is missing in cases when \\(D\\)=1 (i.e. a dog has eaten the homework). There are four possible generative processes discussed. Until I figure out how to caption dagitty objects, lets call these (a), (b), (c), (d) going from the top left corner to the top right, bottom right then bottom left. Dogs eat homework at random Dogs eat the homework of students who study a lot (not paying enough attention to the dog) Noisiness (\\(X\\)) influences both homework quality and tendency for homework to be eaten Dogs prefer to eat bad homework In the first case (a), because whether the dogs eat the homework at random, H is independent of D and so we wouldnt expect the dogs to change the inferences we make about the effect of \\(S\\) on \\(H\\). The second case (b) is also not so bad. There is a backdoor path from \\(D\\) to \\(H\\) through \\(S\\), but since we want to condition on \\(S\\) anyway its not terrible. In both of these cases, the exercises include comparison of inferences made with complete data and when some data is missing (eaten). The main body of this chapter gives a fuller treatment to scenarios (c) and (d), where things get trickier. We simulate some data: set.seed(501) N &lt;- 1000 X &lt;- rnorm(N) S &lt;- rnorm(N) H &lt;- rbinom( N , size=10 , inv_logit( 2 + S - 2*X ) ) D &lt;- if_else( X &gt; 1 , 1 , 0 ) H_obs &lt;- H H_obs[D==1] &lt;- NA Whats happening here: Homework is a binomial variable with 10 trials, where the probability of success is increased by \\(S\\) and decreased by \\(X\\). The chapter says that the true coefficient on S should be 1.00. but I dont understand why. If \\(X\\) is greater than 1, the dog eats the homework. Increased noise is therefore associated both with worse quality homework and missing homework. Heres a summary of the posterior parameter distributions we get assuming we can see \\(H\\) directly: ## mean sd 5.5% 94.5% n_eff Rhat4 ## a 1.1128840 0.02392241 1.0761494 1.1514411 1221.785 1.003241 ## bS 0.6891082 0.02562531 0.6482949 0.7303994 1212.296 1.000231 Now heres the outcome of the same model where the missing cases are simply dropped: ## mean sd 5.5% 94.5% n_eff Rhat4 ## a 1.7948496 0.03578085 1.7380078 1.850438 927.8955 1.000555 ## bS 0.8274926 0.03330760 0.7749454 0.882055 872.6053 1.002531 We can see that \\(bS\\) is now closer to the true value of 1. This is because on average homework is missing from noisy houses, and its usually noisy houses where our estimate of the effect of studying is confounded. In this case the missingness made our inference easier, but in another scenario it could easily make things worse. In scenario (d) dogs prefer to eat bad homework. But the variable causes its own missingness through the non-causal path \\(S \\rightarrow H \\rightarrow D \\rightarrow H_{obs}\\). This is the most difficult situation to deal with. The next section of the chapter applies the above to the problem of imputing missing data in the primate milk example from earlier in the book. Revisit. 15.2 Questions Further Reading Endnote 225: See Molenberghs et al. (2014) for an overview of contemporary approaches, Bayesian and otherwise Endnote 226: In ecology, the absence ofan observation ofa species is a subtle kind of observation. It could mean the species isnt there. Or it could mean it is there but you didnt see it. An entire category of models, occupancy models, exists to take this duality into account. "],["generalized.html", "Chapter 16 Generalized Linear Madness 16.1 Chapter Notes 16.2 Questions Further Reading", " Chapter 16 Generalized Linear Madness 16.1 Chapter Notes This chapter goes beyond generalised linear models, introducing examples of structural, causal models more informed by scientific theory. Geometric People The chapter introduces a simple example of a structural model. In chapter 4, we used peoples weight to predict their heights. But we know more about the relationship between weight and height, and we can give our model this information. One way to do this would be to assume a person is roughly a cylinder, we would have the following equation relating volume to height: \\[ V = \\pi r^2 h. \\] We dont have data on the radius of our population; we assume it is some fixed proportion \\(p\\) of height. We further assume that there is a fixed ratio between volume and weight. We have: \\[ W = kV = k\\pi p^2 h^3. \\] Heres the model we fit: \\[ \\begin{aligned} W_{i} &amp;\\sim \\text{Log-Normal}(\\mu_{i},\\sigma_{i}) \\\\ \\exp(\\mu_i) &amp;= k \\pi p^2 h_i^3 \\end{aligned} \\] We use the log-normal since we know weight must be non-negative. One benefit of a structural model is that the parameters have scientific meaning, and so it can be easier to assign priors. E.g the chapter uses \\(\\text{Beta}(2,18)\\) as a prior for \\(p\\) since we know that it must be between zero and one and is likely below 0.5. The meaning of \\(k\\) is something like density, and we can assign reasonable priors accordingly. You could also set sensible priors by dividing out the units in the volume equation above by e.g. dividing both weight and height by their averages. Then you can get a good guess at \\(k\\) for a person of average height and weight, and set priors informed by this value. Here are the parameter estimates: ## mean sd 5.5% 94.5% n_eff Rhat4 ## p 0.2480813 0.059571721 0.1666790 0.3570929 480.9952 1.003791 ## k 5.7687082 2.732428741 2.3756432 10.7833260 504.3511 1.003261 ## sigma 0.2069715 0.006224872 0.1974887 0.2167353 717.9134 1.002929 Lets plot the posterior predictions: The blue dots are the raw data, the shaded region is the 89% compatibility interval. The exponent of height on weight is not estimated by the model, it is fixed at 3 by our cylinder model, but it performs well. The chapter notes that with a theoretically informed model, deviations can tell us something about the process - e.g. the model fits poorly at low heights, this may be because either \\(p\\) or \\(k\\) is different for children than adults. Hidden Minds and Observed Behaviour The next example I wont go into much detail. It comes from an experiment in developmental psychology where children choose one of three different coloured blocks, and we want to back inferences about their decision making processes using a structural, causal model. The approach laid out is to generate a priori plausible strategies (the design of the experiment suggests some: children were shown four other children making their own colour choice first so one strategy might be to follow the majority). We will know the probability that a child chooses a particular colour, assuming they followed a particular strategy, and so Bayes theorem can tell us the relative probability of each strategy after seeing the colour choices. The chapter describes this as an example of a state space model, where multiple hidden states produce observations. Ordinary Differential Nut Cracking This example Ill go into a little more. It uses data on chimpanzees who try to crack open nuts using tools, and it uses ordinary differential equations in the way that scientific theory informs the model. That said, its not a very different approach than the cylinder weight example above because the ODE has a simple analytical solution. The first model the chapter tries is one in which only strength matters for rate of nut opening. Lets assume that strength is proportional to mass. We have theory about how mass of chimpanzees change as they age: they have a maximum potential mass, and the rate of mass increase depends on how far away they are from that maximum: \\[ \\frac{\\text{d}M}{\\text{d}t} = k (M_\\text{max} - M_t) \\] which is an ordinary differential equation with solution: \\[ M_t = M_\\text{max}(1- \\exp(-kt)) \\] We also have that strength is proportional to mass \\(S = \\beta M_t\\) and we also want to define some function to relate strength to rate of nut cracking \\(\\lambda\\). The chapter chooses one that allows increasing returns to strength \\(\\lambda = \\alpha S^\\theta\\). All together: \\[ \\lambda = \\alpha S^\\theta = \\alpha (\\beta M_\\text{max}(1- \\exp(-kt)))^\\theta. \\] We make simplifications by rescaling mass so that maximum body mass is one. We can also use replace \\(\\alpha \\beta^\\theta\\) by \\(\\phi\\) since that term just rescales units. We have: \\[ \\lambda = \\phi (1- \\exp(-kt))^\\theta. \\] We then fit a model for number of nuts cracked using a Poisson likelihood, where \\(lambda\\) defines our rate of nut cracking. Our predictor is age. We plot the posterior. The blue circles are the raw data, scaled by the number of seconds particular trial lasted. The lines are drawn from the posterior. Population Dynamics In this example, the ODEs used have no analytical solution. We are modelling population dynamics of hare and lynx. We have: \\[ \\frac{\\text{d}H}{\\text{d}t} = H_t b_H - H_t L_t m_H = H_t (b_H - L_t m_H) \\] where: \\(H_t\\) is the population of hare at time \\(t\\). \\(b_H\\) is the hare birth rate the term \\(L_t m_H\\) is the hare death rate, which is influenced by the population of lynx \\(L_t\\). Similarly, for the lynx: \\[ \\frac{\\text{d}L}{\\text{d}t} = L_t (H_t b_L - m_L). \\] In this case we assume the lynx birth rate depends on the number of hare, and the death rate is constant. This is the Lotka-Volterra model. We want a statistical model using these dynamics. One problem though is that our data does not contain true populations of hare and lynx, it contains counts of pelts. We write a model that assumes some proportion of the animal population was trapped each year, with some error term. Our data cannot tell us the proportion of animals that were captured, so we have to fix it using a prior. The chapter points out that although this is not ideal, it is better that our model forces us to grapple with the limitations of the data rather than naively use the pelt data as if they were true population counts. The model is this: \\[ \\begin{aligned} h_{t} &amp;\\sim \\text{Log-Normal}(\\log(p_HH_t),\\sigma_{H}) \\\\ l_{t} &amp;\\sim \\text{Log-Normal}(\\log(p_LL_t),\\sigma_{L}) \\\\ H_{T&gt;1} &amp;= H_1 + \\int^T_1 H_t(b_H - L_t m_H)\\text{dt} \\\\ L_{T&gt;1} &amp;= L_1 + \\int^T_1 L_t(H_t b_L - m_L)\\text{dt} \\end{aligned} \\] where: \\(h_t\\) and \\(l_t\\) are the observed populations \\(H_t\\) and \\(L_t\\) are the true populations \\(p_H\\) and \\(p_L\\) are the proportions of the true population captured each year, fixed by some beta prior We make use of Stans built-in functions for numerically solving differential equations. Heres the model code: ## functions { ## real[] dpop_dt( real t, // time ## real[] pop_init, // initial state {lynx, hares} ## real[] theta, // parameters ## real[] x_r, int[] x_i) { // unused ## real L = pop_init[1]; ## real H = pop_init[2]; ## real bh = theta[1]; ## real mh = theta[2]; ## real ml = theta[3]; ## real bl = theta[4]; ## // differential equations ## real dH_dt = (bh - mh * L) * H; ## real dL_dt = (bl * H - ml) * L; ## return { dL_dt , dH_dt }; ## } ## } ## data { ## int&lt;lower=0&gt; N; // number of measurement times ## real&lt;lower=0&gt; pelts[N,2]; // measured populations ## } ## transformed data{ ## real times_measured[N-1]; // N-1 because first time is initial state ## for ( i in 2:N ) times_measured[i-1] = i; ## } ## parameters { ## real&lt;lower=0&gt; theta[4]; // { bh, mh, ml, bl } ## real&lt;lower=0&gt; pop_init[2]; // initial population state ## real&lt;lower=0&gt; sigma[2]; // measurement errors ## real&lt;lower=0,upper=1&gt; p[2]; // trap rate ## } ## transformed parameters { ## real pop[N, 2]; ## pop[1,1] = pop_init[1]; ## pop[1,2] = pop_init[2]; ## pop[2:N,1:2] = integrate_ode_rk45( ## dpop_dt, pop_init, 0, times_measured, theta, ## rep_array(0.0, 0), rep_array(0, 0), ## 1e-5, 1e-3, 5e2); ## } ## model { ## // priors ## theta[{1,3}] ~ normal( 1 , 0.5 ); // bh,ml ## theta[{2,4}] ~ normal( 0.05, 0.05 ); // mh,bl ## sigma ~ exponential( 1 ); ## pop_init ~ lognormal( log(10) , 1 ); ## p ~ beta(40,200); ## // observation model ## // connect latent population state to observed pelts ## for ( t in 1:N ) ## for ( k in 1:2 ) ## pelts[t,k] ~ lognormal( log(pop[t,k]*p[k]) , sigma[k] ); ## } ## generated quantities { ## real pelts_pred[N,2]; ## for ( t in 1:N ) ## for ( k in 1:2 ) ## pelts_pred[t,k] = lognormal_rng( log(pop[t,k]*p[k]) , sigma[k] ); ## } The functions block at the top includes the specification of the differential equations. Stans integrate_ode_rk45 function does the integration in the transformed parameters block. We run the model, and plot the results: 16.2 Questions Further Reading Endnote 233 recommends a few articles about the philosophy of model building: The strategy of model building in population biology - Levins (1966) Using false models to elaborate constraints on processes: Blending inheritance in organic and cultural evolution. - Wimsatt (2002) Models are stupid, and we need more of them. - Smaldino (2017) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
