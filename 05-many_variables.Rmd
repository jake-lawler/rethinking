# The Many Variables & The Spurious Waffles {#many_variables}


```{r echo = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(rethinking)
library(tidybayes)
library(tidybayes.rethinking)
library(modelr)
library(ggdist)
library(magrittr)
library(dagitty)
library(forcats)
```

## Chapter Notes

This chapter introduces multiple regression and causal models. Will try to race through it, just getting some practice visualising priors, using tidybayes to display model uncertainty etc.

Model 5.1: Modelling divorce rate on age at marriage.

$$
\begin{aligned}
D_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_A A_i \\
\alpha &\sim \text{Normal}(0, 0.2) \\ 
\beta_A &\sim \text{Normal}(0, 0.5) \\
\sigma &\sim \text{Exponential}(1) \\
\end{aligned}
$$
Here are the variables we'll use in this model and later on:

* $D_i$ - the 2009 divorce rate per 1,000 adults for each U.S. state.
* $A_i$ - The 2005-2010 median age at marriage for each state

Plot of these priors:

```{r}
# we generate 100 samples from our prior distributions
prior_sim_5_1 <- tibble(a=rnorm(100, mean = 0, sd=0.2),
                    bA = rnorm(100, mean = 0, sd = 0.5))

# and plot the lines they imply
ggplot(prior_sim_5_1)+
  geom_abline(aes(slope = bA,
                  intercept = a))+
  xlim(-3,3)+
  ylim(-3,3)+
  xlab("Median Age of Marriage (std)")+
  ylab("Divorce Rate (std)")
```
Here's the model and the posterior mean/expected divorce rate:

```{r}
data(WaffleDivorce) 
data_divorce <- WaffleDivorce%>%as_tibble()

data_divorce %<>% mutate(
  divorce_s = (Divorce - mean(Divorce))/sd(Divorce),
  marriage_s = (Marriage - mean(Marriage))/sd(Marriage),
  age_s = (MedianAgeMarriage - mean(MedianAgeMarriage))/sd(MedianAgeMarriage),
)

m5_1 <- quap( 
  alist( 
    divorce_s ~ dnorm( mu , sigma ) , 
    mu <- a + bA * age_s , 
    a ~ dnorm( 0 , 0.2 ) , 
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = data_divorce )

summary_5_1 <- m5_1 %>%
  gather_draws(a, bA, sigma) %>%
  median_qi(.width=0.89)%>%
  mutate(model = "m5_1")

data_grid(data_divorce,age_s=seq_range(age_s, n = 51)) %>% 
  add_linpred_draws(m5_1) %>% # add draws for mean divorce rate
  mutate(MedianAgeMarriage= age_s * sd(data_divorce$MedianAgeMarriage) + mean(data_divorce$MedianAgeMarriage),
         Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%>% 
  ggplot(aes(x = MedianAgeMarriage, y=Divorce)) +
  stat_lineribbon() +
  geom_point(data = data_divorce, colour = "light blue") +
  scale_fill_brewer(palette = "Greys")

```

Here's the same plot for model 5.2 - which uses marriage rate as a predictor instead of age at marriage:

```{r}
m5_2 <- quap( 
  alist( 
    divorce_s ~ dnorm( mu , sigma ) , 
    mu <- a + bM * marriage_s , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = data_divorce )

summary_5_2 <- m5_2 %>%
  gather_draws(a, bM, sigma) %>%
  median_qi(.width=0.89)%>%
  mutate(model = "m5_2")

data_grid(data_divorce,marriage_s=seq_range(marriage_s, n = 51)) %>% 
  add_linpred_draws(m5_2) %>% # add draws for mean divorce rate
  mutate(Marriage= marriage_s * sd(data_divorce$Marriage) + mean(data_divorce$Marriage),
         Divorce = .linpred * sd(data_divorce$Divorce) + mean(data_divorce$Divorce))%>% 
  ggplot(aes(x = Marriage, y=Divorce)) +
  stat_lineribbon() +
  geom_point(data = data_divorce, colour = "light blue") +
  scale_fill_brewer(palette = "Greys")
```

The chapter goes on to introduce DAGs and causal reasoning, before coming back to define a regression model that uses both marriage rate and median age at marriage as predictors.

Here are a couple of possible DAGs for how marriage rate and median age of marriage influence the divorce rate:

```{r, figures-side, fig.show="hold", out.width="50%"}

par(mar = c(4, 4, .1, .1))

dag_div1 <- dagitty("dag{A -> M; M -> D; A -> D }")
coordinates(dag_div1) <- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1))
drawdag(dag_div1)

dag_div2 <- dagitty("dag{A -> M; A -> D }")
coordinates(dag_div2) <- list( x=c(A=0,D=0.5,M=1) , y=c(A=0,M=0,D=1))
drawdag(dag_div2)

```

We can use our data to test these proposed causal models, because they make different predictions. The first has no conditional independencies:

```{r}
impliedConditionalIndependencies(dag_div1)
```
The second predicts that D is independent from M, conditional on A:

```{r}
impliedConditionalIndependencies(dag_div2)
```

Here's our statistical model:

$$
\begin{aligned}
D_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_M M_i + \beta_A A_i \\
\alpha &\sim \text{Normal}(0, 0.2) \\ 
\beta_M &\sim \text{Normal}(0, 0.5) \\
\beta_A &\sim \text{Normal}(0, 0.5) \\
\sigma &\sim \text{Exponential}(1) \\
\end{aligned}
$$

```{r}
m5_3 <- quap( 
  alist( 
    divorce_s ~ dnorm( mu , sigma ) , 
    mu <- a + bM * marriage_s + bA * age_s , 
    a ~ dnorm( 0 , 0.2 ) , 
    bM ~ dnorm( 0 , 0.5 ) ,
    bA ~ dnorm( 0 , 0.5 ) , 
    sigma ~ dexp( 1 )
) , data = data_divorce )

summary_5_3 <- m5_3 %>%
  gather_draws(a, bM, bA, sigma) %>%
  median_qi(.width=0.89)%>%
  mutate(model = "m5_3")

summary_5_3
```

Here's a plot for the three models:

```{r}

bind_rows(summary_5_1, summary_5_2,summary_5_3) %>%
  ggplot(aes(y = fct_rev(.variable), x = .value, xmin = .lower, xmax = .upper, color = model)) +
  geom_pointinterval(position = position_dodge(width = -0.3))+
  ylab("parameters")

```
As predicted by the second DAG, marriage rate is not associated with divorce rate once you condition on age of marriage.

## Questions

### 5E1 {-}

#### Question {-}

>Which of the linear models below are multiple linear regressions? 
>
(1) µi = α + βxi 
>
(2) µi = βx * xi + βz * zi 
>
(3) µi = α + β(xi − zi)
>
(4) µi = α + βx * xi + βz * zi


#### Answer {-}

Number 4 looks the most like the multiple regressions in the chapter: µ is regressed on both xi and zi with the "intercept" α. (2) is just (4) with the α set to zero, so that counts too.

Number 1 is just a bivariate regression.

Number 3 is interesting. I think this is not really a multiple regression, even though there are two variables. Rather than attempting to determine separately the influence of x and z on µ, you are asserting in the model that they have and equal and opposite impact. I think this is not really what you want a multiple regression to do, but don't feel confident about my answer.


### 5E2 {-}

#### Question {-}

>Write down a multiple regression to evaluate the claim: 
>
Animal diversity is linearly related to latitude, but only after controlling for plant diversity. You just need to write down the model definition.


#### Answer {-}


Oh boy. This question immediately feels like a trap with "animal diversity is *linearly* related to latitude." Surely if I choose to use a multiple linear regression with two variables, and control for one, the only relationships I'll observe will be linear.

I'm going to ignore the "linearly" part of the question from this point. It seems like the claim is that if I naively regress animal diversity on to latitude without accounting for plant diversity, I would find no relationship. I.e. that the relationship between latitude and animal diversity is masked.

If that interpretation is correct, I would start with a bivariate model.


$$
A_i \sim Normal(\mu, \sigma) \\

\mu_i = \alpha + \beta_L*L
$$

Where if the claim is true I would expect to see little relationship. I would then move on to a multiple regression including plant diversity:

$$
A_i \sim Normal(\mu, \sigma) \\

\mu_i = \alpha + \beta_L*L +\beta_P * P
$$

and examine whether it appears as if a relationship has now emerged.


### 5E3 {-}

#### Question {-}

>Write down a multiple regression to evaluate the claim: Neither amount of funding nor size of laboratory is by itself a good predictor of time to PhD degree; but together these variables are both positively associated with time to degree. 
>
Write down the model definition and indicate which side of zero each slope parameter should be on.


#### Answer {-}


$$
T_i \sim Normal(\mu, \sigma) \\

\mu_i = \alpha + \beta_F*F +\beta_S * S
$$

T - time to PhD degree
F - amount of funding
S - size of laboratory


### 5E4 {-}

#### Question {-}


>Suppose you have a single categorical predictor with 4 levels (unique values), labeled A, B, C and D. Let $A_i$ be an indicator variable that is 1 where case i is in category A. Also suppose $B_i$, $C_i$, and $D_i$ for the other categories. 
Now which of the following linear models are inferentially equivalent ways to include the categorical variable in a regression? Models are inferentially equivalent when it’s possible to compute one posterior distribution from the posterior distribution of another model.
>
$$
\begin{aligned}
(1) \mu_i &= \alpha + \beta_A A_i + \beta_B B_i + \beta_D D_i  \\
(2) \mu_i &= \alpha + \beta_A A_i + \beta_B B_i + \beta_C C_i + \beta_D D_i \\ 
(3) \mu_i &= \alpha + \beta_B B_i + \beta_C C_i + \beta_D D_i \\
(4) \mu_i &= \alpha_A A_i + α_B B_i + α_C C_i + α_D D_i \\
(5) \mu_i &= \alpha_A(1 − B_i − C_i −D_i) + \alpha_B B_i + \alpha_C C_i + \alpha_D D_i
\end{aligned}
$$

#### Answer {-}

1 is the standard indicator variable approach. Where A, B and D are equal to 0, $\alpha$ is the mean $\mu$ where the predictor is at level C.
2 There is redundancy in two, surely it wouldn't be possible to estimate $\alpha$ - it can take any value and produce the same $\mu$ so long as the appropriate $\beta_x$ adjusts to compensate. I don't know if that means it is not inferentially equivalent though.
3 is clearly equivalent to (1), it doesn't make a difference (except for interpretation) which of the levels you label null.
4 Is equivalent also, just set $\alpha_c$ equal to $\alpha$ from (1).
5 Is an incredibly annoying way to set up your model, but can be pretty easily transformed into (3) with some algebra and relabelling:

$$
\begin{aligned}
\mu_i &= \alpha_A(1 − B_i − C_i −D_i) + \alpha_B B_i + \alpha_C C_i + \alpha_D D_i \\
&= \alpha_A + (\alpha_B-\alpha_A)B_i + (\alpha_C -\alpha_A)C_i +(\alpha_D-\alpha_A)D_i  \\
&= \alpha + \beta_B B_i + \beta_C C_i + \beta_D D_i
\end{aligned}          
$$

## Further Reading {-}
